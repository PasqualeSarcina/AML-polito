{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y8AKrNt5lxaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' estratto con successo nella directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
      "Contenuti della directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted':\n",
      "['SPair-71k']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_file_path = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' \n",
    "extract_dir = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
    "\n",
    "# Crea la directory di estrazione se non esiste\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Estrai il file ZIP solo se esiste\n",
    "if os.path.exists(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"File '{zip_file_path}' estratto con successo nella directory '{extract_dir}'\")\n",
    "    print(f\"Contenuti della directory '{extract_dir}':\\n{os.listdir(extract_dir)}\")\n",
    "else:\n",
    "    print(f\"File zip '{zip_file_path}' non trovato. Assicurati che il dataset sia estratto in '{extract_dir}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "honcpimEq_B2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricati correttamente.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, image_keys):\n",
    "        self.image_keys = image_keys\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for key in self.image_keys:\n",
    "            image[key] /= 255.0\n",
    "            image[key] = self.normalize(image[key])\n",
    "        return image\n",
    "\n",
    "\n",
    "def read_img(path):\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "\n",
    "    return torch.tensor(img.transpose(2, 0, 1).astype(np.float32))\n",
    "\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n",
    "\n",
    "        self.datatype = datatype\n",
    "        self.pck_alpha = pck_alpha\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = self.ann_files[:len(self.ann_files) - 1]\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        self.categories = list(map(lambda x: os.path.basename(x), glob.glob('%s/*' % image_path)))\n",
    "        self.categories.sort()\n",
    "        self.transform = Normalize(['src_img', 'trg_img'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        raw_line = self.ann_files[idx]\n",
    "        ann_filename = raw_line.replace(':', '_')\n",
    "        ann_file = ann_filename + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_img = read_img(os.path.join(self.image_path, category, annotation['src_imname']))\n",
    "        trg_img = read_img(os.path.join(self.image_path, category, annotation['trg_imname']))\n",
    "\n",
    "        trg_bbox = annotation['trg_bndbox']\n",
    "        pck_threshold = max(trg_bbox[2] - trg_bbox[0],  trg_bbox[3] - trg_bbox[1]) * self.pck_alpha\n",
    "\n",
    "        sample = {'pair_id': annotation['pair_id'],\n",
    "                  'filename': annotation['filename'],\n",
    "                  'src_imname': annotation['src_imname'],\n",
    "                  'trg_imname': annotation['trg_imname'],\n",
    "                  'src_imsize': src_img.size(),\n",
    "                  'trg_imsize': trg_img.size(),\n",
    "\n",
    "                  'src_bbox': annotation['src_bndbox'],\n",
    "                  'trg_bbox': annotation['trg_bndbox'],\n",
    "                  'category': annotation['category'],\n",
    "\n",
    "                  'src_pose': annotation['src_pose'],\n",
    "                  'trg_pose': annotation['trg_pose'],\n",
    "\n",
    "                  'src_img': src_img,\n",
    "                  'trg_img': trg_img,\n",
    "                  'src_kps': torch.tensor(annotation['src_kps']).float(),\n",
    "                  'trg_kps': torch.tensor(annotation['trg_kps']).float(),\n",
    "\n",
    "                  'mirror': annotation['mirror'],\n",
    "                  'vp_var': annotation['viewpoint_variation'],\n",
    "                  'sc_var': annotation['scale_variation'],\n",
    "                  'truncn': annotation['truncation'],\n",
    "                  'occlsn': annotation['occlusion'],\n",
    "\n",
    "                  'pck_threshold': pck_threshold}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_dir = r\"C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted\\SPair-71k\\SPair-71k\"    \n",
    "    pair_ann_path = os.path.join(base_dir, 'PairAnnotation')\n",
    "    layout_path = os.path.join(base_dir, 'Layout')\n",
    "    image_path = os.path.join(base_dir, 'JPEGImages')\n",
    "    dataset_size = 'large'\n",
    "    pck_alpha = 0.1\n",
    "    \n",
    "    # Verifica che i percorsi esistano prima di creare il dataset\n",
    "    if os.path.exists(pair_ann_path) and os.path.exists(layout_path) and os.path.exists(image_path):\n",
    "        trn_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='trn')\n",
    "        val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='val')\n",
    "        test_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='test')\n",
    "\n",
    "        trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "        val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "        test_dataloader = DataLoader(test_dataset, num_workers=0)\n",
    "        print(\"Dataset caricati correttamente.\")\n",
    "    else:\n",
    "        print(f\"Errore: Impossibile trovare i percorsi del dataset in '{base_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 1 valid image per class with at least 3 keypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning:   0%|          | 0/12234 [00:00<?, ?it/s]C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_3300\\2796861923.py:149: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.\n",
      "  ax[2].scatter(x, y, c='green', s=150, marker='o', facecolors='none', linewidth=3)\n",
      "Scanning:   0%|          | 12/12234 [00:00<12:20, 16.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> [SUCCESS] Saved comparison for aeroplane (Image ID: 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning:  44%|████▍     | 5422/12234 [00:57<01:12, 94.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> [SUCCESS] Saved comparison for chair (Image ID: 5422)\n",
      "\n",
      "Generated all requested images. Exiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math \n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "\n",
    "# 1. LOAD MODEL\n",
    "print(\"Loading DINOv2 Model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# --- CONFIGURAZIONE ---\n",
    "output_dir = \"qualitative_results_grouped\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "TARGET_CLASSES = ['aeroplane', 'chair']\n",
    "POINTS_NEEDED = 3  # Numero di punti da visualizzare insieme sulla stessa immagine\n",
    "\n",
    "# Stato: traccia se abbiamo finito una categoria\n",
    "category_done = {cat: False for cat in TARGET_CLASSES}\n",
    "\n",
    "# Helper functions\n",
    "def denormalize_image(tensor):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = tensor.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "    img = (img * std) + mean\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "def pad_to_multiple(x, k=14):\n",
    "    h, w = x.shape[-2:]\n",
    "    new_h = math.ceil(h / k) * k\n",
    "    new_w = math.ceil(w / k) * k\n",
    "    pad_bottom = new_h - h\n",
    "    pad_right = new_w - w\n",
    "    if pad_bottom == 0 and pad_right == 0: return x\n",
    "    return F.pad(x, (0, pad_right, 0, pad_bottom), value=0)\n",
    "\n",
    "print(f\"Searching for 1 valid image per class with at least {POINTS_NEEDED} keypoints...\")\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for i, data in enumerate(tqdm(test_dataloader, desc=\"Scanning\")):\n",
    "        \n",
    "        category = data['category'][0]\n",
    "        \n",
    "        # Salta se categoria non richiesta o già completata\n",
    "        if category not in TARGET_CLASSES: continue\n",
    "        if category_done[category]: continue\n",
    "\n",
    "        # --- PROCESSIAMO L'IMMAGINE ---\n",
    "        src_img = data['src_img'].to(device)\n",
    "        trg_img = data['trg_img'].to(device)\n",
    "        src_img_padded = pad_to_multiple(src_img, 14)\n",
    "        trg_img_padded = pad_to_multiple(trg_img, 14)\n",
    "        \n",
    "        # Forward Pass\n",
    "        dict_src = model.forward_features(src_img_padded)\n",
    "        dict_trg = model.forward_features(trg_img_padded)\n",
    "        feats_src = dict_src[\"x_norm_patchtokens\"]\n",
    "        feats_trg = dict_trg[\"x_norm_patchtokens\"]\n",
    "        \n",
    "        # Grid Info\n",
    "        _, _, H_padded, W_padded = src_img_padded.shape \n",
    "        _, _, H_orig, W_orig = data['src_img'].shape\n",
    "        patch_size = 14\n",
    "        w_grid = W_padded // patch_size \n",
    "        h_grid = H_padded // patch_size\n",
    "        kps_list_src = data['src_kps'][0] \n",
    "        trg_kps_gt = data['trg_kps'][0] \n",
    "\n",
    "        # --- LISTE PER ACCUMULARE I 3 PUNTI ---\n",
    "        valid_src_points = []  # (x, y)\n",
    "        valid_pred_points = [] # (x, y)\n",
    "        valid_gt_points = []   # (x, y)\n",
    "\n",
    "        # Scansioniamo tutti i keypoint dell'immagine\n",
    "        for n_keypoint, keypoint_src in enumerate(kps_list_src):\n",
    "            \n",
    "            # Se ne abbiamo già trovati 3, smettiamo di calcolarne altri\n",
    "            if len(valid_src_points) >= POINTS_NEEDED:\n",
    "                break\n",
    "\n",
    "            # 1. Check Source Point\n",
    "            x_src = keypoint_src[0].item()\n",
    "            y_src = keypoint_src[1].item()\n",
    "            if math.isnan(x_src) or math.isnan(y_src): continue\n",
    "            \n",
    "            x_src, y_src = int(x_src), int(y_src)\n",
    "            if not (0 <= x_src < W_orig and 0 <= y_src < H_orig): continue\n",
    "\n",
    "            # 2. Prediction Logic\n",
    "            x_patch = min(x_src // patch_size, w_grid - 1)\n",
    "            y_patch = min(y_src // patch_size, h_grid - 1)\n",
    "            patch_idx = (y_patch * w_grid) + x_patch\n",
    "            if patch_idx >= feats_src.shape[1]: patch_idx = feats_src.shape[1] - 1\n",
    "\n",
    "            source_vec = feats_src[0, patch_idx, :]\n",
    "            sim_map = torch.cosine_similarity(source_vec, feats_trg[0], dim=-1)\n",
    "            best_idx = torch.argmax(sim_map).item()\n",
    "            \n",
    "            x_pred = (best_idx % w_grid) * patch_size + (patch_size // 2)\n",
    "            y_pred = (best_idx // w_grid) * patch_size + (patch_size // 2)\n",
    "\n",
    "            # 3. Check GT Point\n",
    "            x_gt = trg_kps_gt[n_keypoint, 0].item()\n",
    "            y_gt = trg_kps_gt[n_keypoint, 1].item()\n",
    "            if math.isnan(x_gt) or math.isnan(y_gt): continue\n",
    "            \n",
    "            # --- PUNTO VALIDO! AGGIUNGIAMO ALLE LISTE ---\n",
    "            valid_src_points.append((x_src, y_src))\n",
    "            valid_pred_points.append((x_pred, y_pred))\n",
    "            valid_gt_points.append((x_gt, y_gt))\n",
    "\n",
    "        # --- SE ABBIAMO TROVATO 3 PUNTI, FACCIAMO IL PLOT UNICO ---\n",
    "        if len(valid_src_points) == POINTS_NEEDED:\n",
    "            \n",
    "            img_s_vis = denormalize_image(src_img)\n",
    "            img_t_vis = denormalize_image(trg_img)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            # Colori per distinguere i punti 1, 2 e 3 (opzionale, utile per vedere le corrispondenze)\n",
    "            colors = ['cyan', 'orange', 'lime'] \n",
    "\n",
    "            # PANEL 1: Source Image con 3 punti\n",
    "            ax[0].imshow(img_s_vis)\n",
    "            ax[0].set_title(f\"SOURCE ({category})\\n(Blue Crosses)\")\n",
    "            for idx, (x, y) in enumerate(valid_src_points):\n",
    "                # Disegna croce blu grande\n",
    "                ax[0].scatter(x, y, c='blue', s=150, marker='o', linewidth=3)\n",
    "                # Aggiunge numero piccolo per capire quale punto è quale\n",
    "                ax[0].text(x+5, y+5, str(idx+1), color='white', fontsize=12, fontweight='bold')\n",
    "\n",
    "            # PANEL 2: Prediction Image con 3 punti\n",
    "            ax[1].imshow(img_t_vis)\n",
    "            ax[1].set_title(f\"PREDICTION (DINOv2)\\n(Red X)\")\n",
    "            for idx, (x, y) in enumerate(valid_pred_points):\n",
    "                ax[1].scatter(x, y, c='red', s=150, marker='o', linewidth=3)\n",
    "                ax[1].text(x+5, y+5, str(idx+1), color='white', fontsize=12, fontweight='bold')\n",
    "\n",
    "            # PANEL 3: Ground Truth Image con 3 punti\n",
    "            ax[2].imshow(img_t_vis)\n",
    "            ax[2].set_title(f\"GROUND TRUTH\\n(Green Circles)\")\n",
    "            for idx, (x, y) in enumerate(valid_gt_points):\n",
    "                ax[2].scatter(x, y, c='green', s=150, marker='o', facecolors='none', linewidth=3)\n",
    "                ax[2].text(x+5, y+5, str(idx+1), color='white', fontsize=12, fontweight='bold')\n",
    "\n",
    "            # Cleanup plot\n",
    "            for a in ax: a.axis('off')\n",
    "            \n",
    "            # Salvataggio\n",
    "            save_path = os.path.join(output_dir, f\"Comparison_{category}_ID{i}.png\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"--> [SUCCESS] Saved comparison for {category} (Image ID: {i})\")\n",
    "            \n",
    "            # Segna categoria come completata\n",
    "            category_done[category] = True\n",
    "        \n",
    "        # Check if all classes are done\n",
    "        if all(category_done.values()):\n",
    "            print(\"\\nGenerated all requested images. Exiting.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM94XQGmSqU+CbzhlwN85hS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
