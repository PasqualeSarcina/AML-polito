{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y8AKrNt5lxaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' estratto con successo nella directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
      "Contenuti della directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted':\n",
      "['SPair-71k']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_file_path = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' \n",
    "extract_dir = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
    "\n",
    "# Crea la directory di estrazione se non esiste\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Estrai il file ZIP solo se esiste\n",
    "if os.path.exists(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"File '{zip_file_path}' estratto con successo nella directory '{extract_dir}'\")\n",
    "    print(f\"Contenuti della directory '{extract_dir}':\\n{os.listdir(extract_dir)}\")\n",
    "else:\n",
    "    print(f\"File zip '{zip_file_path}' non trovato. Assicurati che il dataset sia estratto in '{extract_dir}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "honcpimEq_B2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Set...\n",
      "Loading Validation Set...\n",
      "Testing batches...\n",
      "Train Batch Images: torch.Size([4, 3, 518, 518])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Batch Images:   torch.Size([4, 3, 518, 518])\n",
      "Dataset setup complete. Ready for Training Loop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- 1. Define the Augmentation Pipeline ---\n",
    "def get_transforms(mode='train', img_size=518):\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            # Geometric Augmentations (Hard - Moves Keypoints)\n",
    "            A.Resize(height=img_size, width=img_size), # Force DINOv2 size\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            \n",
    "            # Pixel Augmentations (Safe - Colors only)\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.GaussianBlur(p=0.1),\n",
    "            \n",
    "            # Normalization & Conversion\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(), # Converts to (C, H, W)\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "    \n",
    "    else:\n",
    "        # Validation/Test: Only Resize & Normalize\n",
    "        return A.Compose([\n",
    "            A.Resize(height=img_size, width=img_size),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "# --- 2. Simple Image Reader ---\n",
    "def read_img(path):\n",
    "    # Keep as HWC (Standard for Albumentations)\n",
    "    # Do not transpose or convert to Tensor here yet\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "    return img\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n",
    "        self.datatype = datatype\n",
    "        self.pck_alpha = pck_alpha\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = [x for x in self.ann_files if x] # Remove empty strings\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        \n",
    "        # Initialize the Transform Pipeline\n",
    "        mode = 'train' if datatype == 'trn' else 'test'\n",
    "        self.transform = get_transforms(mode=mode, img_size=518)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_line = self.ann_files[idx]\n",
    "        ann_filename = raw_line.replace(':', '_')\n",
    "        ann_file = ann_filename + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_path = os.path.join(self.image_path, category, annotation['src_imname'])\n",
    "        trg_path = os.path.join(self.image_path, category, annotation['trg_imname'])\n",
    "\n",
    "        # 1. Load Images\n",
    "        src_img_raw = read_img(src_path)\n",
    "        trg_img_raw = read_img(trg_path)\n",
    "\n",
    "        # 2. Get Keypoints\n",
    "        src_kps = np.array(annotation['src_kps']).astype(np.float32)\n",
    "        trg_kps = np.array(annotation['trg_kps']).astype(np.float32)\n",
    "\n",
    "        # 3. Apply Augmentations\n",
    "        src_aug = self.transform(image=src_img_raw, keypoints=src_kps)\n",
    "        src_tensor = src_aug['image']\n",
    "        src_kps_aug = np.array(src_aug['keypoints'])\n",
    "        \n",
    "        trg_aug = self.transform(image=trg_img_raw, keypoints=trg_kps)\n",
    "        trg_tensor = trg_aug['image']\n",
    "        trg_kps_aug = np.array(trg_aug['keypoints'])\n",
    "\n",
    "        # ==========================================================\n",
    "        # ‚ö†Ô∏è CRITICAL FIX: PADDING LOGIC (Prevents Stack Error)\n",
    "        # ==========================================================\n",
    "        # We enforce a fixed size of 40 points per image.\n",
    "        MAX_KPS = 40 \n",
    "        \n",
    "        # Create empty containers filled with zeros (Shape: [40, 2])\n",
    "        src_kps_padded = np.zeros((MAX_KPS, 2), dtype=np.float32)\n",
    "        trg_kps_padded = np.zeros((MAX_KPS, 2), dtype=np.float32)\n",
    "        \n",
    "        # Get the actual number of points (limit to 40 just in case)\n",
    "        n_src = min(len(src_kps_aug), MAX_KPS)\n",
    "        n_trg = min(len(trg_kps_aug), MAX_KPS)\n",
    "        \n",
    "        # Copy the real points into the empty container\n",
    "        if n_src > 0:\n",
    "            src_kps_padded[:n_src] = src_kps_aug[:n_src]\n",
    "        if n_trg > 0:\n",
    "            trg_kps_padded[:n_trg] = trg_kps_aug[:n_trg]\n",
    "\n",
    "        # Check which points are inside the image (Visibility)\n",
    "        src_vis = self._check_visibility(src_kps_padded, 518, 518)\n",
    "        trg_vis = self._check_visibility(trg_kps_padded, 518, 518)\n",
    "        \n",
    "        # Create the Valid Mask\n",
    "        # A point is valid ONLY if:\n",
    "        # 1. It existed in the original file (index < n_src)\n",
    "        # 2. It is still inside the image boundaries (vis=1)\n",
    "        valid_mask = np.zeros(MAX_KPS, dtype=np.float32)\n",
    "        \n",
    "        # We assume points are ordered pairs (1st src matches 1st trg)\n",
    "        # So we only mark as valid if BOTH exist and are visible\n",
    "        common_len = min(n_src, n_trg)\n",
    "        valid_mask[:common_len] = src_vis[:common_len] * trg_vis[:common_len]\n",
    "        # ==========================================================\n",
    "\n",
    "        pck_threshold = 518 * self.pck_alpha\n",
    "\n",
    "        sample = {\n",
    "            'pair_id': annotation['pair_id'],\n",
    "            'src_img': src_tensor,\n",
    "            'trg_img': trg_tensor,\n",
    "            \n",
    "            # Now these are ALWAYS [40, 2], so PyTorch won't crash!\n",
    "            'src_kps': torch.from_numpy(src_kps_padded).float(), \n",
    "            'trg_kps': torch.from_numpy(trg_kps_padded).float(), \n",
    "            'valid_mask': torch.from_numpy(valid_mask).float(), \n",
    "            \n",
    "            'pck_threshold': pck_threshold,\n",
    "            'category': category\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _check_visibility(self, kps, h, w):\n",
    "        \"\"\"Returns a binary mask (N,) where 1=visible, 0=out of bounds\"\"\"\n",
    "        # kps is shape (N, 2) -> (x, y)\n",
    "        x = kps[:, 0]\n",
    "        y = kps[:, 1]\n",
    "        \n",
    "        # Check boundaries\n",
    "        vis_x = (x >= 0) & (x < w)\n",
    "        vis_y = (y >= 0) & (y < h)\n",
    "        return (vis_x & vis_y).astype(np.float32)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Update this path to your actual path\n",
    "    base_dir = r\"C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted\\SPair-71k\\SPair-71k\"    \n",
    "    \n",
    "    pair_ann_path = os.path.join(base_dir, 'PairAnnotation')\n",
    "    layout_path = os.path.join(base_dir, 'Layout')\n",
    "    image_path = os.path.join(base_dir, 'JPEGImages')\n",
    "\n",
    "    # Check paths\n",
    "    if os.path.exists(base_dir):\n",
    "        \n",
    "        # --- 1. Load TRAINING Set ---\n",
    "        print(\"Loading Training Set...\")\n",
    "        trn_dataset = SPairDataset(\n",
    "            pair_ann_path, layout_path, image_path, \n",
    "            dataset_size='large', pck_alpha=0.05, \n",
    "            datatype='trn'  # <--- Loads from trn.txt\n",
    "        )\n",
    "        # SHUFFLE = TRUE for training (important for learning)\n",
    "        trn_loader = DataLoader(trn_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "        # --- 2. Load VALIDATION Set ---\n",
    "        print(\"Loading Validation Set...\")\n",
    "        val_dataset = SPairDataset(\n",
    "            pair_ann_path, layout_path, image_path, \n",
    "            dataset_size='large', pck_alpha=0.05, \n",
    "            datatype='val'  # <--- Loads from val.txt\n",
    "        )\n",
    "        # SHUFFLE = FALSE for validation (keep order stable)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "        \n",
    "        # --- 3. Test Loading ---\n",
    "        print(\"Testing batches...\")\n",
    "        \n",
    "        # Grab a training batch\n",
    "        trn_batch = next(iter(trn_loader))\n",
    "        print(f\"Train Batch Images: {trn_batch['src_img'].shape}\")\n",
    "        \n",
    "        # Grab a validation batch\n",
    "        val_batch = next(iter(val_loader))\n",
    "        print(f\"Val Batch Images:   {val_batch['src_img'].shape}\")\n",
    "        \n",
    "        print(\"Dataset setup complete. Ready for Training Loop.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Path not found: {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Coarse Grid Search (Including Prof's Suggestions) ---\n",
      "\n",
      ">>> Training Config: LR=5e-05, WD=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter 0: Loss 3.5205\n",
      "  Iter 25: Loss 2.7998\n",
      "  Validating...\n",
      "  >> Validation PCK: 59.30%\n",
      "\n",
      ">>> Training Config: LR=5e-05, WD=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter 0: Loss 3.7596\n",
      "  Iter 25: Loss 3.2685\n",
      "  Validating...\n",
      "  >> Validation PCK: 63.87%\n",
      "\n",
      ">>> Training Config: LR=5e-05, WD=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter 0: Loss 4.2249\n",
      "  Iter 25: Loss 3.6745\n",
      "  Validating...\n",
      "  >> Validation PCK: 63.26%\n",
      "\n",
      ">>> Training Config: LR=1e-05, WD=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter 0: Loss 4.9820\n",
      "  Iter 25: Loss 2.9578\n",
      "  Validating...\n",
      "  >> Validation PCK: 57.01%\n",
      "\n",
      ">>> Training Config: LR=1e-05, WD=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter 0: Loss 3.3437\n",
      "  Iter 25: Loss 3.8029\n",
      "  Validating...\n",
      "  >> Validation PCK: 56.71%\n",
      "\n",
      ">>> Training Config: LR=1e-05, WD=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iter 0: Loss 4.4358\n",
      "  Iter 25: Loss 3.6930\n",
      "  Validating...\n",
      "  >> Validation PCK: 55.95%\n",
      "\n",
      "===========================================\n",
      "üèÜ Best Config: LR=5e-05, WD=1e-05\n",
      "üèÜ Best Validation Accuracy: 63.87%\n",
      "===========================================\n",
      "Now perform Step 5 (Long Training) using these exact values!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. HELPER FUNCTIONS (Need these for Validation)\n",
    "# ==========================================\n",
    "def get_patch_indices(keypoints, img_size=518, patch_size=14):\n",
    "    grid_w = img_size // patch_size\n",
    "    grid_x = (keypoints[:, :, 0] / patch_size).long().clamp(0, grid_w-1)\n",
    "    grid_y = (keypoints[:, :, 1] / patch_size).long().clamp(0, grid_w-1)\n",
    "    flat_indices = grid_y * grid_w + grid_x \n",
    "    return flat_indices\n",
    "\n",
    "def extract_features_at_indices(features, indices):\n",
    "    B, N_patches, Dim = features.shape\n",
    "    B, N_kps = indices.shape\n",
    "    indices_expanded = indices.unsqueeze(-1).expand(-1, -1, Dim)\n",
    "    kps_features = torch.gather(features, 1, indices_expanded)\n",
    "    return kps_features\n",
    "\n",
    "def contrastive_loss(feat_src_kps, feat_trg_all, trg_kps_indices, mask, temp=0.1):\n",
    "    feat_src_kps = F.normalize(feat_src_kps, dim=-1)\n",
    "    feat_trg_all = F.normalize(feat_trg_all, dim=-1)\n",
    "    logits = torch.bmm(feat_src_kps, feat_trg_all.transpose(1, 2)) / temp\n",
    "    valid = mask.bool()\n",
    "    logits_valid = logits[valid]\n",
    "    targets_valid = trg_kps_indices[valid]\n",
    "    loss = F.cross_entropy(logits_valid, targets_valid)\n",
    "    return loss\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct_points = 0\n",
    "    total_points = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            # Limit validation to 20 batches to be fast\n",
    "            if i > 20: break \n",
    "            \n",
    "            src = batch['src_img'].to(device)\n",
    "            trg = batch['trg_img'].to(device)\n",
    "            src_kps = batch['src_kps'].to(device)\n",
    "            trg_kps = batch['trg_kps'].to(device)\n",
    "            mask    = batch['valid_mask'].to(device)\n",
    "            thresholds = batch['pck_threshold'].to(device)\n",
    "\n",
    "            dict_A = model.forward_features(src)\n",
    "            dict_B = model.forward_features(trg)\n",
    "            feat_A_all = dict_A['x_norm_patchtokens']\n",
    "            feat_B_all = dict_B['x_norm_patchtokens']\n",
    "            \n",
    "            src_indices = get_patch_indices(src_kps)\n",
    "            feat_A_kps = extract_features_at_indices(feat_A_all, src_indices)\n",
    "            \n",
    "            sim = torch.bmm(F.normalize(feat_A_kps, dim=-1), \n",
    "                            F.normalize(feat_B_all, dim=-1).transpose(1, 2))\n",
    "            \n",
    "            best_match_idx = torch.argmax(sim, dim=2)\n",
    "            \n",
    "            grid_w = 37\n",
    "            pred_y = best_match_idx // grid_w\n",
    "            pred_x = best_match_idx % grid_w\n",
    "            pred_x = pred_x * 14 + 7\n",
    "            pred_y = pred_y * 14 + 7\n",
    "            \n",
    "            dist = torch.sqrt((pred_x - trg_kps[:, :, 0])**2 + (pred_y - trg_kps[:, :, 1])**2)\n",
    "            thresh_expanded = thresholds.unsqueeze(1).expand(-1, 40)\n",
    "            is_correct = (dist < thresh_expanded) & (mask.bool())\n",
    "            \n",
    "            correct_points += is_correct.sum().item()\n",
    "            total_points += mask.sum().item()\n",
    "            \n",
    "    return correct_points / (total_points + 1e-6)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Step 4: Coarse Grid Search\n",
    "# ==========================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Step 4: Coarse Grid Search ---\")\n",
    "    \n",
    "    # Grid Settings: Combining Best LRs with Prof's Weight Decays\n",
    "    grid_configs = [\n",
    "        # Set A: Using your Best LR (5e-5)\n",
    "        {'lr': 5e-5, 'wd': 1e-4}, \n",
    "        {'lr': 5e-5, 'wd': 1e-5}, # <--- Added\n",
    "        {'lr': 5e-5, 'wd': 0.0},  \n",
    "        \n",
    "        # Set B: Using the Safer LR (1e-5)\n",
    "        {'lr': 1e-5, 'wd': 1e-4}, \n",
    "        {'lr': 1e-5, 'wd': 1e-5}, # <--- Added\n",
    "        {'lr': 1e-5, 'wd': 0.0},  \n",
    "    ]\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Ensure Loaders exist\n",
    "    try:\n",
    "        _ = len(trn_loader)\n",
    "        _ = len(val_loader)\n",
    "    except NameError:\n",
    "        print(\"Error: Loaders not defined. Run Dataset setup first.\")\n",
    "        exit()\n",
    "\n",
    "    best_acc = 0\n",
    "    best_config = None\n",
    "\n",
    "    for config in grid_configs:\n",
    "        lr = config['lr']\n",
    "        wd = config['wd']\n",
    "        print(f\"\\n>>> Training Config: LR={lr}, WD={wd}\")\n",
    "        \n",
    "        # 1. Reset Model\n",
    "        model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').to(device)\n",
    "        for param in model.parameters(): param.requires_grad = False\n",
    "        for block in model.blocks[-2:]: \n",
    "            for param in block.parameters(): param.requires_grad = True\n",
    "            \n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                                lr=lr, weight_decay=wd)\n",
    "        \n",
    "        # 2. Train (Short run: 50 batches)\n",
    "        model.train()\n",
    "        for i, batch in enumerate(trn_loader):\n",
    "            if i >= 50: break # Stop early to save time\n",
    "            \n",
    "            src = batch['src_img'].to(device)\n",
    "            trg = batch['trg_img'].to(device)\n",
    "            src_kps = batch['src_kps'].to(device)\n",
    "            trg_kps = batch['trg_kps'].to(device)\n",
    "            mask    = batch['valid_mask'].to(device)\n",
    "            \n",
    "            src_indices = get_patch_indices(src_kps)\n",
    "            trg_indices = get_patch_indices(trg_kps)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            dict_A = model.forward_features(src)\n",
    "            dict_B = model.forward_features(trg)\n",
    "            feat_A_kps = extract_features_at_indices(dict_A['x_norm_patchtokens'], src_indices)\n",
    "            feat_B_all = dict_B['x_norm_patchtokens']\n",
    "            \n",
    "            loss = contrastive_loss(feat_A_kps, feat_B_all, trg_indices, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 25 == 0: print(f\"  Iter {i}: Loss {loss.item():.4f}\")\n",
    "\n",
    "        # 3. Validate\n",
    "        print(\"  Validating...\")\n",
    "        acc = validate_model(model, val_loader, device)\n",
    "        print(f\"  >> Validation PCK: {acc*100:.2f}%\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_config = config\n",
    "\n",
    "    print(\"\\n===========================================\")\n",
    "    print(f\"üèÜ Best Config: LR={best_config['lr']}, WD={best_config['wd']}\")\n",
    "    print(f\"üèÜ Best Validation Accuracy: {best_acc*100:.2f}%\")\n",
    "    print(\"===========================================\")\n",
    "    print(\"Now perform Step 5 (Long Training) using these exact values!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM94XQGmSqU+CbzhlwN85hS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
