{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:20:09.506433102Z",
     "start_time": "2026-01-04T22:20:07.418684676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.normalization import ImageNetNorm\n",
    "import torch\n",
    "from data.spair import SPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "using_colab = False\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    !wget -P ./AML-polito/dataset/ \"https://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    !tar -xvzf ./AML-polito/dataset/SPair-71k.tar.gz -C ./AML-polito/dataset/\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "dataset_size = 'small'\n",
    "\n",
    "# Load dataset and construct dataloader\n",
    "if os.path.exists(dataset_dir):\n",
    "    trn_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='trn')\n",
    "    val_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='val')\n",
    "    test_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='test')\n",
    "\n",
    "    trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1, collate_fn=collate_single)\n",
    "    print(\"Dataset loeaded\")\n",
    "else:\n",
    "    raise IOError(f\"Cannot find dataset files in '{dataset_dir}'.\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loeaded\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:20:10.504292661Z",
     "start_time": "2026-01-04T22:20:09.509608159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# SAM initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "\n",
    "print(f\"SAM '{selected_model}' loaded. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM 'vit_b' loaded. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:20:10.602006634Z",
     "start_time": "2026-01-04T22:20:10.551634894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale → lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "c0b1d5b0579cac9c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:20:10.651151611Z",
     "start_time": "2026-01-04T22:20:10.603296255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        activation[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "# Hook registration\n",
    "target_layers = [2, 5, 8, 11]  # Layer to extract (SAM has 12 layers)\n",
    "for i in target_layers:\n",
    "    predictor.model.image_encoder.blocks[i].register_forward_hook(get_activation(f'layer_{i}'))"
   ],
   "id": "3c087326ad2f11de",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:28:43.089002575Z",
     "start_time": "2026-01-04T22:20:10.652573511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_correspondence import hard_argmax\n",
    "from utils.utils_results import *\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dizionario finale: chiave = layer, valore = lista (stessa struttura di results originale)\n",
    "all_results = {li: [] for li in target_layers}\n",
    "\n",
    "# (opzionale) qualitativo per layer (solo sul primo batch)\n",
    "qualitative_by_layer = {\n",
    "    li: {\n",
    "        \"src_img\": None,\n",
    "        \"trg_img\": None,\n",
    "        \"src_kps\": [],\n",
    "        \"trg_gt_kps\": [],\n",
    "        \"trg_pred_kps\": []\n",
    "    }\n",
    "    for li in target_layers\n",
    "}\n",
    "\n",
    "max_images = 300\n",
    "data = islice(test_dataloader, max_images)\n",
    "size = max_images\n",
    "if using_colab:\n",
    "    data = test_dataloader\n",
    "    size = len(test_dataloader)\n",
    "\n",
    "img_enc = predictor.model.image_encoder  # contiene .neck\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, batch in enumerate(tqdm(data, total=size, desc=f\"Elaborazione con SAM {selected_model}\")):\n",
    "\n",
    "        category = batch[\"category\"]\n",
    "        src_img = batch[\"src_img\"].to(device).unsqueeze(0)  # [1,3,Hs,Ws]\n",
    "        trg_img = batch[\"trg_img\"].to(device).unsqueeze(0)  # [1,3,Ht,Wt]\n",
    "        orig_size_src = tuple(batch[\"src_imsize\"][1:])  # (Hs, Ws)\n",
    "        orig_size_trg = tuple(batch[\"trg_imsize\"][1:])  # (Ht, Wt)\n",
    "\n",
    "        src_resized = predictor.transform.apply_image_torch(src_img)  # [1,3,Hs',Ws']\n",
    "        trg_resized = predictor.transform.apply_image_torch(trg_img)  # [1,3,Ht',Wt']\n",
    "\n",
    "        # --- 1) Forward sorgente (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(src_resized, orig_size_src)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        src_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 2) Forward target (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(trg_resized, orig_size_trg)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        trg_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 3) Keypoints & metadata ---\n",
    "        src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "        pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "        pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "        pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "        # --- regione valida target (per evitare padding) ---\n",
    "        H_prime, W_prime = trg_resized.shape[-2:]\n",
    "        hv_t = math.ceil(H_prime / PATCH)\n",
    "        wv_t = math.ceil(W_prime / PATCH)\n",
    "\n",
    "        N_kps = src_kps.shape[0]\n",
    "\n",
    "        # (opzionale) salva immagini una sola volta per layer quando iter==0\n",
    "        if iter == 0:\n",
    "            for li in target_layers:\n",
    "                qualitative_by_layer[li][\"src_img\"] = batch[\"src_img\"]\n",
    "                qualitative_by_layer[li][\"trg_img\"] = batch[\"trg_img\"]\n",
    "\n",
    "        # ===== Loop sui layer: riuso activation, NON rifaccio forward =====\n",
    "        for selected_layer in target_layers:\n",
    "\n",
    "            # Hook output: [1,64,64,768] (NHWC)\n",
    "            src_hook = src_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "            trg_hook = trg_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "\n",
    "            # NHWC -> NCHW : [1,768,64,64]\n",
    "            src_feat = src_hook.permute(0, 3, 1, 2).contiguous()\n",
    "            trg_feat = trg_hook.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            # ======= QUI la scelta pre-neck / post-neck =======\n",
    "            if selected_layer <= 4:  # \"iniziale\" (puoi cambiare soglia: 3/4/5)\n",
    "                # PRE-NECK: uso direttamente i 768 canali\n",
    "                # (facoltativo ma consigliato) L2-normalizzazione per cosine\n",
    "                src_emb = F.normalize(src_feat, dim=1)[0]  # [768,64,64]\n",
    "                trg_emb = F.normalize(trg_feat, dim=1)[0]  # [768,64,64]\n",
    "            else:\n",
    "                # POST-NECK: porto a 256 canali come output standard SAM\n",
    "                src_emb = img_enc.neck(src_feat)[0]  # [256,64,64]\n",
    "                trg_emb = img_enc.neck(trg_feat)[0]  # [256,64,64]\n",
    "                # (facoltativo) normalizza anche qui se vuoi massima coerenza\n",
    "                # src_emb = F.normalize(src_emb, dim=0)\n",
    "                # trg_emb = F.normalize(trg_emb, dim=0)\n",
    "            # ================================================\n",
    "\n",
    "            C_ft = trg_emb.shape[0]\n",
    "\n",
    "            trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "            trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "            distances_this_image = []\n",
    "\n",
    "            for i in range(N_kps):\n",
    "                src_keypoint = src_kps[i].unsqueeze(0)  # [1,2] (x,y)\n",
    "                trg_keypoint = trg_kps[i]  # [2]   (x,y)\n",
    "\n",
    "                if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                    continue\n",
    "\n",
    "                # originale src -> feature src\n",
    "                x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "                # feature vector sorgente\n",
    "                src_vec = src_emb[:, y_idx, x_idx]  # [256]\n",
    "\n",
    "                # cosine similarity con tutte le posizioni valide del target\n",
    "                sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "                #max_idx = torch.argmax(sim).item()\n",
    "                #y_idx_t = max_idx // wv_t\n",
    "                #x_idx_t = max_idx % wv_t\n",
    "\n",
    "                sim2d = sim.view(hv_t, wv_t)\n",
    "                x_idx_t, y_idx_t = hard_argmax(sim2d)\n",
    "\n",
    "                # feature target -> pixel originali target\n",
    "                x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "                if iter == 0:\n",
    "                    qualitative_by_layer[selected_layer][\"src_kps\"].append(src_keypoint.squeeze(0).tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_gt_kps\"].append(trg_keypoint.tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_pred_kps\"].append([x_pred, y_pred])\n",
    "\n",
    "                dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "                distances_this_image.append(dist)\n",
    "\n",
    "            # Append con struttura IDENTICA all'originale, ma salvata per-layer\n",
    "            all_results[selected_layer].append(\n",
    "                CorrespondenceResult(\n",
    "                    category=category,\n",
    "                    distances=distances_this_image,\n",
    "                    pck_threshold_0_05=pck_thr_0_05,\n",
    "                    pck_threshold_0_1=pck_thr_0_1,\n",
    "                    pck_threshold_0_2=pck_thr_0_2\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # cleanup per-layer\n",
    "            del src_hook, trg_hook, src_feat, trg_feat, src_emb, trg_emb, trg_valid, trg_flat, src_vec, sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # cleanup per-batch\n",
    "        predictor.reset_image()\n",
    "        torch.cuda.empty_cache()"
   ],
   "id": "c888ec88287af227",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b: 100%|██████████| 300/300 [08:32<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:28:43.412721818Z",
     "start_time": "2026-01-04T22:28:43.308318436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute and print results per layer\n",
    "for li in target_layers:\n",
    "    print(f\"Layer {li}\")\n",
    "    correct = compute_correct_per_category(all_results[li])\n",
    "    compute_pckt_keypoints(correct)\n",
    "    compute_pckt_images(correct)\n",
    "    print(\"<\" + \"=\" * 50 + \">\")\n",
    "    print(\"\")"
   ],
   "id": "e3a7661228d5944c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2\n",
      "PCK Results per keypoints (%):\n",
      "    Category  PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  4.838710  11.669829  24.667932\n",
      "1    bicycle  1.145038   4.325700  12.595420\n",
      "2       bird  3.703704  10.648148  37.500000\n",
      "3        All  3.229151   8.881226  24.921117\n",
      "PCK per-image (%):\n",
      "    Category  PCK 0.05   PCK 0.1    PCK 0.2\n",
      "0  aeroplane  3.236609  9.174934  20.855081\n",
      "1    bicycle  1.116261  4.065188  12.057737\n",
      "2       bird  3.998316  9.956710  33.333333\n",
      "3        All  2.783729  7.732277  22.082050\n",
      "<==================================================>\n",
      "\n",
      "Layer 5\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane   9.487666  16.508539  30.740038\n",
      "1    bicycle   3.053435   6.361323  17.175573\n",
      "2       bird  19.444444  32.870370  45.370370\n",
      "3        All  10.661849  18.580077  31.095327\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane   7.571696  13.826586  27.785783\n",
      "1    bicycle   3.275834   6.416317  17.057324\n",
      "2       bird  17.752525  30.031265  42.183742\n",
      "3        All   9.533352  16.758056  29.008949\n",
      "<==================================================>\n",
      "\n",
      "Layer 8\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  15.559772  22.106262  35.009488\n",
      "1    bicycle   6.361323  12.086514  20.101781\n",
      "2       bird  24.074074  31.481481  47.685185\n",
      "3        All  15.331723  21.891419  34.265485\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  13.216284  19.334815  32.018058\n",
      "1    bicycle   5.300001  10.437279  17.834093\n",
      "2       bird  22.524050  29.080087  45.971621\n",
      "3        All  13.680112  19.617394  31.941257\n",
      "<==================================================>\n",
      "\n",
      "Layer 11\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  14.326376  23.149905  36.622391\n",
      "1    bicycle   6.870229  13.867684  24.809160\n",
      "2       bird  22.222222  33.796296  50.000000\n",
      "3        All  14.472942  23.604629  37.143850\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  11.897210  19.928235  34.056969\n",
      "1    bicycle   6.168804  12.230924  23.127458\n",
      "2       bird  19.791967  29.975950  46.254209\n",
      "3        All  12.619327  20.711703  34.479545\n",
      "<==================================================>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:28:43.719545742Z",
     "start_time": "2026-01-04T22:28:43.418901939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualization and saving of qualitative results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_to_image(img: torch.Tensor) -> np.ndarray:\n",
    "    img = img.detach().cpu()  # sicurezza\n",
    "    img = img.permute(1, 2, 0)  # [H,W,3]\n",
    "    img = img.numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_keypoints(ax, image, keypoints, color, label=None, marker='o'):\n",
    "    ax.imshow(image)\n",
    "    if len(keypoints) > 0:\n",
    "        xs = [kp[0] for kp in keypoints]\n",
    "        ys = [kp[1] for kp in keypoints]\n",
    "        ax.scatter(xs, ys, c=color, s=40, marker=marker, label=label)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_keypoints_save(\n",
    "        src_img_chw: torch.Tensor,\n",
    "        trg_img_chw: torch.Tensor,\n",
    "        qualitative_result: dict,\n",
    "        dpi: int = 200\n",
    ") -> None:\n",
    "    # --- cartella output ---\n",
    "    save_dir = os.path.join(base_dir, \"qualitative-results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(save_dir, f\"{selected_model}_src.png\")\n",
    "    gt_path = os.path.join(save_dir, f\"{selected_model}_trg_gt.png\")\n",
    "    pr_path = os.path.join(save_dir, f\"{selected_model}_trg_pred.png\")\n",
    "\n",
    "    # --- immagini ---\n",
    "    src_img = tensor_to_image(src_img_chw)\n",
    "    trg_img = tensor_to_image(trg_img_chw)\n",
    "\n",
    "    src_kps = qualitative_result[\"src_kps\"]  # [[x,y],...]\n",
    "    trg_gt = qualitative_result[\"trg_gt_kps\"]  # [[x,y],...]\n",
    "    trg_pr = qualitative_result[\"trg_pred_kps\"]  # [[x,y],...]\n",
    "\n",
    "    n = min(len(src_kps), len(trg_gt), len(trg_pr))\n",
    "    cmap = plt.get_cmap(\"tab10\", max(n, 1))\n",
    "\n",
    "    # ---------- 1) SOURCE ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(src_img)\n",
    "    ax.set_title(\"Source\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xs, ys = src_kps[i]\n",
    "        ax.scatter(xs, ys, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xs + 6, ys + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(src_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 2) TARGET GT ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target GT\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        ax.scatter(xg, yg, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xg + 6, yg + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gt_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 3) TARGET PRED ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target Pred\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        xp, yp = trg_pr[i]\n",
    "\n",
    "        # pred: pallino vuoto\n",
    "        ax.scatter(xp, yp, s=60, color=c, linewidths=2, marker=\"o\")\n",
    "        ax.text(xp + 6, yp + 6, str(i), color=c, fontsize=10)\n",
    "\n",
    "        # (opzionale) linea di errore GT->Pred\n",
    "        ax.plot([xg, xp], [yg, yp], color=c, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pr_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "plot_keypoints_save(\n",
    "    src_img_chw=batch[\"src_img\"],\n",
    "    trg_img_chw=batch[\"trg_img\"],\n",
    "    qualitative_result=qualitative_result\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a0bcd406ee3f4f98",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qualitative_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 100\u001B[39m\n\u001B[32m     93\u001B[39m     plt.savefig(pr_path, dpi=dpi, bbox_inches=\u001B[33m\"\u001B[39m\u001B[33mtight\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     94\u001B[39m     plt.close(fig)\n\u001B[32m     97\u001B[39m plot_keypoints_save(\n\u001B[32m     98\u001B[39m     src_img_chw=batch[\u001B[33m\"\u001B[39m\u001B[33msrc_img\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     99\u001B[39m     trg_img_chw=batch[\u001B[33m\"\u001B[39m\u001B[33mtrg_img\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m     qualitative_result=\u001B[43mqualitative_result\u001B[49m\n\u001B[32m    101\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'qualitative_result' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "12deb7cd23ce024",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
