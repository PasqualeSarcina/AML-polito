{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:38.917616384Z",
     "start_time": "2025-12-29T19:30:37.882855187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import TypedDict, Tuple, Any\n",
    "import torch\n"
   ],
   "id": "8bdfc0a2cd090d60",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:39.864368118Z",
     "start_time": "2025-12-29T19:30:38.918317451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from data.spair import SPairDataset\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "using_colab = False\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    !wget -P ./AML-polito/dataset/ \"https://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    !tar -xvzf ./AML-polito/dataset/SPair-71k.tar.gz -C ./AML-polito/dataset/\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "dataset_size = 'large'\n",
    "#pck_alpha = 0.05\n",
    "\n",
    "# Verifica che i percorsi esistano prima di creare il dataset\n",
    "if os.path.exists(dataset_dir):\n",
    "    trn_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='trn')\n",
    "    val_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='val')\n",
    "    test_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='test')\n",
    "\n",
    "    trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1, collate_fn=collate_single)\n",
    "    print(\"Dataset caricati correttamente.\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Errore: Impossibile trovare i percorsi del dataset in '{dataset_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricati correttamente.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:40.924603789Z",
     "start_time": "2025-12-29T19:30:39.924146542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# inizializzazione SAM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === 1) Carico SAM ===\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# parametri utili\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "#PATCH = IMG_SIZE // 64  # 16\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "print(f\"SAM modello '{selected_model}' caricato. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM modello 'vit_b' caricato. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:41.022980565Z",
     "start_time": "2025-12-29T19:30:40.973760818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale → lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "c0b1d5b0579cac9c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:41.192267560Z",
     "start_time": "2025-12-29T19:30:41.023988515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PCK@T per keypoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_pckt_keypoints(category_results: dict):\n",
    "    rows_keypoints = []\n",
    "\n",
    "    for cat, stats_list in category_results.items():\n",
    "        tot_keypoints = sum(s[\"num_keypoints\"] for s in stats_list)\n",
    "        tot_0_05 = sum(s[\"correct_0_05\"] for s in stats_list)\n",
    "        tot_0_1 = sum(s[\"correct_0_1\"] for s in stats_list)\n",
    "        tot_0_2 = sum(s[\"correct_0_2\"] for s in stats_list)\n",
    "\n",
    "        pck_0_05 = tot_0_05 / tot_keypoints if tot_keypoints > 0 else np.nan\n",
    "        pck_0_1 = tot_0_1 / tot_keypoints if tot_keypoints > 0 else np.nan\n",
    "        pck_0_2 = tot_0_2 / tot_keypoints if tot_keypoints > 0 else np.nan\n",
    "\n",
    "        rows_keypoints.append({\n",
    "            \"Category\": cat,\n",
    "            \"PCK 0.05\": pck_0_05 * 100,\n",
    "            \"PCK 0.1\": pck_0_1 * 100,\n",
    "            \"PCK 0.2\": pck_0_2 * 100,\n",
    "        })\n",
    "\n",
    "    df_keypoints = pd.DataFrame(rows_keypoints).sort_values(\"Category\")\n",
    "\n",
    "    #  \"All\" = macro-average on categories\n",
    "    mean_row_kp = {\n",
    "        \"Category\": \"All\",\n",
    "        \"PCK 0.05\": df_keypoints[\"PCK 0.05\"].mean(skipna=True),\n",
    "        \"PCK 0.1\": df_keypoints[\"PCK 0.1\"].mean(skipna=True),\n",
    "        \"PCK 0.2\": df_keypoints[\"PCK 0.2\"].mean(skipna=True),\n",
    "    }\n",
    "\n",
    "    df_keypoints = pd.concat(\n",
    "        [df_keypoints, pd.DataFrame([mean_row_kp])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(\"PCK Results per keypoints (%):\")\n",
    "    print(df_keypoints)"
   ],
   "id": "9ced78b272827f8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:41.240535789Z",
     "start_time": "2025-12-29T19:30:41.193516811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def compute_correct_per_category(results: List[Any]) -> dict:\n",
    "    category_results = {}\n",
    "\n",
    "    for res in results:\n",
    "        cat = res[\"category\"]\n",
    "        if cat not in category_results:\n",
    "            category_results[cat] = []\n",
    "\n",
    "        dists_list = res[\"distances\"]\n",
    "        num_keypoints = len(dists_list)\n",
    "        dists = torch.tensor(dists_list)\n",
    "\n",
    "        thr_0_05 = res[\"pck_threshold_0_05\"]\n",
    "        thr_0_1 = res[\"pck_threshold_0_1\"]\n",
    "        thr_0_2 = res[\"pck_threshold_0_2\"]\n",
    "\n",
    "        correct_0_05 = (dists <= thr_0_05).sum().item()\n",
    "        correct_0_1 = (dists <= thr_0_1).sum().item()\n",
    "        correct_0_2 = (dists <= thr_0_2).sum().item()\n",
    "\n",
    "        category_results[cat].append({\n",
    "            \"correct_0_05\": correct_0_05,\n",
    "            \"correct_0_1\": correct_0_1,\n",
    "            \"correct_0_2\": correct_0_2,\n",
    "            \"num_keypoints\": num_keypoints\n",
    "        })\n",
    "    return category_results"
   ],
   "id": "1b7f9dd166701a30",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:41.289473936Z",
     "start_time": "2025-12-29T19:30:41.241610926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PCK@T per image\n",
    "def compute_pckt_images(category_results: dict):\n",
    "    rows_images = []\n",
    "\n",
    "    for cat, stats_list in category_results.items():\n",
    "\n",
    "        pck_imgs_0_05 = []\n",
    "        pck_imgs_0_1 = []\n",
    "        pck_imgs_0_2 = []\n",
    "\n",
    "        for s in stats_list:\n",
    "            if s[\"num_keypoints\"] == 0:\n",
    "                continue\n",
    "\n",
    "            pck_imgs_0_05.append(s[\"correct_0_05\"] / s[\"num_keypoints\"])\n",
    "            pck_imgs_0_1.append(s[\"correct_0_1\"] / s[\"num_keypoints\"])\n",
    "            pck_imgs_0_2.append(s[\"correct_0_2\"] / s[\"num_keypoints\"])\n",
    "\n",
    "        rows_images.append({\n",
    "            \"Category\": cat,\n",
    "            \"PCK 0.05\": np.mean(pck_imgs_0_05) * 100 if pck_imgs_0_05 else np.nan,\n",
    "            \"PCK 0.1\": np.mean(pck_imgs_0_1) * 100 if pck_imgs_0_1 else np.nan,\n",
    "            \"PCK 0.2\": np.mean(pck_imgs_0_2) * 100 if pck_imgs_0_2 else np.nan,\n",
    "        })\n",
    "\n",
    "    df_image = pd.DataFrame(rows_images).sort_values(\"Category\")\n",
    "\n",
    "    #  \"All\" = macro-average on categories\n",
    "    all_row = {\n",
    "        \"Category\": \"All\",\n",
    "        \"PCK 0.05\": df_image[\"PCK 0.05\"].mean(skipna=True),\n",
    "        \"PCK 0.1\": df_image[\"PCK 0.1\"].mean(skipna=True),\n",
    "        \"PCK 0.2\": df_image[\"PCK 0.2\"].mean(skipna=True),\n",
    "    }\n",
    "\n",
    "    df_image = pd.concat([df_image, pd.DataFrame([all_row])], ignore_index=True)\n",
    "\n",
    "    print(\"PCK per-image (%):\")\n",
    "    print(df_image)"
   ],
   "id": "b29c899ffe282bcf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:30:41.337041726Z",
     "start_time": "2025-12-29T19:30:41.290753987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        activation[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "# Registrazione hook (fallo UNA volta sola prima del loop)\n",
    "# Scegliamo alcuni indici. Per ViT-B (base) ci sono 12 blocchi (0-11).\n",
    "target_layers = [2, 5, 8, 11]\n",
    "for i in target_layers:\n",
    "    predictor.model.image_encoder.blocks[i].register_forward_hook(get_activation(f'layer_{i}'))"
   ],
   "id": "3c087326ad2f11de",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T19:37:19.560791516Z",
     "start_time": "2025-12-29T19:37:14.236605615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dizionario finale: chiave = layer, valore = lista (stessa struttura di results originale)\n",
    "all_results = {li: [] for li in target_layers}\n",
    "\n",
    "# (opzionale) qualitativo per layer (solo sul primo batch)\n",
    "qualitative_by_layer = {\n",
    "    li: {\n",
    "        \"src_img\": None,\n",
    "        \"trg_img\": None,\n",
    "        \"src_kps\": [],\n",
    "        \"trg_gt_kps\": [],\n",
    "        \"trg_pred_kps\": []\n",
    "    }\n",
    "    for li in target_layers\n",
    "}\n",
    "\n",
    "max_images = 3\n",
    "data = islice(test_dataloader, max_images)\n",
    "size = max_images\n",
    "if using_colab:\n",
    "    data = test_dataloader\n",
    "    size = len(test_dataloader)\n",
    "\n",
    "img_enc = predictor.model.image_encoder  # contiene .neck\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, batch in enumerate(tqdm(data, total=size, desc=f\"Elaborazione con SAM {selected_model}\")):\n",
    "\n",
    "        category = batch[\"category\"]\n",
    "        src_img = batch[\"src_img\"].to(device).unsqueeze(0)  # [1,3,Hs,Ws]\n",
    "        trg_img = batch[\"trg_img\"].to(device).unsqueeze(0)  # [1,3,Ht,Wt]\n",
    "        orig_size_src = tuple(batch[\"src_imsize\"][1:])  # (Hs, Ws)\n",
    "        orig_size_trg = tuple(batch[\"trg_imsize\"][1:])   # (Ht, Wt)\n",
    "\n",
    "        src_resized = predictor.transform.apply_image_torch(src_img)  # [1,3,Hs',Ws']\n",
    "        trg_resized = predictor.transform.apply_image_torch(trg_img)  # [1,3,Ht',Wt']\n",
    "\n",
    "        # --- 1) Forward sorgente (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(src_resized, orig_size_src)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        src_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 2) Forward target (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(trg_resized, orig_size_trg)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        trg_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 3) Keypoints & metadata ---\n",
    "        src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "        pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "        pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "        pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "        # --- regione valida target (per evitare padding) ---\n",
    "        H_prime, W_prime = trg_resized.shape[-2:]\n",
    "        hv_t = math.ceil(H_prime / PATCH)\n",
    "        wv_t = math.ceil(W_prime / PATCH)\n",
    "\n",
    "        N_kps = src_kps.shape[0]\n",
    "\n",
    "        # (opzionale) salva immagini una sola volta per layer quando iter==0\n",
    "        if iter == 0:\n",
    "            for li in target_layers:\n",
    "                qualitative_by_layer[li][\"src_img\"] = batch[\"src_img\"]\n",
    "                qualitative_by_layer[li][\"trg_img\"] = batch[\"trg_img\"]\n",
    "\n",
    "        # ===== Loop sui layer: riuso activation, NON rifaccio forward =====\n",
    "        for selected_layer in target_layers:\n",
    "\n",
    "            # Hook output: [1,64,64,768] (NHWC)\n",
    "            src_hook = src_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "            trg_hook = trg_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "\n",
    "            # NHWC -> NCHW : [1,768,64,64]\n",
    "            src_feat = src_hook.permute(0, 3, 1, 2).contiguous()\n",
    "            trg_feat = trg_hook.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            # ======= QUI la scelta pre-neck / post-neck =======\n",
    "            if selected_layer <= 4:  # \"iniziale\" (puoi cambiare soglia: 3/4/5)\n",
    "                # PRE-NECK: uso direttamente i 768 canali\n",
    "                # (facoltativo ma consigliato) L2-normalizzazione per cosine\n",
    "                src_emb = F.normalize(src_feat, dim=1)[0]   # [768,64,64]\n",
    "                trg_emb = F.normalize(trg_feat, dim=1)[0]   # [768,64,64]\n",
    "            else:\n",
    "                # POST-NECK: porto a 256 canali come output standard SAM\n",
    "                src_emb = img_enc.neck(src_feat)[0]         # [256,64,64]\n",
    "                trg_emb = img_enc.neck(trg_feat)[0]         # [256,64,64]\n",
    "                # (facoltativo) normalizza anche qui se vuoi massima coerenza\n",
    "                # src_emb = F.normalize(src_emb, dim=0)\n",
    "                # trg_emb = F.normalize(trg_emb, dim=0)\n",
    "            # ================================================\n",
    "\n",
    "            C_ft = trg_emb.shape[0]\n",
    "\n",
    "            trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "            trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "            distances_this_image = []\n",
    "\n",
    "            for i in range(N_kps):\n",
    "                src_keypoint = src_kps[i].unsqueeze(0)  # [1,2] (x,y)\n",
    "                trg_keypoint = trg_kps[i]              # [2]   (x,y)\n",
    "\n",
    "                if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                    continue\n",
    "\n",
    "                # originale src -> feature src\n",
    "                x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "                # feature vector sorgente\n",
    "                src_vec = src_emb[:, y_idx, x_idx]  # [256]\n",
    "\n",
    "                # cosine similarity con tutte le posizioni valide del target\n",
    "                sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "                max_idx = torch.argmax(sim).item()\n",
    "                y_idx_t = max_idx // wv_t\n",
    "                x_idx_t = max_idx % wv_t\n",
    "\n",
    "                # feature target -> pixel originali target\n",
    "                x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "                if iter == 0:\n",
    "                    qualitative_by_layer[selected_layer][\"src_kps\"].append(src_keypoint.squeeze(0).tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_gt_kps\"].append(trg_keypoint.tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_pred_kps\"].append([x_pred, y_pred])\n",
    "\n",
    "                dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "                distances_this_image.append(dist)\n",
    "\n",
    "            # Append con struttura IDENTICA all'originale, ma salvata per-layer\n",
    "            all_results[selected_layer].append({\n",
    "                #\"pair_id\": pair_id,\n",
    "                #\"filename\": filename,\n",
    "                \"category\": category,\n",
    "                \"pck_threshold_0_05\": pck_thr_0_05,\n",
    "                \"pck_threshold_0_1\": pck_thr_0_1,\n",
    "                \"pck_threshold_0_2\": pck_thr_0_2,\n",
    "                \"distances\": distances_this_image\n",
    "            })\n",
    "\n",
    "            # cleanup per-layer\n",
    "            del src_hook, trg_hook, src_feat, trg_feat, src_emb, trg_emb, trg_valid, trg_flat, src_vec, sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # cleanup per-batch\n",
    "        predictor.reset_image()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Elaborazione completata.\")\n",
    "for li in target_layers:\n",
    "    print(f\"Layer {li}\")\n",
    "    correct = compute_correct_per_category(all_results[li])\n",
    "    compute_pckt_keypoints(correct)\n",
    "    compute_pckt_images(correct)\n",
    "    print(\"#\" * 50)\n",
    "    print(\"\")\n"
   ],
   "id": "c888ec88287af227",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b: 100%|██████████| 3/3 [00:05<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elaborazione completata.\n",
      "Layer 2\n",
      "PCK Results per keypoints (%):\n",
      "    Category  PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  7.692308  11.538462  34.615385\n",
      "1        All  7.692308  11.538462  34.615385\n",
      "PCK per-image (%):\n",
      "    Category  PCK 0.05   PCK 0.1    PCK 0.2\n",
      "0  aeroplane  5.897436  8.461538  23.846154\n",
      "1        All  5.897436  8.461538  23.846154\n",
      "##################################################\n",
      "\n",
      "Layer 5\n",
      "PCK Results per keypoints (%):\n",
      "    Category  PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  7.692308  15.384615  46.153846\n",
      "1        All  7.692308  15.384615  46.153846\n",
      "PCK per-image (%):\n",
      "    Category  PCK 0.05   PCK 0.1    PCK 0.2\n",
      "0  aeroplane  5.128205  10.25641  33.076923\n",
      "1        All  5.128205  10.25641  33.076923\n",
      "##################################################\n",
      "\n",
      "Layer 8\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  23.076923  38.461538  53.846154\n",
      "1        All  23.076923  38.461538  53.846154\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  32.478632  44.273504  54.529915\n",
      "1        All  32.478632  44.273504  54.529915\n",
      "##################################################\n",
      "\n",
      "Layer 11\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  23.076923  38.461538  65.384615\n",
      "1        All  23.076923  38.461538  65.384615\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  24.700855  44.273504  71.538462\n",
      "1        All  24.700855  44.273504  71.538462\n",
      "##################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualization and saving of qualitative results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_to_image(img: torch.Tensor) -> np.ndarray:\n",
    "    img = img.detach().cpu()  # sicurezza\n",
    "    img = img.permute(1, 2, 0)  # [H,W,3]\n",
    "    img = img.numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_keypoints(ax, image, keypoints, color, label=None, marker='o'):\n",
    "    ax.imshow(image)\n",
    "    if len(keypoints) > 0:\n",
    "        xs = [kp[0] for kp in keypoints]\n",
    "        ys = [kp[1] for kp in keypoints]\n",
    "        ax.scatter(xs, ys, c=color, s=40, marker=marker, label=label)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_keypoints_save(\n",
    "        src_img_chw: torch.Tensor,\n",
    "        trg_img_chw: torch.Tensor,\n",
    "        qualitative_result: dict,\n",
    "        dpi: int = 200\n",
    ") -> None:\n",
    "    # --- cartella output ---\n",
    "    save_dir = os.path.join(base_dir, \"qualitative-results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(save_dir, f\"{selected_model}_src.png\")\n",
    "    gt_path = os.path.join(save_dir, f\"{selected_model}_trg_gt.png\")\n",
    "    pr_path = os.path.join(save_dir, f\"{selected_model}_trg_pred.png\")\n",
    "\n",
    "    # --- immagini ---\n",
    "    src_img = tensor_to_image(src_img_chw)\n",
    "    trg_img = tensor_to_image(trg_img_chw)\n",
    "\n",
    "    src_kps = qualitative_result[\"src_kps\"]  # [[x,y],...]\n",
    "    trg_gt = qualitative_result[\"trg_gt_kps\"]  # [[x,y],...]\n",
    "    trg_pr = qualitative_result[\"trg_pred_kps\"]  # [[x,y],...]\n",
    "\n",
    "    n = min(len(src_kps), len(trg_gt), len(trg_pr))\n",
    "    cmap = plt.get_cmap(\"tab10\", max(n, 1))\n",
    "\n",
    "    # ---------- 1) SOURCE ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(src_img)\n",
    "    ax.set_title(\"Source\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xs, ys = src_kps[i]\n",
    "        ax.scatter(xs, ys, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xs + 6, ys + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(src_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 2) TARGET GT ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target GT\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        ax.scatter(xg, yg, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xg + 6, yg + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gt_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 3) TARGET PRED ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target Pred\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        xp, yp = trg_pr[i]\n",
    "\n",
    "        # pred: pallino vuoto\n",
    "        ax.scatter(xp, yp, s=60, color=c, linewidths=2, marker=\"o\")\n",
    "        ax.text(xp + 6, yp + 6, str(i), color=c, fontsize=10)\n",
    "\n",
    "        # (opzionale) linea di errore GT->Pred\n",
    "        ax.plot([xg, xp], [yg, yp], color=c, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pr_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "plot_keypoints_save(\n",
    "    src_img_chw=batch[\"src_img\"],\n",
    "    trg_img_chw=batch[\"trg_img\"],\n",
    "    qualitative_result=qualitative_result\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a0bcd406ee3f4f98",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
