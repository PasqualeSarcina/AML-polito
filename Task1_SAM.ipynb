{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:52.207455382Z",
     "start_time": "2025-12-18T19:29:52.159244620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import TypedDict, Tuple, Any\n",
    "import torch\n",
    "\n",
    "\n",
    "class SPairSample(TypedDict):\n",
    "    pair_id: int\n",
    "    filename: str\n",
    "    src_imname: str\n",
    "    trg_imname: str\n",
    "    src_imsize: Tuple[int, int]\n",
    "    trg_imsize: Tuple[int, int]\n",
    "    src_bbox: Tuple[int, int, int, int]\n",
    "    trg_bbox: Tuple[int, int, int, int]\n",
    "    category: str\n",
    "    src_pose: str\n",
    "    trg_pose: str\n",
    "    src_img: torch.Tensor\n",
    "    trg_img: torch.Tensor\n",
    "    src_kps: torch.Tensor\n",
    "    trg_kps: torch.Tensor\n",
    "    mirror: int\n",
    "    vp_var: int\n",
    "    sc_var: int\n",
    "    truncn: int\n",
    "    occlsn: int\n",
    "    pck_threshold_0_05: float\n",
    "    pck_threshold_0_1: float\n",
    "    pck_threshold_0_2: float"
   ],
   "id": "8bdfc0a2cd090d60",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:52.271054981Z",
     "start_time": "2025-12-18T19:29:52.208844177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "using_colab = False\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    !wget -P ./AML-polito/dataset/ \"https://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    !tar -xvzf ./AML-polito/dataset/SPair-71k.tar.gz -C ./AML-polito/dataset/\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def read_img(path: str) -> torch.Tensor:\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "    return torch.tensor(img.transpose(2, 0, 1).astype(np.float32))\n",
    "\n",
    "\n",
    "def collate_single(batch_list: List[SPairSample]) -> SPairSample:\n",
    "    # batch_size deve essere 1\n",
    "    return batch_list[0]\n",
    "\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, datatype):\n",
    "        self.datatype = datatype\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = self.ann_files[:len(self.ann_files) - 1]\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        self.categories = list(map(lambda x: os.path.basename(x), glob.glob('%s/*' % image_path)))\n",
    "        self.categories.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx) -> SPairSample:\n",
    "        ann_filename = self.ann_files[idx]\n",
    "        ann_file = ann_filename + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_img = read_img(str(os.path.join(self.image_path, category, annotation['src_imname'])))\n",
    "        trg_img = read_img(str(os.path.join(self.image_path, category, annotation['trg_imname'])))\n",
    "\n",
    "        sx1, sy1, sx2, sy2 = annotation[\"src_bndbox\"]\n",
    "        tx1, ty1, tx2, ty2 = annotation[\"trg_bndbox\"]\n",
    "\n",
    "        sample: SPairSample = {'pair_id': int(annotation['pair_id']),\n",
    "                               'filename': str(annotation['filename']),\n",
    "                               'src_imname': str(annotation['src_imname']),\n",
    "                               'trg_imname': str(annotation['trg_imname']),\n",
    "                               'src_imsize': (int(src_img.shape[1]), int(src_img.shape[2])),  # height, width\n",
    "                               'trg_imsize': (int(trg_img.shape[1]), int(trg_img.shape[2])),  # height, width\n",
    "\n",
    "                               'src_bbox': (int(sx1), int(sy1), int(sx2), int(sy2)),\n",
    "                               'trg_bbox': (int(tx1), int(ty1), int(tx2), int(ty2)),\n",
    "                               'category': str(annotation['category']),\n",
    "\n",
    "                               'src_pose': str(annotation['src_pose']),\n",
    "                               'trg_pose': str(annotation['trg_pose']),\n",
    "\n",
    "                               'src_img': src_img,\n",
    "                               'trg_img': trg_img,\n",
    "                               'src_kps': torch.tensor(annotation['src_kps']).float(),\n",
    "                               'trg_kps': torch.tensor(annotation['trg_kps']).float(),\n",
    "\n",
    "                               'mirror': int(annotation['mirror']),\n",
    "                               'vp_var': int(annotation['viewpoint_variation']),\n",
    "                               'sc_var': int(annotation['scale_variation']),\n",
    "                               'truncn': int(annotation['truncation']),\n",
    "                               'occlsn': int(annotation['occlusion']),\n",
    "\n",
    "                               'pck_threshold_0_05': float(max(tx2 - tx1, ty2 - ty1) * 0.05),\n",
    "                               'pck_threshold_0_1': float(max(tx2 - tx1, ty2 - ty1) * 0.1),\n",
    "                               'pck_threshold_0_2': float(max(tx2 - tx1, ty2 - ty1) * 0.2)\n",
    "                               }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'dataset', \"SPair-71k\")\n",
    "pair_ann_path = os.path.join(dataset_dir, 'PairAnnotation')\n",
    "layout_path = os.path.join(dataset_dir, 'Layout')\n",
    "image_path = os.path.join(dataset_dir, 'JPEGImages')\n",
    "dataset_size = 'large'\n",
    "#pck_alpha = 0.05\n",
    "\n",
    "# Verifica che i percorsi esistano prima di creare il dataset\n",
    "if os.path.exists(pair_ann_path) and os.path.exists(layout_path) and os.path.exists(image_path):\n",
    "    trn_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, datatype='trn')\n",
    "    val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, datatype='val')\n",
    "    test_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, datatype='test')\n",
    "\n",
    "    trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1, collate_fn=collate_single)\n",
    "    print(\"Dataset caricati correttamente.\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Errore: Impossibile trovare i percorsi del dataset in '{base_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricati correttamente.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:52.846633828Z",
     "start_time": "2025-12-18T19:29:52.280997172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# inizializzazione SAM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === 1) Carico SAM ===\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# parametri utili\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "#PATCH = IMG_SIZE // 64  # 16\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "print(f\"SAM modello '{selected_model}' caricato. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM modello 'vit_b' caricato. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:53.039375330Z",
     "start_time": "2025-12-18T19:29:52.989778637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale → lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "c0b1d5b0579cac9c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:53.090172108Z",
     "start_time": "2025-12-18T19:29:53.041380545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PCK@T per keypoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_pckt_keypoints(category_results: dict):\n",
    "    rows_keypoints = []\n",
    "\n",
    "    for cat, stats_list in category_results.items():\n",
    "        tot_keypoints = sum(s[\"num_keypoints\"] for s in stats_list)\n",
    "        tot_0_05 = sum(s[\"correct_0_05\"] for s in stats_list)\n",
    "        tot_0_1 = sum(s[\"correct_0_1\"] for s in stats_list)\n",
    "        tot_0_2 = sum(s[\"correct_0_2\"] for s in stats_list)\n",
    "\n",
    "        pck_0_05 = tot_0_05 / tot_keypoints if tot_keypoints > 0 else np.nan\n",
    "        pck_0_1 = tot_0_1 / tot_keypoints if tot_keypoints > 0 else np.nan\n",
    "        pck_0_2 = tot_0_2 / tot_keypoints if tot_keypoints > 0 else np.nan\n",
    "\n",
    "        rows_keypoints.append({\n",
    "            \"Category\": cat,\n",
    "            \"PCK 0.05\": pck_0_05 * 100,\n",
    "            \"PCK 0.1\": pck_0_1 * 100,\n",
    "            \"PCK 0.2\": pck_0_2 * 100,\n",
    "        })\n",
    "\n",
    "    df_keypoints = pd.DataFrame(rows_keypoints).sort_values(\"Category\")\n",
    "\n",
    "    #  \"All\" = macro-average on categories\n",
    "    mean_row_kp = {\n",
    "        \"Category\": \"All\",\n",
    "        \"PCK 0.05\": df_keypoints[\"PCK 0.05\"].mean(skipna=True),\n",
    "        \"PCK 0.1\": df_keypoints[\"PCK 0.1\"].mean(skipna=True),\n",
    "        \"PCK 0.2\": df_keypoints[\"PCK 0.2\"].mean(skipna=True),\n",
    "    }\n",
    "\n",
    "    df_keypoints = pd.concat(\n",
    "        [df_keypoints, pd.DataFrame([mean_row_kp])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(\"PCK Results per keypoints (%):\")\n",
    "    print(df_keypoints)"
   ],
   "id": "9ced78b272827f8",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:53.139376433Z",
     "start_time": "2025-12-18T19:29:53.091747169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def compute_correct_per_category(results: List[Any]) -> dict:\n",
    "    category_results = {}\n",
    "\n",
    "    for res in results:\n",
    "        cat = res[\"category\"]\n",
    "        if cat not in category_results:\n",
    "            category_results[cat] = []\n",
    "\n",
    "        dists_list = res[\"distances\"]\n",
    "        num_keypoints = len(dists_list)\n",
    "        dists = torch.tensor(dists_list)\n",
    "\n",
    "        thr_0_05 = res[\"pck_threshold_0_05\"]\n",
    "        thr_0_1 = res[\"pck_threshold_0_1\"]\n",
    "        thr_0_2 = res[\"pck_threshold_0_2\"]\n",
    "\n",
    "        correct_0_05 = (dists <= thr_0_05).sum().item()\n",
    "        correct_0_1 = (dists <= thr_0_1).sum().item()\n",
    "        correct_0_2 = (dists <= thr_0_2).sum().item()\n",
    "\n",
    "        category_results[cat].append({\n",
    "            \"correct_0_05\": correct_0_05,\n",
    "            \"correct_0_1\": correct_0_1,\n",
    "            \"correct_0_2\": correct_0_2,\n",
    "            \"num_keypoints\": num_keypoints\n",
    "        })\n",
    "    return category_results"
   ],
   "id": "1b7f9dd166701a30",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:53.187849869Z",
     "start_time": "2025-12-18T19:29:53.140690847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PCK@T per image\n",
    "def compute_pckt_images(category_results: dict):\n",
    "    rows_images = []\n",
    "\n",
    "    for cat, stats_list in category_results.items():\n",
    "\n",
    "        pck_imgs_0_05 = []\n",
    "        pck_imgs_0_1 = []\n",
    "        pck_imgs_0_2 = []\n",
    "\n",
    "        for s in stats_list:\n",
    "            if s[\"num_keypoints\"] == 0:\n",
    "                continue\n",
    "\n",
    "            pck_imgs_0_05.append(s[\"correct_0_05\"] / s[\"num_keypoints\"])\n",
    "            pck_imgs_0_1.append(s[\"correct_0_1\"] / s[\"num_keypoints\"])\n",
    "            pck_imgs_0_2.append(s[\"correct_0_2\"] / s[\"num_keypoints\"])\n",
    "\n",
    "        rows_images.append({\n",
    "            \"Category\": cat,\n",
    "            \"PCK 0.05\": np.mean(pck_imgs_0_05) * 100 if pck_imgs_0_05 else np.nan,\n",
    "            \"PCK 0.1\": np.mean(pck_imgs_0_1) * 100 if pck_imgs_0_1 else np.nan,\n",
    "            \"PCK 0.2\": np.mean(pck_imgs_0_2) * 100 if pck_imgs_0_2 else np.nan,\n",
    "        })\n",
    "\n",
    "    df_image = pd.DataFrame(rows_images).sort_values(\"Category\")\n",
    "\n",
    "    #  \"All\" = macro-average on categories\n",
    "    all_row = {\n",
    "        \"Category\": \"All\",\n",
    "        \"PCK 0.05\": df_image[\"PCK 0.05\"].mean(skipna=True),\n",
    "        \"PCK 0.1\": df_image[\"PCK 0.1\"].mean(skipna=True),\n",
    "        \"PCK 0.2\": df_image[\"PCK 0.2\"].mean(skipna=True),\n",
    "    }\n",
    "\n",
    "    df_image = pd.concat([df_image, pd.DataFrame([all_row])], ignore_index=True)\n",
    "\n",
    "    print(\"PCK per-image (%):\")\n",
    "    print(df_image)"
   ],
   "id": "b29c899ffe282bcf",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:29:53.236535274Z",
     "start_time": "2025-12-18T19:29:53.189152549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        activation[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "# Registrazione hook (fallo UNA volta sola prima del loop)\n",
    "# Scegliamo alcuni indici. Per ViT-B (base) ci sono 12 blocchi (0-11).\n",
    "target_layers = [2, 5, 8, 11]\n",
    "for i in target_layers:\n",
    "    predictor.model.image_encoder.blocks[i].register_forward_hook(get_activation(f'layer_{i}'))"
   ],
   "id": "3c087326ad2f11de",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T20:13:24.077570132Z",
     "start_time": "2025-12-18T19:29:53.237787319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dizionario finale: chiave = layer, valore = lista (stessa struttura di results originale)\n",
    "all_results = {li: [] for li in target_layers}\n",
    "\n",
    "# (opzionale) qualitativo per layer (solo sul primo batch)\n",
    "qualitative_by_layer = {\n",
    "    li: {\n",
    "        \"src_img\": None,\n",
    "        \"trg_img\": None,\n",
    "        \"src_kps\": [],\n",
    "        \"trg_gt_kps\": [],\n",
    "        \"trg_pred_kps\": []\n",
    "    }\n",
    "    for li in target_layers\n",
    "}\n",
    "\n",
    "max_images = 1500\n",
    "data = islice(test_dataloader, max_images)\n",
    "size = max_images\n",
    "if using_colab:\n",
    "    data = test_dataloader\n",
    "    size = len(test_dataloader)\n",
    "\n",
    "img_enc = predictor.model.image_encoder  # contiene .neck\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, batch in enumerate(tqdm(data, total=size, desc=f\"Elaborazione con SAM {selected_model}\")):\n",
    "\n",
    "        category = batch[\"category\"]\n",
    "        src_img = batch[\"src_img\"].to(device).unsqueeze(0)  # [1,3,Hs,Ws]\n",
    "        trg_img = batch[\"trg_img\"].to(device).unsqueeze(0)  # [1,3,Ht,Wt]\n",
    "        orig_size_src = batch[\"src_imsize\"]  # (Hs, Ws)\n",
    "        orig_size_trg = batch[\"trg_imsize\"]  # (Ht, Wt)\n",
    "\n",
    "        src_resized = predictor.transform.apply_image_torch(src_img)  # [1,3,Hs',Ws']\n",
    "        trg_resized = predictor.transform.apply_image_torch(trg_img)  # [1,3,Ht',Wt']\n",
    "\n",
    "        # --- 1) Forward sorgente (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(src_resized, orig_size_src)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        src_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 2) Forward target (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(trg_resized, orig_size_trg)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        trg_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 3) Keypoints & metadata ---\n",
    "        src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "        pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "        pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "        pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "        pair_id = batch[\"pair_id\"]\n",
    "        filename = batch[\"filename\"]\n",
    "\n",
    "        # --- regione valida target (per evitare padding) ---\n",
    "        H_prime, W_prime = trg_resized.shape[-2:]\n",
    "        hv_t = math.ceil(H_prime / PATCH)\n",
    "        wv_t = math.ceil(W_prime / PATCH)\n",
    "\n",
    "        N_kps = src_kps.shape[0]\n",
    "\n",
    "        # (opzionale) salva immagini una sola volta per layer quando iter==0\n",
    "        if iter == 0:\n",
    "            for li in target_layers:\n",
    "                qualitative_by_layer[li][\"src_img\"] = batch[\"src_img\"]\n",
    "                qualitative_by_layer[li][\"trg_img\"] = batch[\"trg_img\"]\n",
    "\n",
    "        # ===== Loop sui layer: riuso activation, NON rifaccio forward =====\n",
    "        for selected_layer in target_layers:\n",
    "\n",
    "            # Hook output: [1,64,64,768] (NHWC)\n",
    "            src_hook = src_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "            trg_hook = trg_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "\n",
    "            # NHWC -> NCHW : [1,768,64,64]\n",
    "            src_feat = src_hook.permute(0, 3, 1, 2).contiguous()\n",
    "            trg_feat = trg_hook.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            # ======= QUI la scelta pre-neck / post-neck =======\n",
    "            if selected_layer <= 4:  # \"iniziale\" (puoi cambiare soglia: 3/4/5)\n",
    "                # PRE-NECK: uso direttamente i 768 canali\n",
    "                # (facoltativo ma consigliato) L2-normalizzazione per cosine\n",
    "                src_emb = F.normalize(src_feat, dim=1)[0]   # [768,64,64]\n",
    "                trg_emb = F.normalize(trg_feat, dim=1)[0]   # [768,64,64]\n",
    "            else:\n",
    "                # POST-NECK: porto a 256 canali come output standard SAM\n",
    "                src_emb = img_enc.neck(src_feat)[0]         # [256,64,64]\n",
    "                trg_emb = img_enc.neck(trg_feat)[0]         # [256,64,64]\n",
    "                # (facoltativo) normalizza anche qui se vuoi massima coerenza\n",
    "                # src_emb = F.normalize(src_emb, dim=0)\n",
    "                # trg_emb = F.normalize(trg_emb, dim=0)\n",
    "            # ================================================\n",
    "\n",
    "            C_ft = trg_emb.shape[0]\n",
    "\n",
    "            trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "            trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "            distances_this_image = []\n",
    "\n",
    "            for i in range(N_kps):\n",
    "                src_keypoint = src_kps[i].unsqueeze(0)  # [1,2] (x,y)\n",
    "                trg_keypoint = trg_kps[i]              # [2]   (x,y)\n",
    "\n",
    "                if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                    continue\n",
    "\n",
    "                # originale src -> feature src\n",
    "                x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "                # feature vector sorgente\n",
    "                src_vec = src_emb[:, y_idx, x_idx]  # [256]\n",
    "\n",
    "                # cosine similarity con tutte le posizioni valide del target\n",
    "                sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "                max_idx = torch.argmax(sim).item()\n",
    "                y_idx_t = max_idx // wv_t\n",
    "                x_idx_t = max_idx % wv_t\n",
    "\n",
    "                # feature target -> pixel originali target\n",
    "                x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "                if iter == 0:\n",
    "                    qualitative_by_layer[selected_layer][\"src_kps\"].append(src_keypoint.squeeze(0).tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_gt_kps\"].append(trg_keypoint.tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_pred_kps\"].append([x_pred, y_pred])\n",
    "\n",
    "                dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "                distances_this_image.append(dist)\n",
    "\n",
    "            # Append con struttura IDENTICA all'originale, ma salvata per-layer\n",
    "            all_results[selected_layer].append({\n",
    "                \"pair_id\": pair_id,\n",
    "                #\"filename\": filename,\n",
    "                \"category\": category,\n",
    "                \"pck_threshold_0_05\": pck_thr_0_05,\n",
    "                \"pck_threshold_0_1\": pck_thr_0_1,\n",
    "                \"pck_threshold_0_2\": pck_thr_0_2,\n",
    "                \"distances\": distances_this_image\n",
    "            })\n",
    "\n",
    "            # cleanup per-layer\n",
    "            del src_hook, trg_hook, src_feat, trg_feat, src_emb, trg_emb, trg_valid, trg_flat, src_vec, sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # cleanup per-batch\n",
    "        predictor.reset_image()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Elaborazione completata.\")\n",
    "for li in target_layers:\n",
    "    print(f\"Layer {li}\")\n",
    "    correct = compute_correct_per_category(all_results[li])\n",
    "    compute_pckt_keypoints(correct)\n",
    "    compute_pckt_images(correct)\n",
    "    print(\"#\" * 50)\n",
    "    print(\"\")\n"
   ],
   "id": "c888ec88287af227",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b: 100%|██████████| 1500/1500 [43:30<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elaborazione completata.\n",
      "Layer 2\n",
      "PCK Results per keypoints (%):\n",
      "    Category  PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  6.779353  14.649218  30.625227\n",
      "1    bicycle  1.991723   4.966374  14.614589\n",
      "2       bird  4.057971  11.497585  25.507246\n",
      "3        All  4.276349  10.371059  23.582354\n",
      "PCK per-image (%):\n",
      "    Category  PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  5.990497  13.294406  27.212843\n",
      "1    bicycle  1.809424   4.412904  13.332184\n",
      "2       bird  3.911007  11.629419  24.729933\n",
      "3        All  3.903643   9.778910  21.758320\n",
      "##################################################\n",
      "\n",
      "Layer 5\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  13.486005  20.556161  35.823337\n",
      "1    bicycle   6.052768  10.475944  20.563890\n",
      "2       bird  14.782609  23.768116  36.908213\n",
      "3        All  11.440461  18.266740  31.098480\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  12.016869  18.656085  33.334769\n",
      "1    bicycle   5.740243  10.118393  19.534893\n",
      "2       bird  14.367357  22.952832  35.235074\n",
      "3        All  10.708156  17.242436  29.368245\n",
      "##################################################\n",
      "\n",
      "Layer 8\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  16.921120  24.754635  39.512904\n",
      "1    bicycle   9.156751  14.562856  26.176927\n",
      "2       bird  22.415459  33.719807  47.826087\n",
      "3        All  16.164443  24.345766  37.838639\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  15.158699  22.559483  37.029405\n",
      "1    bicycle   7.903974  12.902165  23.656987\n",
      "2       bird  21.611314  32.499346  45.967262\n",
      "3        All  14.891329  22.653665  35.551218\n",
      "##################################################\n",
      "\n",
      "Layer 11\n",
      "PCK Results per keypoints (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  17.938931  26.045075  39.912759\n",
      "1    bicycle  10.191412  16.063114  27.340921\n",
      "2       bird  22.608696  32.850242  49.275362\n",
      "3        All  16.913013  24.986143  38.843014\n",
      "PCK per-image (%):\n",
      "    Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0  aeroplane  15.815338  23.647742  37.358754\n",
      "1    bicycle   9.014452  14.314169  24.890543\n",
      "2       bird  21.326840  30.803120  46.789953\n",
      "3        All  15.385543  22.921677  36.346416\n",
      "##################################################\n",
      "\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T20:13:25.245174459Z",
     "start_time": "2025-12-18T20:13:24.256673846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualization and saving of qualitative results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_to_image(img: torch.Tensor) -> np.ndarray:\n",
    "    img = img.detach().cpu()  # sicurezza\n",
    "    img = img.permute(1, 2, 0)  # [H,W,3]\n",
    "    img = img.numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_keypoints(ax, image, keypoints, color, label=None, marker='o'):\n",
    "    ax.imshow(image)\n",
    "    if len(keypoints) > 0:\n",
    "        xs = [kp[0] for kp in keypoints]\n",
    "        ys = [kp[1] for kp in keypoints]\n",
    "        ax.scatter(xs, ys, c=color, s=40, marker=marker, label=label)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_keypoints_save(\n",
    "        src_img_chw: torch.Tensor,\n",
    "        trg_img_chw: torch.Tensor,\n",
    "        qualitative_result: dict,\n",
    "        dpi: int = 200\n",
    ") -> None:\n",
    "    # --- cartella output ---\n",
    "    save_dir = os.path.join(base_dir, \"qualitative-results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(save_dir, f\"{selected_model}_src.png\")\n",
    "    gt_path = os.path.join(save_dir, f\"{selected_model}_trg_gt.png\")\n",
    "    pr_path = os.path.join(save_dir, f\"{selected_model}_trg_pred.png\")\n",
    "\n",
    "    # --- immagini ---\n",
    "    src_img = tensor_to_image(src_img_chw)\n",
    "    trg_img = tensor_to_image(trg_img_chw)\n",
    "\n",
    "    src_kps = qualitative_result[\"src_kps\"]  # [[x,y],...]\n",
    "    trg_gt = qualitative_result[\"trg_gt_kps\"]  # [[x,y],...]\n",
    "    trg_pr = qualitative_result[\"trg_pred_kps\"]  # [[x,y],...]\n",
    "\n",
    "    n = min(len(src_kps), len(trg_gt), len(trg_pr))\n",
    "    cmap = plt.get_cmap(\"tab10\", max(n, 1))\n",
    "\n",
    "    # ---------- 1) SOURCE ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(src_img)\n",
    "    ax.set_title(\"Source\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xs, ys = src_kps[i]\n",
    "        ax.scatter(xs, ys, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xs + 6, ys + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(src_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 2) TARGET GT ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target GT\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        ax.scatter(xg, yg, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xg + 6, yg + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gt_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 3) TARGET PRED ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target Pred\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        xp, yp = trg_pr[i]\n",
    "\n",
    "        # pred: pallino vuoto\n",
    "        ax.scatter(xp, yp, s=60, color=c, linewidths=2, marker=\"o\")\n",
    "        ax.text(xp + 6, yp + 6, str(i), color=c, fontsize=10)\n",
    "\n",
    "        # (opzionale) linea di errore GT->Pred\n",
    "        ax.plot([xg, xp], [yg, yp], color=c, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pr_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "plot_keypoints_save(\n",
    "    src_img_chw=batch[\"src_img\"],\n",
    "    trg_img_chw=batch[\"trg_img\"],\n",
    "    qualitative_result=qualitative_result\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a0bcd406ee3f4f98",
   "outputs": [],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
