{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T00:42:07.124488790Z",
     "start_time": "2026-01-10T00:40:33.259149452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from data.spair import SPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "using_colab = True\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "dataset_size = 'large' # 'small' or 'large'\n",
    "\n",
    "# Load dataset and construct dataloader\n",
    "if os.path.exists(dataset_dir):\n",
    "    test_dataset = SPairDataset(datatype='test', dataset_size=dataset_size)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1, collate_fn=collate_single)\n",
    "    print(\"Dataset loaded\")\n",
    "else:\n",
    "    raise IOError(f\"Cannot find dataset files in '{dataset_dir}'.\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged annotation file already exists: /home/pasquale/PycharmProjects/AML-polito/dataset/ap-10k/annotations/ap10k-test-merged.jsonl\n",
      "[INTRA-SPECIES] Total pairs: 4802\n",
      "Intra-species pairs generated.\n",
      "[CROSS-SPECIES] Total pairs: 4250\n",
      "Cross-species pairs generated.\n",
      "[CROSS-FAMILY] Total pairs: 4200\n",
      "Cross-family pairs generated.\n",
      "Processed annotation file created: /home/pasquale/PycharmProjects/AML-polito/dataset/ap-10k/annotations/ap10k-test-processed.jsonl\n",
      "Dataset loaded\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T00:42:58.946295366Z",
     "start_time": "2026-01-10T00:42:58.876103481Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13252"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3,
   "source": "len(test_dataloader)",
   "id": "a9deeddd10048454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T00:44:29.246203689Z",
     "start_time": "2026-01-10T00:44:27.640780371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# SAM initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "\n",
    "print(f\"SAM '{selected_model}' loaded. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM 'vit_b' loaded. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T00:44:30.884203844Z",
     "start_time": "2026-01-10T00:44:30.859393217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale â†’ lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "c0b1d5b0579cac9c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T00:44:33.350641030Z",
     "start_time": "2026-01-10T00:44:33.324413324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        activation[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "# Hook registration\n",
    "target_layers = [7, 9, 10, 11]  # Layer to extract (SAM has 12 layers)\n",
    "for i in target_layers:\n",
    "    predictor.model.image_encoder.blocks[i].register_forward_hook(get_activation(f'layer_{i}'))"
   ],
   "id": "3c087326ad2f11de",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T00:44:58.224188930Z",
     "start_time": "2026-01-10T00:44:39.729336655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_correspondence import hard_argmax\n",
    "from utils.utils_results import *\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dizionario finale: chiave = layer, valore = lista (stessa struttura di results originale)\n",
    "all_results = {li: [] for li in target_layers}\n",
    "\n",
    "# (opzionale) qualitativo per layer (solo sul primo batch)\n",
    "qualitative_by_layer = {\n",
    "    li: {\n",
    "        \"src_img\": None,\n",
    "        \"trg_img\": None,\n",
    "        \"src_kps\": [],\n",
    "        \"trg_gt_kps\": [],\n",
    "        \"trg_pred_kps\": []\n",
    "    }\n",
    "    for li in target_layers\n",
    "}\n",
    "\n",
    "max_images = 300\n",
    "data = islice(test_dataloader, max_images)\n",
    "size = max_images\n",
    "if using_colab:\n",
    "    data = test_dataloader\n",
    "    size = len(test_dataloader)\n",
    "\n",
    "img_enc = predictor.model.image_encoder  # contiene .neck\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, batch in enumerate(tqdm(data, total=size, desc=f\"Elaborazione con SAM {selected_model}\")):\n",
    "\n",
    "        category = batch[\"category\"]\n",
    "        src_img = batch[\"src_img\"].to(device).unsqueeze(0)  # [1,3,Hs,Ws]\n",
    "        trg_img = batch[\"trg_img\"].to(device).unsqueeze(0)  # [1,3,Ht,Wt]\n",
    "        orig_size_src = tuple(batch[\"src_imsize\"][1:])  # (Hs, Ws)\n",
    "        orig_size_trg = tuple(batch[\"trg_imsize\"][1:])  # (Ht, Wt)\n",
    "\n",
    "        src_resized = predictor.transform.apply_image_torch(src_img)  # [1,3,Hs',Ws']\n",
    "        trg_resized = predictor.transform.apply_image_torch(trg_img)  # [1,3,Ht',Wt']\n",
    "\n",
    "        # --- 1) Forward sorgente (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(src_resized, orig_size_src)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        src_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 2) Forward target (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(trg_resized, orig_size_trg)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        trg_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 3) Keypoints & metadata ---\n",
    "        src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "        pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "        pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "        pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "        # --- regione valida target (per evitare padding) ---\n",
    "        H_prime, W_prime = trg_resized.shape[-2:]\n",
    "        hv_t = math.ceil(H_prime / PATCH)\n",
    "        wv_t = math.ceil(W_prime / PATCH)\n",
    "\n",
    "        N_kps = src_kps.shape[0]\n",
    "\n",
    "        # (opzionale) salva immagini una sola volta per layer quando iter==0\n",
    "        if iter == 0:\n",
    "            for li in target_layers:\n",
    "                qualitative_by_layer[li][\"src_img\"] = batch[\"src_img\"]\n",
    "                qualitative_by_layer[li][\"trg_img\"] = batch[\"trg_img\"]\n",
    "\n",
    "        # ===== Loop sui layer: riuso activation, NON rifaccio forward =====\n",
    "        for selected_layer in target_layers:\n",
    "\n",
    "            # Hook output: [1,64,64,768] (NHWC)\n",
    "            src_hook = src_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "            trg_hook = trg_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "\n",
    "            # NHWC -> NCHW : [1,768,64,64]\n",
    "            src_feat = src_hook.permute(0, 3, 1, 2).contiguous()\n",
    "            trg_feat = trg_hook.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            src_emb = img_enc.neck(src_feat)[0]  # [256,64,64]\n",
    "            trg_emb = img_enc.neck(trg_feat)[0]  # [256,64,64]\n",
    "\n",
    "            C_ft = trg_emb.shape[0]\n",
    "\n",
    "            trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "            trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "            distances_this_image = []\n",
    "\n",
    "            for i in range(N_kps):\n",
    "                src_keypoint = src_kps[i].unsqueeze(0)  # [1,2] (x,y)\n",
    "                trg_keypoint = trg_kps[i]  # [2]   (x,y)\n",
    "\n",
    "                if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                    continue\n",
    "\n",
    "                # originale src -> feature src\n",
    "                x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "                # feature vector sorgente\n",
    "                src_vec = src_emb[:, y_idx, x_idx]  # [256]\n",
    "\n",
    "                # cosine similarity con tutte le posizioni valide del target\n",
    "                sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "                #max_idx = torch.argmax(sim).item()\n",
    "                #y_idx_t = max_idx // wv_t\n",
    "                #x_idx_t = max_idx % wv_t\n",
    "\n",
    "                sim2d = sim.view(hv_t, wv_t)\n",
    "                x_idx_t, y_idx_t = hard_argmax(sim2d)\n",
    "\n",
    "                # feature target -> pixel originali target\n",
    "                x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "                if iter == 0:\n",
    "                    qualitative_by_layer[selected_layer][\"src_kps\"].append(src_keypoint.squeeze(0).tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_gt_kps\"].append(trg_keypoint.tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_pred_kps\"].append([x_pred, y_pred])\n",
    "\n",
    "                dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "                distances_this_image.append(dist)\n",
    "\n",
    "            # Append con struttura IDENTICA all'originale, ma salvata per-layer\n",
    "            all_results[selected_layer].append(\n",
    "                CorrespondenceResult(\n",
    "                    category=category,\n",
    "                    distances=distances_this_image,\n",
    "                    pck_threshold_0_05=pck_thr_0_05,\n",
    "                    pck_threshold_0_1=pck_thr_0_1,\n",
    "                    pck_threshold_0_2=pck_thr_0_2\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # cleanup per-layer\n",
    "            del src_hook, trg_hook, src_feat, trg_feat, src_emb, trg_emb, trg_valid, trg_flat, src_vec, sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # cleanup per-batch\n",
    "        predictor.reset_image()\n",
    "        torch.cuda.empty_cache()"
   ],
   "id": "c888ec88287af227",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b:   0%|          | 10/13252 [00:17<6:29:13,  1.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 46\u001B[39m\n\u001B[32m     43\u001B[39m trg_resized = predictor.transform.apply_image_torch(trg_img)  \u001B[38;5;66;03m# [1,3,Ht',Wt']\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[38;5;66;03m# --- 1) Forward sorgente (riempie activation con output dei blocchi) ---\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m \u001B[43mpredictor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mset_torch_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_resized\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morig_size_src\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     47\u001B[39m _ = predictor.get_image_embedding()[\u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# trigger hooks\u001B[39;00m\n\u001B[32m     48\u001B[39m src_intermediate_emb = {k: v.detach().clone() \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m activation.items()}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/segment_anything/predictor.py:89\u001B[39m, in \u001B[36mSamPredictor.set_torch_image\u001B[39m\u001B[34m(self, transformed_image, original_image_size)\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[38;5;28mself\u001B[39m.input_size = \u001B[38;5;28mtuple\u001B[39m(transformed_image.shape[-\u001B[32m2\u001B[39m:])\n\u001B[32m     88\u001B[39m input_image = \u001B[38;5;28mself\u001B[39m.model.preprocess(transformed_image)\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m \u001B[38;5;28mself\u001B[39m.features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimage_encoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_image\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     90\u001B[39m \u001B[38;5;28mself\u001B[39m.is_image_set = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/segment_anything/modeling/image_encoder.py:112\u001B[39m, in \u001B[36mImageEncoderViT.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    109\u001B[39m     x = x + \u001B[38;5;28mself\u001B[39m.pos_embed\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.blocks:\n\u001B[32m--> \u001B[39m\u001B[32m112\u001B[39m     x = \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    114\u001B[39m x = \u001B[38;5;28mself\u001B[39m.neck(x.permute(\u001B[32m0\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m))\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1881\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1880\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1881\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1883\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1884\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1885\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n\u001B[32m   1886\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m _global_forward_hooks.items():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1829\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1826\u001B[39m     bw_hook = BackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[32m   1827\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1829\u001B[39m result = \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1830\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n\u001B[32m   1831\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[32m   1832\u001B[39m         *_global_forward_hooks.items(),\n\u001B[32m   1833\u001B[39m         *\u001B[38;5;28mself\u001B[39m._forward_hooks.items(),\n\u001B[32m   1834\u001B[39m     ):\n\u001B[32m   1835\u001B[39m         \u001B[38;5;66;03m# mark that always called hook is run\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/segment_anything/modeling/image_encoder.py:174\u001B[39m, in \u001B[36mBlock.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    171\u001B[39m     H, W = x.shape[\u001B[32m1\u001B[39m], x.shape[\u001B[32m2\u001B[39m]\n\u001B[32m    172\u001B[39m     x, pad_hw = window_partition(x, \u001B[38;5;28mself\u001B[39m.window_size)\n\u001B[32m--> \u001B[39m\u001B[32m174\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[38;5;66;03m# Reverse window partition\u001B[39;00m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.window_size > \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/segment_anything/modeling/image_encoder.py:234\u001B[39m, in \u001B[36mAttention.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    231\u001B[39m attn = (q * \u001B[38;5;28mself\u001B[39m.scale) @ k.transpose(-\u001B[32m2\u001B[39m, -\u001B[32m1\u001B[39m)\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.use_rel_pos:\n\u001B[32m--> \u001B[39m\u001B[32m234\u001B[39m     attn = \u001B[43madd_decomposed_rel_pos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrel_pos_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrel_pos_w\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    236\u001B[39m attn = attn.softmax(dim=-\u001B[32m1\u001B[39m)\n\u001B[32m    237\u001B[39m x = (attn @ v).view(B, \u001B[38;5;28mself\u001B[39m.num_heads, H, W, -\u001B[32m1\u001B[39m).permute(\u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m4\u001B[39m).reshape(B, H, W, -\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/segment_anything/modeling/image_encoder.py:350\u001B[39m, in \u001B[36madd_decomposed_rel_pos\u001B[39m\u001B[34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001B[39m\n\u001B[32m    348\u001B[39m k_h, k_w = k_size\n\u001B[32m    349\u001B[39m Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n\u001B[32m--> \u001B[39m\u001B[32m350\u001B[39m Rw = \u001B[43mget_rel_pos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq_w\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk_w\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrel_pos_w\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    352\u001B[39m B, _, dim = q.shape\n\u001B[32m    353\u001B[39m r_q = q.reshape(B, q_h, q_w, dim)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/segment_anything/modeling/image_encoder.py:292\u001B[39m, in \u001B[36mget_rel_pos\u001B[39m\u001B[34m(q_size, k_size, rel_pos)\u001B[39m\n\u001B[32m    288\u001B[39m         x = x[:, :H, :W, :].contiguous()\n\u001B[32m    289\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[32m--> \u001B[39m\u001B[32m292\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_rel_pos\u001B[39m(q_size: \u001B[38;5;28mint\u001B[39m, k_size: \u001B[38;5;28mint\u001B[39m, rel_pos: torch.Tensor) -> torch.Tensor:\n\u001B[32m    293\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    294\u001B[39m \u001B[33;03m    Get relative positional embeddings according to the relative positions of\u001B[39;00m\n\u001B[32m    295\u001B[39m \u001B[33;03m        query and key sizes.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    302\u001B[39m \u001B[33;03m        Extracted positional embeddings according to relative positions.\u001B[39;00m\n\u001B[32m    303\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    304\u001B[39m     max_rel_dist = \u001B[38;5;28mint\u001B[39m(\u001B[32m2\u001B[39m * \u001B[38;5;28mmax\u001B[39m(q_size, k_size) - \u001B[32m1\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute and print results per layer\n",
    "for li in target_layers:\n",
    "    print(f\"Layer {li}\")\n",
    "    correct = compute_correct_per_category(all_results[li])\n",
    "    compute_pckt_keypoints(correct)\n",
    "    compute_pckt_images(correct)\n",
    "    print(\"<\" + \"=\" * 50 + \">\")\n",
    "    print(\"\")"
   ],
   "id": "e3a7661228d5944c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualization and saving of qualitative results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_to_image(img: torch.Tensor) -> np.ndarray:\n",
    "    img = img.detach().cpu()  # sicurezza\n",
    "    img = img.permute(1, 2, 0)  # [H,W,3]\n",
    "    img = img.numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_keypoints(ax, image, keypoints, color, label=None, marker='o'):\n",
    "    ax.imshow(image)\n",
    "    if len(keypoints) > 0:\n",
    "        xs = [kp[0] for kp in keypoints]\n",
    "        ys = [kp[1] for kp in keypoints]\n",
    "        ax.scatter(xs, ys, c=color, s=40, marker=marker, label=label)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_keypoints_save(\n",
    "        src_img_chw: torch.Tensor,\n",
    "        trg_img_chw: torch.Tensor,\n",
    "        qualitative_result: dict,\n",
    "        dpi: int = 200\n",
    ") -> None:\n",
    "    # --- cartella output ---\n",
    "    save_dir = os.path.join(base_dir, \"qualitative-results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(save_dir, f\"{selected_model}_src.png\")\n",
    "    gt_path = os.path.join(save_dir, f\"{selected_model}_trg_gt.png\")\n",
    "    pr_path = os.path.join(save_dir, f\"{selected_model}_trg_pred.png\")\n",
    "\n",
    "    # --- immagini ---\n",
    "    src_img = tensor_to_image(src_img_chw)\n",
    "    trg_img = tensor_to_image(trg_img_chw)\n",
    "\n",
    "    src_kps = qualitative_result[\"src_kps\"]  # [[x,y],...]\n",
    "    trg_gt = qualitative_result[\"trg_gt_kps\"]  # [[x,y],...]\n",
    "    trg_pr = qualitative_result[\"trg_pred_kps\"]  # [[x,y],...]\n",
    "\n",
    "    n = min(len(src_kps), len(trg_gt), len(trg_pr))\n",
    "    cmap = plt.get_cmap(\"tab10\", max(n, 1))\n",
    "\n",
    "    # ---------- 1) SOURCE ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(src_img)\n",
    "    ax.set_title(\"Source\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xs, ys = src_kps[i]\n",
    "        ax.scatter(xs, ys, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xs + 6, ys + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(src_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 2) TARGET GT ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target GT\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        ax.scatter(xg, yg, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xg + 6, yg + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gt_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 3) TARGET PRED ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target Pred\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        xp, yp = trg_pr[i]\n",
    "\n",
    "        # pred: pallino vuoto\n",
    "        ax.scatter(xp, yp, s=60, color=c, linewidths=2, marker=\"o\")\n",
    "        ax.text(xp + 6, yp + 6, str(i), color=c, fontsize=10)\n",
    "\n",
    "        # (opzionale) linea di errore GT->Pred\n",
    "        ax.plot([xg, xp], [yg, yp], color=c, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pr_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "plot_keypoints_save(\n",
    "    src_img_chw=batch[\"src_img\"],\n",
    "    trg_img_chw=batch[\"trg_img\"],\n",
    "    qualitative_result=qualitative_result\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a0bcd406ee3f4f98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "12deb7cd23ce024",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
