{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:21:44.948712178Z",
     "start_time": "2025-12-14T12:21:44.916099351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import TypedDict, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class SPairSample(TypedDict):\n",
    "    pair_id: int\n",
    "    filename: str\n",
    "    src_imname: str\n",
    "    trg_imname: str\n",
    "    src_imsize: Tuple[int, int]\n",
    "    trg_imsize: Tuple[int, int]\n",
    "    src_bbox: Tuple[int, int, int, int]\n",
    "    trg_bbox: Tuple[int, int, int, int]\n",
    "    category: str\n",
    "    src_pose: str\n",
    "    trg_pose: str\n",
    "    src_img: torch.Tensor\n",
    "    trg_img: torch.Tensor\n",
    "    src_kps: torch.Tensor\n",
    "    trg_kps: torch.Tensor\n",
    "    mirror: int\n",
    "    vp_var: int\n",
    "    sc_var: int\n",
    "    truncn: int\n",
    "    occlsn: int\n",
    "    pck_threshold_0_05: float\n",
    "    pck_threshold_0_1: float\n",
    "    pck_threshold_0_2: float"
   ],
   "id": "8bdfc0a2cd090d60",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:21:45.030669580Z",
     "start_time": "2025-12-14T12:21:44.953281058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "using_colab = False\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    !wget -P ./AML-polito/dataset/ \"https://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    !tar -xvzf ./AML-polito/dataset/SPair-71k.tar.gz -C ./AML-polito/dataset/\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def read_img(path: str) -> torch.Tensor:\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "    return torch.tensor(img.transpose(2, 0, 1).astype(np.float32))\n",
    "\n",
    "\n",
    "def collate_single(batch_list: List[SPairSample]) -> SPairSample:\n",
    "    # batch_size deve essere 1\n",
    "    return batch_list[0]\n",
    "\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, datatype):\n",
    "        self.datatype = datatype\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = self.ann_files[:len(self.ann_files) - 1]\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        self.categories = list(map(lambda x: os.path.basename(x), glob.glob('%s/*' % image_path)))\n",
    "        self.categories.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx) -> SPairSample:\n",
    "        ann_filename = self.ann_files[idx]\n",
    "        ann_file = ann_filename + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_img = read_img(str(os.path.join(self.image_path, category, annotation['src_imname'])))\n",
    "        trg_img = read_img(str(os.path.join(self.image_path, category, annotation['trg_imname'])))\n",
    "\n",
    "        sx1, sy1, sx2, sy2 = annotation[\"src_bndbox\"]\n",
    "        tx1, ty1, tx2, ty2 = annotation[\"trg_bndbox\"]\n",
    "\n",
    "        sample: SPairSample = {'pair_id': int(annotation['pair_id']),\n",
    "                               'filename': str(annotation['filename']),\n",
    "                               'src_imname': str(annotation['src_imname']),\n",
    "                               'trg_imname': str(annotation['trg_imname']),\n",
    "                               'src_imsize': (int(src_img.shape[1]), int(src_img.shape[2])),  # height, width\n",
    "                               'trg_imsize': (int(trg_img.shape[1]), int(trg_img.shape[2])),  # height, width\n",
    "\n",
    "                               'src_bbox': (int(sx1), int(sy1), int(sx2), int(sy2)),\n",
    "                               'trg_bbox': (int(tx1), int(ty1), int(tx2), int(ty2)),\n",
    "                               'category': str(annotation['category']),\n",
    "\n",
    "                               'src_pose': str(annotation['src_pose']),\n",
    "                               'trg_pose': str(annotation['trg_pose']),\n",
    "\n",
    "                               'src_img': src_img,\n",
    "                               'trg_img': trg_img,\n",
    "                               'src_kps': torch.tensor(annotation['src_kps']).float(),\n",
    "                               'trg_kps': torch.tensor(annotation['trg_kps']).float(),\n",
    "\n",
    "                               'mirror': int(annotation['mirror']),\n",
    "                               'vp_var': int(annotation['viewpoint_variation']),\n",
    "                               'sc_var': int(annotation['scale_variation']),\n",
    "                               'truncn': int(annotation['truncation']),\n",
    "                               'occlsn': int(annotation['occlusion']),\n",
    "\n",
    "                               'pck_threshold_0_05': float(max(tx2 - tx1, ty2 - ty1) * 0.05),\n",
    "                               'pck_threshold_0_1': float(max(tx2 - tx1, ty2 - ty1) * 0.1),\n",
    "                               'pck_threshold_0_2': float(max(tx2 - tx1, ty2 - ty1) * 0.2)\n",
    "                               }\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'dataset', \"SPair-71k\")\n",
    "pair_ann_path = os.path.join(dataset_dir, 'PairAnnotation')\n",
    "layout_path = os.path.join(dataset_dir, 'Layout')\n",
    "image_path = os.path.join(dataset_dir, 'JPEGImages')\n",
    "dataset_size = 'large'\n",
    "#pck_alpha = 0.05\n",
    "\n",
    "# Verifica che i percorsi esistano prima di creare il dataset\n",
    "if os.path.exists(pair_ann_path) and os.path.exists(layout_path) and os.path.exists(image_path):\n",
    "    trn_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, datatype='trn')\n",
    "    val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, datatype='val')\n",
    "    test_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, datatype='test')\n",
    "\n",
    "    trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1, collate_fn=collate_single)\n",
    "    print(\"Dataset caricati correttamente.\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Errore: Impossibile trovare i percorsi del dataset in '{base_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricati correttamente.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:21:45.608974213Z",
     "start_time": "2025-12-14T12:21:45.034156268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "\n",
    "import torch\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# inizializzazione SAM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === 1) Carico SAM ===\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# parametri utili\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "PATCH = IMG_SIZE // 64  # 16"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:21:45.728632243Z",
     "start_time": "2025-12-14T12:21:45.678607344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale → lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "c0b1d5b0579cac9c",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:21:47.430775474Z",
     "start_time": "2025-12-14T12:21:45.732186258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import math\n",
    "import torch\n",
    "\n",
    "results = []\n",
    "qualitative_result = {\n",
    "    \"src_img\": None,\n",
    "    \"trg_img\": None,\n",
    "    \"src_kps\": [],\n",
    "    \"trg_gt_kps\": [],\n",
    "    \"trg_pred_kps\": []\n",
    "}\n",
    "\n",
    "max_images = 1\n",
    "data = islice(test_dataloader, max_images)\n",
    "size = max_images\n",
    "if using_colab:\n",
    "    data = test_dataloader\n",
    "    size = len(test_dataloader)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, batch in enumerate(tqdm(data, total=size, desc=\"Elaborazione con SAM\")):\n",
    "\n",
    "        category = batch[\"category\"]\n",
    "        src_img = batch[\"src_img\"].to(device).unsqueeze(0)  # [1,3,Hs,Ws]\n",
    "        trg_img = batch[\"trg_img\"].to(device).unsqueeze(0)  # [1,3,Ht,Wt]\n",
    "        orig_size_src = batch[\"src_imsize\"]  # (Hs, Ws)\n",
    "        orig_size_trg = batch[\"trg_imsize\"]  # (Ht, Wt)\n",
    "\n",
    "        src_resized = predictor.transform.apply_image_torch(src_img)  # [1,3,Hs',Ws']\n",
    "        trg_resized = predictor.transform.apply_image_torch(trg_img)  # [1,3,Ht',Wt']\n",
    "\n",
    "        # --- 1) Embedding SAM sorgente ---\n",
    "        predictor.set_torch_image(src_resized, orig_size_src)\n",
    "        src_emb = predictor.get_image_embedding()[0]  # [C,64,64]\n",
    "\n",
    "        # --- 2) Embedding SAM target ---\n",
    "        predictor.set_torch_image(trg_resized, orig_size_trg)\n",
    "        trg_emb = predictor.get_image_embedding()[0]  # [C,64,64]\n",
    "        C_ft = trg_emb.shape[0]\n",
    "\n",
    "        # --- 3) Keypoints & metadata ---\n",
    "        src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "        pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "        pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "        pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "        pair_id = batch[\"pair_id\"]\n",
    "        filename = batch[\"filename\"]\n",
    "\n",
    "        # --- regione valida target (per evitare padding) ---\n",
    "        # resized (prima padding)\n",
    "        H_prime, W_prime = trg_resized.shape[-2:]\n",
    "\n",
    "        # regione valida feature map\n",
    "        hv_t = math.ceil(H_prime / PATCH)\n",
    "        wv_t = math.ceil(W_prime / PATCH)\n",
    "\n",
    "        # Get valid part of trg embedding\n",
    "        trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "        # Flatten valid part\n",
    "        trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "        # --- 4) Loop sui keypoint sorgente ---\n",
    "        N_kps = src_kps.shape[0]\n",
    "        distances_this_image = []\n",
    "\n",
    "        if iter == 0:\n",
    "            qualitative_result[\"src_img\"] = batch[\"src_img\"]\n",
    "            qualitative_result[\"trg_img\"] = batch[\"trg_img\"]\n",
    "\n",
    "        for i in range(N_kps):\n",
    "            src_keypoint = src_kps[i].unsqueeze(0)  # [x, y]\n",
    "            trg_keypoint = trg_kps[i]  # [x, y]\n",
    "\n",
    "            # salta keypoint mancanti\n",
    "            if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                continue\n",
    "\n",
    "            # 4.1: originale src -> feature src (coerente con SAM)\n",
    "            x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "            # 4.2: vettore feature sorgente in quel punto\n",
    "            src_vec = src_emb[:, y_idx, x_idx]  # [C]\n",
    "\n",
    "            # 4.3: cosine similarity con tutte le posizioni valide del target (senza normalizzare a mano)\n",
    "            sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "\n",
    "            # 4.4: argmax su regione valida\n",
    "            max_idx = torch.argmax(sim).item()\n",
    "            y_idx_t = max_idx // wv_t\n",
    "            x_idx_t = max_idx % wv_t\n",
    "\n",
    "            # 4.5: feature target -> pixel originali target (passando per resized)\n",
    "            x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "            if iter == 0:\n",
    "                qualitative_result[\"src_kps\"].append(src_keypoint.squeeze(0).tolist())\n",
    "                qualitative_result[\"trg_gt_kps\"].append(trg_keypoint.tolist())\n",
    "                qualitative_result[\"trg_pred_kps\"].append([x_pred, y_pred])\n",
    "\n",
    "            # 4.6: distanza in pixel nello spazio originale target\n",
    "            dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "            distances_this_image.append(dist)\n",
    "\n",
    "        results.append({\n",
    "            \"pair_id\": pair_id,\n",
    "            \"filename\": filename,\n",
    "            \"category\": category,\n",
    "            \"pck_threshold_0_05\": pck_thr_0_05,\n",
    "            \"pck_threshold_0_1\": pck_thr_0_1,\n",
    "            \"pck_threshold_0_2\": pck_thr_0_2,\n",
    "            \"distances\": distances_this_image\n",
    "        })\n",
    "\n",
    "        # --- cleanup GPU ---\n",
    "        predictor.reset_image()\n",
    "        for name in (\"src_emb\", \"trg_emb\", \"trg_flat\", \"src_vec\", \"sim\"):\n",
    "            if name in locals():\n",
    "                del locals()[name]\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Elaborazione completata. Numero di coppie elaborate:\", len(results))\n",
    "print(qualitative_result)"
   ],
   "id": "ad2760abc609fa70",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elaborazione completata. Numero di coppie elaborate: 1\n",
      "{'src_img': tensor([[[129., 129., 130.,  ..., 210., 210., 210.],\n",
      "         [129., 129., 129.,  ..., 210., 210., 210.],\n",
      "         [128., 128., 128.,  ..., 210., 210., 210.],\n",
      "         ...,\n",
      "         [124., 129., 135.,  ..., 169., 170., 171.],\n",
      "         [129., 134., 140.,  ..., 171., 172., 172.],\n",
      "         [136., 139., 143.,  ..., 172., 173., 173.]],\n",
      "\n",
      "        [[146., 146., 147.,  ..., 211., 211., 211.],\n",
      "         [146., 146., 146.,  ..., 211., 211., 211.],\n",
      "         [145., 145., 145.,  ..., 211., 211., 211.],\n",
      "         ...,\n",
      "         [ 86.,  91.,  97.,  ..., 174., 175., 176.],\n",
      "         [ 91.,  96., 102.,  ..., 176., 177., 177.],\n",
      "         [ 98., 101., 105.,  ..., 177., 178., 178.]],\n",
      "\n",
      "        [[162., 162., 163.,  ..., 213., 213., 213.],\n",
      "         [162., 162., 162.,  ..., 213., 213., 213.],\n",
      "         [163., 163., 163.,  ..., 213., 213., 213.],\n",
      "         ...,\n",
      "         [ 63.,  68.,  74.,  ..., 180., 181., 182.],\n",
      "         [ 68.,  73.,  79.,  ..., 182., 183., 183.],\n",
      "         [ 75.,  78.,  82.,  ..., 183., 184., 184.]]]), 'trg_img': tensor([[[117., 109., 124.,  ..., 253., 252., 251.],\n",
      "         [111., 119., 127.,  ..., 253., 253., 254.],\n",
      "         [111., 124., 121.,  ..., 253., 254., 254.],\n",
      "         ...,\n",
      "         [ 12.,  17.,  12.,  ...,  31.,  34.,  33.],\n",
      "         [  9.,  18.,   6.,  ...,  37.,  36.,  36.],\n",
      "         [ 11.,   9.,  16.,  ...,  46.,  39.,  38.]],\n",
      "\n",
      "        [[130., 122., 137.,  ..., 254., 253., 252.],\n",
      "         [124., 132., 140.,  ..., 254., 254., 255.],\n",
      "         [124., 137., 134.,  ..., 254., 255., 255.],\n",
      "         ...,\n",
      "         [ 22.,  25.,  23.,  ...,  37.,  41.,  40.],\n",
      "         [ 14.,  25.,  18.,  ...,  44.,  43.,  43.],\n",
      "         [ 11.,  16.,  30.,  ...,  53.,  46.,  45.]],\n",
      "\n",
      "        [[165., 157., 172.,  ..., 255., 255., 254.],\n",
      "         [159., 167., 175.,  ..., 255., 255., 255.],\n",
      "         [159., 172., 169.,  ..., 255., 255., 255.],\n",
      "         ...,\n",
      "         [ 13.,  14.,   7.,  ...,  33.,  34.,  33.],\n",
      "         [  8.,  17.,   4.,  ...,  37.,  36.,  35.],\n",
      "         [  9.,   9.,  17.,  ...,  46.,  38.,  37.]]]), 'src_kps': [[212.0, 153.0], [300.0, 130.0], [330.0, 127.0]], 'trg_gt_kps': [[39.0, 145.0], [364.0, 171.0], [416.0, 173.0]], 'trg_pred_kps': [[58.59375, 128.90625], [363.28125, 175.78125], [371.09375, 191.40625]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:21:47.549349985Z",
     "start_time": "2025-12-14T12:21:47.488761306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cella 4: calcolo PCK\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "tot_keypoints = 0\n",
    "tot_0_05 = 0\n",
    "tot_0_1 = 0\n",
    "tot_0_2 = 0\n",
    "image_results = []\n",
    "\n",
    "for res in results:\n",
    "    dists_list = res[\"distances\"]\n",
    "    num_keypoints = len(dists_list)\n",
    "\n",
    "    # Se non ci sono keypoint validi, salva comunque info ma non fare divisioni\n",
    "    if num_keypoints == 0:\n",
    "        image_results.append({\n",
    "            \"filename\": res[\"filename\"],\n",
    "            \"category\": res[\"category\"],\n",
    "            \"correct_0_05\": 0,\n",
    "            \"correct_0_1\": 0,\n",
    "            \"correct_0_2\": 0,\n",
    "            \"num_keypoints\": 0\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    dists = torch.tensor(dists_list)\n",
    "    thr_0_05 = res[\"pck_threshold_0_05\"]\n",
    "    thr_0_1 = res[\"pck_threshold_0_1\"]\n",
    "    thr_0_2 = res[\"pck_threshold_0_2\"]\n",
    "\n",
    "    tot_keypoints += num_keypoints\n",
    "\n",
    "    correct_0_05 = (dists <= thr_0_05).sum().item()\n",
    "    correct_0_1 = (dists <= thr_0_1).sum().item()\n",
    "    correct_0_2 = (dists <= thr_0_2).sum().item()\n",
    "\n",
    "    tot_0_05 += correct_0_05\n",
    "    tot_0_1 += correct_0_1\n",
    "    tot_0_2 += correct_0_2\n",
    "\n",
    "    image_results.append({\n",
    "        \"filename\": res[\"filename\"],\n",
    "        \"category\": res[\"category\"],\n",
    "        \"correct_0_05\": correct_0_05,\n",
    "        \"correct_0_1\": correct_0_1,\n",
    "        \"correct_0_2\": correct_0_2,\n",
    "        \"num_keypoints\": num_keypoints\n",
    "    })\n",
    "\n",
    "# --- Per keypoints (micro-average) ---\n",
    "print(\"PCK Results per keypoints:\")\n",
    "df_keypoints = pd.DataFrame({\n",
    "    \"PCK 0.05\": [tot_0_05 / tot_keypoints if tot_keypoints > 0 else np.nan],\n",
    "    \"PCK 0.1\": [tot_0_1 / tot_keypoints if tot_keypoints > 0 else np.nan],\n",
    "    \"PCK 0.2\": [tot_0_2 / tot_keypoints if tot_keypoints > 0 else np.nan],\n",
    "})\n",
    "print(df_keypoints)\n",
    "\n",
    "# --- Per image (macro-average) SOLO su immagini con num_keypoints > 0 ---\n",
    "valid_imgs = [r for r in image_results if r[\"num_keypoints\"] > 0]\n",
    "\n",
    "print(\"PCK Results per image:\")\n",
    "df_image = pd.DataFrame({\n",
    "    \"PCK 0.05\": [np.mean([r[\"correct_0_05\"] / r[\"num_keypoints\"] for r in valid_imgs]) if valid_imgs else np.nan],\n",
    "    \"PCK 0.1\": [np.mean([r[\"correct_0_1\"] / r[\"num_keypoints\"] for r in valid_imgs]) if valid_imgs else np.nan],\n",
    "    \"PCK 0.2\": [np.mean([r[\"correct_0_2\"] / r[\"num_keypoints\"] for r in valid_imgs]) if valid_imgs else np.nan],\n",
    "})\n",
    "print(df_image)"
   ],
   "id": "a09efdf778486b41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCK Results per keypoints:\n",
      "   PCK 0.05   PCK 0.1  PCK 0.2\n",
      "0  0.333333  0.666667      1.0\n",
      "PCK Results per image:\n",
      "   PCK 0.05   PCK 0.1  PCK 0.2\n",
      "0  0.333333  0.666667      1.0\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:21:47.605095657Z",
     "start_time": "2025-12-14T12:21:47.551840177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "category_results = {}\n",
    "\n",
    "for r in image_results:\n",
    "    cat = r[\"category\"]\n",
    "    if cat not in category_results:\n",
    "        category_results[cat] = {\n",
    "            \"correct_0_05\": 0,\n",
    "            \"correct_0_1\": 0,\n",
    "            \"correct_0_2\": 0,\n",
    "            \"total_keypoints\": 0\n",
    "        }\n",
    "\n",
    "    category_results[cat][\"correct_0_05\"] += r[\"correct_0_05\"]\n",
    "    category_results[cat][\"correct_0_1\"] += r[\"correct_0_1\"]\n",
    "    category_results[cat][\"correct_0_2\"] += r[\"correct_0_2\"]\n",
    "    category_results[cat][\"total_keypoints\"] += r[\"num_keypoints\"]\n",
    "\n",
    "rows = []\n",
    "for cat, stats in category_results.items():\n",
    "    total = stats[\"total_keypoints\"]\n",
    "\n",
    "    rows.append({\n",
    "        \"Category\": cat,\n",
    "        \"Total keypoints\": total,\n",
    "        \"PCK 0.05\": (stats[\"correct_0_05\"] / total) if total > 0 else np.nan,\n",
    "        \"PCK 0.1\": (stats[\"correct_0_1\"] / total) if total > 0 else np.nan,\n",
    "        \"PCK 0.2\": (stats[\"correct_0_2\"] / total) if total > 0 else np.nan,\n",
    "    })\n",
    "\n",
    "df_cat = pd.DataFrame(rows).sort_values(\"Category\")\n",
    "\n",
    "print(\"PCK Results per category:\")\n",
    "print(df_cat)"
   ],
   "id": "4d94b7e847f3f1ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCK Results per category:\n",
      "    Category  Total keypoints  PCK 0.05   PCK 0.1  PCK 0.2\n",
      "0  aeroplane                3  0.333333  0.666667      1.0\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T12:28:15.622316755Z",
     "start_time": "2025-12-14T12:28:14.815083490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualization and saving of qualitative results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_to_image(img: torch.Tensor) -> np.ndarray:\n",
    "    img = img.detach().cpu()  # sicurezza\n",
    "    img = img.permute(1, 2, 0)  # [H,W,3]\n",
    "    img = img.numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_keypoints(ax, image, keypoints, color, label=None, marker='o'):\n",
    "    ax.imshow(image)\n",
    "    if len(keypoints) > 0:\n",
    "        xs = [kp[0] for kp in keypoints]\n",
    "        ys = [kp[1] for kp in keypoints]\n",
    "        ax.scatter(xs, ys, c=color, s=40, marker=marker, label=label)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_keypoints_save(\n",
    "        src_img_chw: torch.Tensor,\n",
    "        trg_img_chw: torch.Tensor,\n",
    "        qualitative_result: dict,\n",
    "        dpi: int = 200\n",
    ") -> None:\n",
    "    # --- cartella output ---\n",
    "    save_dir = os.path.join(base_dir, \"qualitative-results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(save_dir, f\"{selected_model}_src.png\")\n",
    "    gt_path = os.path.join(save_dir, f\"{selected_model}_trg_gt.png\")\n",
    "    pr_path = os.path.join(save_dir, f\"{selected_model}_trg_pred.png\")\n",
    "\n",
    "    # --- immagini ---\n",
    "    src_img = tensor_to_image(src_img_chw)\n",
    "    trg_img = tensor_to_image(trg_img_chw)\n",
    "\n",
    "    src_kps = qualitative_result[\"src_kps\"]  # [[x,y],...]\n",
    "    trg_gt = qualitative_result[\"trg_gt_kps\"]  # [[x,y],...]\n",
    "    trg_pr = qualitative_result[\"trg_pred_kps\"]  # [[x,y],...]\n",
    "\n",
    "    n = min(len(src_kps), len(trg_gt), len(trg_pr))\n",
    "    cmap = plt.get_cmap(\"tab10\", max(n, 1))\n",
    "\n",
    "    # ---------- 1) SOURCE ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(src_img)\n",
    "    ax.set_title(\"Source\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xs, ys = src_kps[i]\n",
    "        ax.scatter(xs, ys, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xs + 6, ys + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(src_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 2) TARGET GT ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target GT\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        ax.scatter(xg, yg, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xg + 6, yg + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gt_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 3) TARGET PRED ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target Pred\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        xp, yp = trg_pr[i]\n",
    "\n",
    "        # pred: pallino vuoto\n",
    "        ax.scatter(xp, yp, s=60, color=c,  linewidths=2, marker=\"o\")\n",
    "        ax.text(xp + 6, yp + 6, str(i), color=c, fontsize=10)\n",
    "\n",
    "        # (opzionale) linea di errore GT->Pred\n",
    "        ax.plot([xg, xp], [yg, yp], color=c, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pr_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "plot_keypoints_save(\n",
    "    src_img_chw=batch[\"src_img\"],\n",
    "    trg_img_chw=batch[\"trg_img\"],\n",
    "    qualitative_result=qualitative_result\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a0bcd406ee3f4f98",
   "outputs": [],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
