{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:19:05.414792114Z",
     "start_time": "2025-12-30T18:19:03.358824993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data.pfwillow import PFWillowDataset\n",
    "from typing import TypedDict, Tuple, Any\n",
    "import torch\n",
    "\n",
    "from data.pfpascal import PFPascalDataset\n",
    "from data.spair import SPairDataset\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "using_colab = False\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    !wget -P ./AML-polito/dataset/ \"https://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    !tar -xvzf ./AML-polito/dataset/SPair-71k.tar.gz -C ./AML-polito/dataset/\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "dataset_size = 'large'\n",
    "#pck_alpha = 0.05\n",
    "\n",
    "# Verifica che i percorsi esistano prima di creare il dataset\n",
    "if os.path.exists(dataset_dir):\n",
    "    trn_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='trn')\n",
    "    val_dataset = SPairDataset(dataset_size=dataset_size, dataset_dir=dataset_dir, datatype='val')\n",
    "    test_dataset = PFWillowDataset(dataset_dir=dataset_dir, datatype='test')\n",
    "\n",
    "    trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1, collate_fn=collate_single)\n",
    "    print(\"Dataset caricati correttamente.\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Errore: Impossibile trovare i percorsi del dataset in '{dataset_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricati correttamente.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:19:06.495947834Z",
     "start_time": "2025-12-30T18:19:05.430132472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# inizializzazione SAM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === 1) Carico SAM ===\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# parametri utili\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "#PATCH = IMG_SIZE // 64  # 16\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "print(f\"SAM modello '{selected_model}' caricato. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM modello 'vit_b' caricato. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:19:06.591996641Z",
     "start_time": "2025-12-30T18:19:06.542942917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale → lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "c0b1d5b0579cac9c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:19:06.989750088Z",
     "start_time": "2025-12-30T18:19:06.941680916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activation = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        activation[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "# Registrazione hook (fallo UNA volta sola prima del loop)\n",
    "# Scegliamo alcuni indici. Per ViT-B (base) ci sono 12 blocchi (0-11).\n",
    "target_layers = [2, 5, 8, 11]\n",
    "for i in target_layers:\n",
    "    predictor.model.image_encoder.blocks[i].register_forward_hook(get_activation(f'layer_{i}'))"
   ],
   "id": "3c087326ad2f11de",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:19:24.417977668Z",
     "start_time": "2025-12-30T18:19:06.991358673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.compute_results import *\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dizionario finale: chiave = layer, valore = lista (stessa struttura di results originale)\n",
    "all_results = {li: [] for li in target_layers}\n",
    "\n",
    "# (opzionale) qualitativo per layer (solo sul primo batch)\n",
    "qualitative_by_layer = {\n",
    "    li: {\n",
    "        \"src_img\": None,\n",
    "        \"trg_img\": None,\n",
    "        \"src_kps\": [],\n",
    "        \"trg_gt_kps\": [],\n",
    "        \"trg_pred_kps\": []\n",
    "    }\n",
    "    for li in target_layers\n",
    "}\n",
    "\n",
    "max_images = 10\n",
    "data = islice(test_dataloader, max_images)\n",
    "size = max_images\n",
    "if using_colab:\n",
    "    data = test_dataloader\n",
    "    size = len(test_dataloader)\n",
    "\n",
    "img_enc = predictor.model.image_encoder  # contiene .neck\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, batch in enumerate(tqdm(data, total=size, desc=f\"Elaborazione con SAM {selected_model}\")):\n",
    "\n",
    "        category = batch[\"category\"]\n",
    "        src_img = batch[\"src_img\"].to(device).unsqueeze(0)  # [1,3,Hs,Ws]\n",
    "        trg_img = batch[\"trg_img\"].to(device).unsqueeze(0)  # [1,3,Ht,Wt]\n",
    "        orig_size_src = tuple(batch[\"src_imsize\"][1:])  # (Hs, Ws)\n",
    "        orig_size_trg = tuple(batch[\"trg_imsize\"][1:])   # (Ht, Wt)\n",
    "\n",
    "        src_resized = predictor.transform.apply_image_torch(src_img)  # [1,3,Hs',Ws']\n",
    "        trg_resized = predictor.transform.apply_image_torch(trg_img)  # [1,3,Ht',Wt']\n",
    "\n",
    "        # --- 1) Forward sorgente (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(src_resized, orig_size_src)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        src_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 2) Forward target (riempie activation con output dei blocchi) ---\n",
    "        predictor.set_torch_image(trg_resized, orig_size_trg)\n",
    "        _ = predictor.get_image_embedding()[0]  # trigger hooks\n",
    "        trg_intermediate_emb = {k: v.detach().clone() for k, v in activation.items()}\n",
    "\n",
    "        # --- 3) Keypoints & metadata ---\n",
    "        src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "        pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "        pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "        pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "        # --- regione valida target (per evitare padding) ---\n",
    "        H_prime, W_prime = trg_resized.shape[-2:]\n",
    "        hv_t = math.ceil(H_prime / PATCH)\n",
    "        wv_t = math.ceil(W_prime / PATCH)\n",
    "\n",
    "        N_kps = src_kps.shape[0]\n",
    "\n",
    "        # (opzionale) salva immagini una sola volta per layer quando iter==0\n",
    "        if iter == 0:\n",
    "            for li in target_layers:\n",
    "                qualitative_by_layer[li][\"src_img\"] = batch[\"src_img\"]\n",
    "                qualitative_by_layer[li][\"trg_img\"] = batch[\"trg_img\"]\n",
    "\n",
    "        # ===== Loop sui layer: riuso activation, NON rifaccio forward =====\n",
    "        for selected_layer in target_layers:\n",
    "\n",
    "            # Hook output: [1,64,64,768] (NHWC)\n",
    "            src_hook = src_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "            trg_hook = trg_intermediate_emb[f\"layer_{selected_layer}\"]\n",
    "\n",
    "            # NHWC -> NCHW : [1,768,64,64]\n",
    "            src_feat = src_hook.permute(0, 3, 1, 2).contiguous()\n",
    "            trg_feat = trg_hook.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "            # ======= QUI la scelta pre-neck / post-neck =======\n",
    "            if selected_layer <= 4:  # \"iniziale\" (puoi cambiare soglia: 3/4/5)\n",
    "                # PRE-NECK: uso direttamente i 768 canali\n",
    "                # (facoltativo ma consigliato) L2-normalizzazione per cosine\n",
    "                src_emb = F.normalize(src_feat, dim=1)[0]   # [768,64,64]\n",
    "                trg_emb = F.normalize(trg_feat, dim=1)[0]   # [768,64,64]\n",
    "            else:\n",
    "                # POST-NECK: porto a 256 canali come output standard SAM\n",
    "                src_emb = img_enc.neck(src_feat)[0]         # [256,64,64]\n",
    "                trg_emb = img_enc.neck(trg_feat)[0]         # [256,64,64]\n",
    "                # (facoltativo) normalizza anche qui se vuoi massima coerenza\n",
    "                # src_emb = F.normalize(src_emb, dim=0)\n",
    "                # trg_emb = F.normalize(trg_emb, dim=0)\n",
    "            # ================================================\n",
    "\n",
    "            C_ft = trg_emb.shape[0]\n",
    "\n",
    "            trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "            trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "            distances_this_image = []\n",
    "\n",
    "            for i in range(N_kps):\n",
    "                src_keypoint = src_kps[i].unsqueeze(0)  # [1,2] (x,y)\n",
    "                trg_keypoint = trg_kps[i]              # [2]   (x,y)\n",
    "\n",
    "                if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                    continue\n",
    "\n",
    "                # originale src -> feature src\n",
    "                x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "                # feature vector sorgente\n",
    "                src_vec = src_emb[:, y_idx, x_idx]  # [256]\n",
    "\n",
    "                # cosine similarity con tutte le posizioni valide del target\n",
    "                sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "                max_idx = torch.argmax(sim).item()\n",
    "                y_idx_t = max_idx // wv_t\n",
    "                x_idx_t = max_idx % wv_t\n",
    "\n",
    "                # feature target -> pixel originali target\n",
    "                x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "                if iter == 0:\n",
    "                    qualitative_by_layer[selected_layer][\"src_kps\"].append(src_keypoint.squeeze(0).tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_gt_kps\"].append(trg_keypoint.tolist())\n",
    "                    qualitative_by_layer[selected_layer][\"trg_pred_kps\"].append([x_pred, y_pred])\n",
    "\n",
    "                dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "                distances_this_image.append(dist)\n",
    "\n",
    "            # Append con struttura IDENTICA all'originale, ma salvata per-layer\n",
    "            all_results[selected_layer].append(\n",
    "                CorrespondenceResult(\n",
    "                    category=category,\n",
    "                    distances=distances_this_image,\n",
    "                    pck_threshold_0_05=pck_thr_0_05,\n",
    "                    pck_threshold_0_1=pck_thr_0_1,\n",
    "                    pck_threshold_0_2=pck_thr_0_2\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # cleanup per-layer\n",
    "            del src_hook, trg_hook, src_feat, trg_feat, src_emb, trg_emb, trg_valid, trg_flat, src_vec, sim\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # cleanup per-batch\n",
    "        predictor.reset_image()\n",
    "        torch.cuda.empty_cache()"
   ],
   "id": "c888ec88287af227",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b: 100%|██████████| 10/10 [00:17<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elaborazione completata.\n",
      "Layer 2\n",
      "PCK Results per keypoints (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)       6.0      9.0     25.0\n",
      "1      All       6.0      9.0     25.0\n",
      "PCK per-image (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)       6.0      9.0     25.0\n",
      "1      All       6.0      9.0     25.0\n",
      "##################################################\n",
      "\n",
      "Layer 5\n",
      "PCK Results per keypoints (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)      14.0     28.0     38.0\n",
      "1      All      14.0     28.0     38.0\n",
      "PCK per-image (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)      14.0     28.0     38.0\n",
      "1      All      14.0     28.0     38.0\n",
      "##################################################\n",
      "\n",
      "Layer 8\n",
      "PCK Results per keypoints (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)      25.0     42.0     58.0\n",
      "1      All      25.0     42.0     58.0\n",
      "PCK per-image (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)      25.0     42.0     58.0\n",
      "1      All      25.0     42.0     58.0\n",
      "##################################################\n",
      "\n",
      "Layer 11\n",
      "PCK Results per keypoints (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)      23.0     46.0     63.0\n",
      "1      All      23.0     46.0     63.0\n",
      "PCK per-image (%):\n",
      "  Category  PCK 0.05  PCK 0.1  PCK 0.2\n",
      "0   car(G)      23.0     46.0     63.0\n",
      "1      All      23.0     46.0     63.0\n",
      "##################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute and print results per layer\n",
    "for li in target_layers:\n",
    "    print(f\"Layer {li}\")\n",
    "    correct = compute_correct_per_category(all_results[li])\n",
    "    compute_pckt_keypoints(correct)\n",
    "    compute_pckt_images(correct)\n",
    "    print(\"<\"+\"=\" * 50+\">\")\n",
    "    print(\"\")"
   ],
   "id": "e3a7661228d5944c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T18:19:24.817444042Z",
     "start_time": "2025-12-30T18:19:24.633785071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualization and saving of qualitative results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_to_image(img: torch.Tensor) -> np.ndarray:\n",
    "    img = img.detach().cpu()  # sicurezza\n",
    "    img = img.permute(1, 2, 0)  # [H,W,3]\n",
    "    img = img.numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def draw_keypoints(ax, image, keypoints, color, label=None, marker='o'):\n",
    "    ax.imshow(image)\n",
    "    if len(keypoints) > 0:\n",
    "        xs = [kp[0] for kp in keypoints]\n",
    "        ys = [kp[1] for kp in keypoints]\n",
    "        ax.scatter(xs, ys, c=color, s=40, marker=marker, label=label)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_keypoints_save(\n",
    "        src_img_chw: torch.Tensor,\n",
    "        trg_img_chw: torch.Tensor,\n",
    "        qualitative_result: dict,\n",
    "        dpi: int = 200\n",
    ") -> None:\n",
    "    # --- cartella output ---\n",
    "    save_dir = os.path.join(base_dir, \"qualitative-results\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    src_path = os.path.join(save_dir, f\"{selected_model}_src.png\")\n",
    "    gt_path = os.path.join(save_dir, f\"{selected_model}_trg_gt.png\")\n",
    "    pr_path = os.path.join(save_dir, f\"{selected_model}_trg_pred.png\")\n",
    "\n",
    "    # --- immagini ---\n",
    "    src_img = tensor_to_image(src_img_chw)\n",
    "    trg_img = tensor_to_image(trg_img_chw)\n",
    "\n",
    "    src_kps = qualitative_result[\"src_kps\"]  # [[x,y],...]\n",
    "    trg_gt = qualitative_result[\"trg_gt_kps\"]  # [[x,y],...]\n",
    "    trg_pr = qualitative_result[\"trg_pred_kps\"]  # [[x,y],...]\n",
    "\n",
    "    n = min(len(src_kps), len(trg_gt), len(trg_pr))\n",
    "    cmap = plt.get_cmap(\"tab10\", max(n, 1))\n",
    "\n",
    "    # ---------- 1) SOURCE ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(src_img)\n",
    "    ax.set_title(\"Source\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xs, ys = src_kps[i]\n",
    "        ax.scatter(xs, ys, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xs + 6, ys + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(src_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 2) TARGET GT ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target GT\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        ax.scatter(xg, yg, s=60, color=c, marker=\"o\")\n",
    "        ax.text(xg + 6, yg + 6, str(i), color=c, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(gt_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- 3) TARGET PRED ----------\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(trg_img)\n",
    "    ax.set_title(\"Target Pred\")\n",
    "    ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        c = cmap(i)\n",
    "        xg, yg = trg_gt[i]\n",
    "        xp, yp = trg_pr[i]\n",
    "\n",
    "        # pred: pallino vuoto\n",
    "        ax.scatter(xp, yp, s=60, color=c, linewidths=2, marker=\"o\")\n",
    "        ax.text(xp + 6, yp + 6, str(i), color=c, fontsize=10)\n",
    "\n",
    "        # (opzionale) linea di errore GT->Pred\n",
    "        ax.plot([xg, xp], [yg, yp], color=c, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pr_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "plot_keypoints_save(\n",
    "    src_img_chw=batch[\"src_img\"],\n",
    "    trg_img_chw=batch[\"trg_img\"],\n",
    "    qualitative_result=qualitative_result\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a0bcd406ee3f4f98",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qualitative_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 100\u001B[39m\n\u001B[32m     93\u001B[39m     plt.savefig(pr_path, dpi=dpi, bbox_inches=\u001B[33m\"\u001B[39m\u001B[33mtight\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     94\u001B[39m     plt.close(fig)\n\u001B[32m     97\u001B[39m plot_keypoints_save(\n\u001B[32m     98\u001B[39m     src_img_chw=batch[\u001B[33m\"\u001B[39m\u001B[33msrc_img\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     99\u001B[39m     trg_img_chw=batch[\u001B[33m\"\u001B[39m\u001B[33mtrg_img\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m     qualitative_result=\u001B[43mqualitative_result\u001B[49m\n\u001B[32m    101\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'qualitative_result' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "12deb7cd23ce024",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
