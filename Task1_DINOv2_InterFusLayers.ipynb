{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y8AKrNt5lxaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' estratto con successo nella directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
      "Contenuti della directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted':\n",
      "['SPair-71k']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_file_path = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' \n",
    "extract_dir = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
    "\n",
    "# Crea la directory di estrazione se non esiste\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Estrai il file ZIP solo se esiste\n",
    "if os.path.exists(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"File '{zip_file_path}' estratto con successo nella directory '{extract_dir}'\")\n",
    "    print(f\"Contenuti della directory '{extract_dir}':\\n{os.listdir(extract_dir)}\")\n",
    "else:\n",
    "    print(f\"File zip '{zip_file_path}' non trovato. Assicurati che il dataset sia estratto in '{extract_dir}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "honcpimEq_B2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricati correttamente.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, image_keys):\n",
    "        self.image_keys = image_keys\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for key in self.image_keys:\n",
    "            image[key] /= 255.0\n",
    "            image[key] = self.normalize(image[key])\n",
    "        return image\n",
    "\n",
    "\n",
    "def read_img(path):\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "\n",
    "    return torch.tensor(img.transpose(2, 0, 1).astype(np.float32))\n",
    "\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n",
    "\n",
    "        self.datatype = datatype\n",
    "        self.pck_alpha = pck_alpha\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = self.ann_files[:len(self.ann_files) - 1]\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        self.categories = list(map(lambda x: os.path.basename(x), glob.glob('%s/*' % image_path)))\n",
    "        self.categories.sort()\n",
    "        self.transform = Normalize(['src_img', 'trg_img'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        raw_line = self.ann_files[idx]\n",
    "        ann_filename = raw_line.replace(':', '_')\n",
    "        ann_file = ann_filename + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_img = read_img(os.path.join(self.image_path, category, annotation['src_imname']))\n",
    "        trg_img = read_img(os.path.join(self.image_path, category, annotation['trg_imname']))\n",
    "\n",
    "        trg_bbox = annotation['trg_bndbox']\n",
    "        pck_threshold = max(trg_bbox[2] - trg_bbox[0],  trg_bbox[3] - trg_bbox[1]) * self.pck_alpha\n",
    "\n",
    "        sample = {'pair_id': annotation['pair_id'],\n",
    "                  'filename': annotation['filename'],\n",
    "                  'src_imname': annotation['src_imname'],\n",
    "                  'trg_imname': annotation['trg_imname'],\n",
    "                  'src_imsize': src_img.size(),\n",
    "                  'trg_imsize': trg_img.size(),\n",
    "\n",
    "                  'src_bbox': annotation['src_bndbox'],\n",
    "                  'trg_bbox': annotation['trg_bndbox'],\n",
    "                  'category': annotation['category'],\n",
    "\n",
    "                  'src_pose': annotation['src_pose'],\n",
    "                  'trg_pose': annotation['trg_pose'],\n",
    "\n",
    "                  'src_img': src_img,\n",
    "                  'trg_img': trg_img,\n",
    "                  'src_kps': torch.tensor(annotation['src_kps']).float(),\n",
    "                  'trg_kps': torch.tensor(annotation['trg_kps']).float(),\n",
    "\n",
    "                  'mirror': annotation['mirror'],\n",
    "                  'vp_var': annotation['viewpoint_variation'],\n",
    "                  'sc_var': annotation['scale_variation'],\n",
    "                  'truncn': annotation['truncation'],\n",
    "                  'occlsn': annotation['occlusion'],\n",
    "\n",
    "                  'pck_threshold': pck_threshold}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_dir = r\"C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted\\SPair-71k\\SPair-71k\"    \n",
    "    pair_ann_path = os.path.join(base_dir, 'PairAnnotation')\n",
    "    layout_path = os.path.join(base_dir, 'Layout')\n",
    "    image_path = os.path.join(base_dir, 'JPEGImages')\n",
    "    dataset_size = 'large'\n",
    "    pck_alpha = 0.1\n",
    "    \n",
    "    # Verifica che i percorsi esistano prima di creare il dataset\n",
    "    if os.path.exists(pair_ann_path) and os.path.exists(layout_path) and os.path.exists(image_path):\n",
    "        trn_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='trn')\n",
    "        val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='val')\n",
    "        test_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='test')\n",
    "\n",
    "        trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "        val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "        test_dataloader = DataLoader(test_dataset, num_workers=0)\n",
    "        print(\"Dataset caricati correttamente.\")\n",
    "    else:\n",
    "        print(f\"Errore: Impossibile trovare i percorsi del dataset in '{base_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Official DINOv2 Model from Torch Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 12234/12234 [27:17<00:00,  7.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# PCK PER POINT - OFFICIAL DINOv2 VERSION (INTERMEDIATE LAYERS + PADDING FIX)\n",
    "import torch\n",
    "import math \n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. LOAD OFFICIAL MODEL\n",
    "print(\"Loading Official DINOv2 Model from Torch Hub...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "\n",
    "# Helper function for Padding\n",
    "def pad_to_multiple(x, k=14):\n",
    "    \"\"\"\n",
    "    Pads the image (bottom and right) so that H and W are multiples of k.\n",
    "    \"\"\"\n",
    "    h, w = x.shape[-2:]\n",
    "    new_h = math.ceil(h / k) * k\n",
    "    new_w = math.ceil(w / k) * k\n",
    "    \n",
    "    pad_bottom = new_h - h\n",
    "    pad_right = new_w - w\n",
    "    \n",
    "    if pad_bottom == 0 and pad_right == 0:\n",
    "        return x\n",
    "    return F.pad(x, (0, pad_right, 0, pad_bottom), value=0)\n",
    "\n",
    "# Initialize counters\n",
    "total_keypoints = 0\n",
    "correct_kps_0_05 = 0\n",
    "class_pck_data = {}\n",
    "\n",
    "with torch.no_grad(): # Disable gradients\n",
    "    for i, data in enumerate(tqdm(test_dataloader, desc=\"Evaluation\")):\n",
    "        \n",
    "        category = data['category'][0]\n",
    "        if category not in class_pck_data:\n",
    "            class_pck_data[category] = {\n",
    "                'total_keypoints': 0,\n",
    "                'correct_kps_0_05': 0\n",
    "            }\n",
    "\n",
    "        src_img = data['src_img'].to(device)\n",
    "        trg_img = data['trg_img'].to(device)\n",
    "        \n",
    "\n",
    "        # --- FIX: APPLY PADDING ---\n",
    "        # Ensure dimensions are multiples of 14 to avoid AssertionError\n",
    "        src_img_padded = pad_to_multiple(src_img, 14)\n",
    "        trg_img_padded = pad_to_multiple(trg_img, 14)\n",
    "\n",
    "        # 2. FORWARD PASS (INTERMEDIATE LAYERS STRATEGY)\n",
    "        # Instead of just the last layer, we grab the last 4 layers.\n",
    "        # reshape=False -> Returns [Batch, Patch, Channels]\n",
    "        # return_class_token=False -> Automatically removes CLS token!\n",
    "        \n",
    "        n_layers = 4\n",
    "        layers_src = model.get_intermediate_layers(src_img_padded, n=n_layers, reshape=False, return_class_token=False)\n",
    "        layers_trg = model.get_intermediate_layers(trg_img_padded, n=n_layers, reshape=False, return_class_token=False)\n",
    "        \n",
    "       # --- FIX IMPORTANTE: L2 NORMALIZATION ---\n",
    "        # Normalizziamo ogni singolo layer separatamente PRIMA di concatenare.\n",
    "        # Questo impedisce che i layer con valori più alti (es. l'ultimo) dominino il calcolo.\n",
    "        layers_src_norm = [F.normalize(layer, p=2, dim=-1, eps=1e-6) for layer in layers_src]\n",
    "        layers_trg_norm = [F.normalize(layer, p=2, dim=-1, eps=1e-6) for layer in layers_trg]\n",
    "        \n",
    "        # Ora concateniamo i tensori normalizzati\n",
    "        # Shape finale: [1, N_patches, 768 * 4] = [1, N_patches, 3072]\n",
    "        feats_src = torch.cat(layers_src_norm, dim=-1) \n",
    "        feats_trg = torch.cat(layers_trg_norm, dim=-1)\n",
    "        # --- IMPORTANT: GRID CALCULATION ---\n",
    "        # We must use PADDED dimensions for the grid\n",
    "        _, _, H_padded, W_padded = src_img_padded.shape \n",
    "        \n",
    "        # We keep ORIGINAL dimensions for valid boundary checks\n",
    "        _, _, H_orig, W_orig = data['src_img'].shape\n",
    "\n",
    "        patch_size = 14\n",
    "        w_grid = W_padded // patch_size \n",
    "        h_grid = H_padded // patch_size\n",
    "\n",
    "        kps_list_src = data['src_kps'][0] \n",
    "        trg_kps_gt = data['trg_kps'][0] \n",
    "        \n",
    "        # Get threshold value\n",
    "        pck_threshold = data['pck_threshold'][0].item() \n",
    "        \n",
    "        for n_keypoint, keypoint_src in enumerate(kps_list_src):\n",
    "\n",
    "            x_src_val = keypoint_src[0].item()\n",
    "            y_src_val = keypoint_src[1].item()\n",
    "\n",
    "            # NaN Check\n",
    "            if math.isnan(x_src_val) or math.isnan(y_src_val):\n",
    "                continue\n",
    "            \n",
    "            x_pixel_src = int(x_src_val)\n",
    "            y_pixel_src = int(y_src_val)\n",
    "\n",
    "            # Boundary Check on ORIGINAL image\n",
    "            if not (0 <= x_pixel_src < W_orig and 0 <= y_pixel_src < H_orig):\n",
    "                continue\n",
    "\n",
    "            # Grid Clamp\n",
    "            x_patch_src = min(x_pixel_src // patch_size, w_grid - 1)\n",
    "            y_patch_src = min(y_pixel_src // patch_size, h_grid - 1)\n",
    "\n",
    "            # 3. INDEX CALCULATION\n",
    "            # No +1 needed because get_intermediate_layers(return_class_token=False) removes it\n",
    "            patch_index_src = (y_patch_src * w_grid) + x_patch_src\n",
    "\n",
    "            # Safety clamp for index\n",
    "            if patch_index_src >= feats_src.shape[1]:\n",
    "                patch_index_src = feats_src.shape[1] - 1\n",
    "\n",
    "            # Extract Vector (Now it is size 3072 instead of 768)\n",
    "            source_vec = feats_src[0, patch_index_src, :]\n",
    "\n",
    "            # Cosine Similarity\n",
    "            similarity_map = torch.cosine_similarity(source_vec, feats_trg[0], dim=-1)\n",
    "            \n",
    "            # Prediction\n",
    "            patch_idx_spatial = torch.argmax(similarity_map).item()\n",
    "\n",
    "            # Convert Index -> Grid -> Pixel\n",
    "            x_col_pred = patch_idx_spatial % w_grid\n",
    "            y_row_pred = patch_idx_spatial // w_grid\n",
    "\n",
    "            x_pred_pixel = x_col_pred * patch_size + (patch_size // 2)\n",
    "            y_pred_pixel = y_row_pred * patch_size + (patch_size // 2)\n",
    "\n",
    "            # Ground Truth Check\n",
    "            gt_x = trg_kps_gt[n_keypoint, 0].item()\n",
    "            gt_y = trg_kps_gt[n_keypoint, 1].item()\n",
    "\n",
    "            if math.isnan(gt_x) or math.isnan(gt_y):\n",
    "                continue\n",
    "            if not (0 <= gt_x < W_orig and 0 <= gt_y < H_orig):\n",
    "                continue\n",
    "\n",
    "            # Distance & Update\n",
    "            distance = math.sqrt((x_pred_pixel - gt_x)**2 + (y_pred_pixel - gt_y)**2)\n",
    "\n",
    "            class_pck_data[category]['total_keypoints'] += 1\n",
    "            if distance <= pck_threshold:\n",
    "                class_pck_data[category]['correct_kps_0_05'] += 1\n",
    "        \n",
    "        # Free memory (Important with bigger feature vectors)\n",
    "        del feats_src, feats_trg, layers_src, layers_trg, src_img_padded, trg_img_padded\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Official DINOv2 Model from Torch Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato su: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valutazione: 100%|██████████| 12234/12234 [48:44<00:00,  4.18it/s]     \n"
     ]
    }
   ],
   "source": [
    "# CALCOLO PCK PER IMAGE (OFFICIAL DINOv2 VERSION + PADDING FIX)\n",
    "import torch\n",
    "import math \n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. LOAD OFFICIAL MODEL (Replaces Hugging Face)\n",
    "print(\"Loading Official DINOv2 Model from Torch Hub...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Modello caricato su: {device}\")\n",
    "\n",
    "# Helper function for Padding (Crucial for DINOv2 official)\n",
    "def pad_to_multiple(x, k=14):\n",
    "    \"\"\"\n",
    "    Pads the image (bottom and right) so that H and W are multiples of k.\n",
    "    \"\"\"\n",
    "    h, w = x.shape[-2:]\n",
    "    new_h = math.ceil(h / k) * k\n",
    "    new_w = math.ceil(w / k) * k\n",
    "    \n",
    "    pad_bottom = new_h - h\n",
    "    pad_right = new_w - w\n",
    "    \n",
    "    if pad_bottom == 0 and pad_right == 0:\n",
    "        return x\n",
    "    return F.pad(x, (0, pad_right, 0, pad_bottom), value=0)\n",
    "\n",
    "# Inizializza i contatori per la metrica PCK\n",
    "class_pck_data = {}\n",
    "\n",
    "with torch.no_grad(): # Disabilita il calcolo dei gradienti\n",
    "    for i, data in enumerate(tqdm(test_dataloader, desc=\"Valutazione\")):\n",
    "\n",
    "        # Retrieve the category for the current item\n",
    "        category = data['category'][0]\n",
    "\n",
    "        # Initialize category entry in class_pck_data if it doesn't exist\n",
    "        if category not in class_pck_data:\n",
    "            class_pck_data[category] = {\n",
    "                'total_image': 0,\n",
    "                'image_value_sum': 0, # Accumulatore per le medie delle singole immagini\n",
    "            }\n",
    "\n",
    "        # Counters specific for THIS image\n",
    "        img_tot_keypoints = 0\n",
    "        img_correct_keypoints = 0\n",
    "\n",
    "        src_img = data['src_img'].to(device)\n",
    "        trg_img = data['trg_img'].to(device)\n",
    "\n",
    "        # --- FIX: APPLY PADDING ---\n",
    "        # Ensure dimensions are multiples of 14 to avoid AssertionError\n",
    "        src_img_padded = pad_to_multiple(src_img, 14)\n",
    "        trg_img_padded = pad_to_multiple(trg_img, 14)\n",
    "\n",
    "        n_layers = 4\n",
    "        layers_src = model.get_intermediate_layers(src_img_padded, n=n_layers, reshape=False, return_class_token=False)\n",
    "        layers_trg = model.get_intermediate_layers(trg_img_padded, n=n_layers, reshape=False, return_class_token=False)\n",
    "        \n",
    "        # Concatenate the layers along the feature dimension (dim=-1)\n",
    "        # Each layer has 768 dim. 4 layers -> 768 * 4 = 3072 dim.\n",
    "        feats_src = torch.cat(layers_src, dim=-1) \n",
    "        feats_trg = torch.cat(layers_trg, dim=-1)\n",
    "\n",
    "       \n",
    "        # --- IMPORTANT: GRID CALCULATION ---\n",
    "        # We must use PADDED dimensions for the grid, otherwise indices will drift\n",
    "        _, _, H_padded, W_padded = src_img_padded.shape \n",
    "        \n",
    "        # We keep ORIGINAL dimensions for valid boundary checks (Ground Truth)\n",
    "        _, _, H_orig, W_orig = data['src_img'].shape\n",
    "\n",
    "        patch_size = 14\n",
    "        w_grid = W_padded // patch_size \n",
    "        h_grid = H_padded // patch_size\n",
    "\n",
    "        kps_list_src = data['src_kps'][0] \n",
    "        trg_kps_gt = data['trg_kps'][0] \n",
    "        \n",
    "        # Estrai threshold value dal tensore\n",
    "        pck_threshold = data['pck_threshold'][0].item() \n",
    "        \n",
    "        for n_keypoint, keypoint_src in enumerate(kps_list_src):\n",
    "\n",
    "            x_src_val = keypoint_src[0].item()\n",
    "            y_src_val = keypoint_src[1].item()\n",
    "\n",
    "            # CHECK 1: NaN / Validità Keypoint Sorgente\n",
    "            if math.isnan(x_src_val) or math.isnan(y_src_val):\n",
    "                continue\n",
    "            \n",
    "            x_pixel_src = int(x_src_val)\n",
    "            y_pixel_src = int(y_src_val)\n",
    "\n",
    "            # Boundary Check on ORIGINAL image (ignore points in padded area)\n",
    "            if not (0 <= x_pixel_src < W_orig and 0 <= y_pixel_src < H_orig):\n",
    "                continue\n",
    "\n",
    "            # CHECK 2: Grid Clamp\n",
    "            x_patch_src = min(x_pixel_src // patch_size, w_grid - 1)\n",
    "            y_patch_src = min(y_pixel_src // patch_size, h_grid - 1)\n",
    "\n",
    "            # CALCOLO INDICE (OFFICIAL LOGIC)\n",
    "            # feats_src NON ha il CLS token. Non serve aggiungere +1.\n",
    "            patch_index_src = (y_patch_src * w_grid) + x_patch_src\n",
    "\n",
    "            # Safety check per l'indice\n",
    "            if patch_index_src >= feats_src.shape[1]:\n",
    "                patch_index_src = feats_src.shape[1] - 1\n",
    "\n",
    "            # Estrai feature sorgente\n",
    "            source_vec = feats_src[0, patch_index_src, :]\n",
    "\n",
    "            # Similarità con tutte le patch target\n",
    "            similarity_map = torch.cosine_similarity(source_vec, feats_trg[0], dim=-1)\n",
    "            \n",
    "            # L'argmax ci da l'indice spaziale diretto\n",
    "            patch_idx_spatial = torch.argmax(similarity_map).item()\n",
    "\n",
    "            # Conversione indice -> coordinate griglia\n",
    "            x_col_pred = patch_idx_spatial % w_grid\n",
    "            y_row_pred = patch_idx_spatial // w_grid\n",
    "\n",
    "            # Conversione griglia -> pixel (centro della patch)\n",
    "            x_pred_pixel = x_col_pred * patch_size + (patch_size // 2)\n",
    "            y_pred_pixel = y_row_pred * patch_size + (patch_size // 2)\n",
    "\n",
    "            gt_x = trg_kps_gt[n_keypoint, 0].item()\n",
    "            gt_y = trg_kps_gt[n_keypoint, 1].item()\n",
    "\n",
    "            # CHECK 3: Validità Keypoint Target (Ground Truth)\n",
    "            if math.isnan(gt_x) or math.isnan(gt_y):\n",
    "                continue\n",
    "            if not (0 <= gt_x < W_orig and 0 <= gt_y < H_orig):\n",
    "                continue\n",
    "\n",
    "            # Calcola distanza\n",
    "            distance = math.sqrt((x_pred_pixel - gt_x)**2 + (y_pred_pixel - gt_y)**2)\n",
    "\n",
    "            # Aggiorna contatori per QUESTA immagine\n",
    "            img_tot_keypoints += 1\n",
    "            if distance <= pck_threshold:\n",
    "                img_correct_keypoints += 1\n",
    "        \n",
    "        # AGGIORNAMENTO DATI CATEGORIA (PCK PER IMAGE)\n",
    "        # Se l'immagine aveva almeno un punto valido, calcoliamo la sua accuratezza\n",
    "        if img_tot_keypoints > 0:\n",
    "            image_accuracy = img_correct_keypoints / img_tot_keypoints\n",
    "            \n",
    "            class_pck_data[category]['total_image'] += 1\n",
    "            class_pck_data[category]['image_value_sum'] += image_accuracy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PCK per Class ---\n",
      "Category: aeroplane\n",
      "  PCK@0.05: 22.18% (152.40139104101974/687)\n",
      "--------------------\n",
      "Category: bicycle\n",
      "  PCK@0.05: 9.48% (59.70865800865805/630)\n",
      "--------------------\n",
      "Category: bird\n",
      "  PCK@0.05: 23.14% (162.4627927627926/702)\n",
      "--------------------\n",
      "Category: boat\n",
      "  PCK@0.05: 6.47% (42.07698412698412/650)\n",
      "--------------------\n",
      "Category: bottle\n",
      "  PCK@0.05: 5.03% (42.326190476190504/841)\n",
      "--------------------\n",
      "Category: bus\n",
      "  PCK@0.05: 13.31% (84.79045464339585/637)\n",
      "--------------------\n",
      "Category: car\n",
      "  PCK@0.05: 10.59% (58.75425685425689/555)\n",
      "--------------------\n",
      "Category: cat\n",
      "  PCK@0.05: 36.03% (215.83100788100788/599)\n",
      "--------------------\n",
      "Category: chair\n",
      "  PCK@0.05: 3.33% (20.664971139971133/621)\n",
      "--------------------\n",
      "Category: cow\n",
      "  PCK@0.05: 14.35% (91.3805957802089/637)\n",
      "--------------------\n",
      "Category: dog\n",
      "  PCK@0.05: 17.97% (107.82708680208684/600)\n",
      "--------------------\n",
      "Category: horse\n",
      "  PCK@0.05: 4.57% (26.983311133311123/590)\n",
      "--------------------\n",
      "Category: motorbike\n",
      "  PCK@0.05: 9.27% (63.24087301587306/682)\n",
      "--------------------\n",
      "Category: person\n",
      "  PCK@0.05: 11.35% (73.32206959706963/646)\n",
      "--------------------\n",
      "Category: pottedplant\n",
      "  PCK@0.05: 3.40% (24.89404761904761/732)\n",
      "--------------------\n",
      "Category: sheep\n",
      "  PCK@0.05: 11.77% (77.10222400148879/655)\n",
      "--------------------\n",
      "Category: train\n",
      "  PCK@0.05: 12.95% (97.92735320235316/756)\n",
      "--------------------\n",
      "Category: tvmonitor\n",
      "  PCK@0.05: 5.50% (37.92318376068376/689)\n",
      "--------------------\n",
      "\n",
      "--- Overall Mean PCK ---\n",
      "Overall Mean PCK@0.05: 12.26%\n"
     ]
    }
   ],
   "source": [
    "# CALCOLO PCK PER IMAGE\n",
    "print(\"--- PCK per Class ---\")\n",
    "class_pck_0_05_list = []\n",
    "#class_pck_0_1_list = []\n",
    "#class_pck_0_2_list = []\n",
    "\n",
    "for category, data in class_pck_data.items():\n",
    "    total_image = data['total_image']\n",
    "    correct_image_0_05 = data['image_value_sum']\n",
    "    #correct_image_0_1 = data['image_value_sum']\n",
    "    #correct_image_0_2 = data['image_value_sum']\n",
    "\n",
    "    pck_0_05 = (correct_image_0_05 / total_image) * 100 if total_image > 0 else 0\n",
    "    #pck_0_1 = (correct_image_0_1 / total_image) * 100 if total_image > 0 else 0\n",
    "    #pck_0_2 = (correct_image_0_2 / total_image) * 100 if total_image > 0 else 0\n",
    "\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"  PCK@0.05: {pck_0_05:.2f}% ({correct_image_0_05}/{total_image})\")\n",
    "    #print(f\"  PCK@0.1: {pck_0_1:.2f}% ({correct_image_0_1}/{total_image})\")\n",
    "    #print(f\"  PCK@0.2: {pck_0_2:.2f}% ({correct_image_0_2}/{total_image})\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    if total_image> 0: # Only add to the list if there were keypoints for this class\n",
    "        class_pck_0_05_list.append(pck_0_05)\n",
    "        #class_pck_0_1_list.append(pck_0_1)\n",
    "        #class_pck_0_2_list.append(pck_0_2)\n",
    "\n",
    "# 4. Calculate and Display Overall Mean PCK\n",
    "print(\"\\n--- Overall Mean PCK ---\")\n",
    "overall_mean_pck_0_05 = sum(class_pck_0_05_list) / len(class_pck_0_05_list) if class_pck_0_05_list else 0\n",
    "#overall_mean_pck_0_1 = sum(class_pck_0_1_list) / len(class_pck_0_1_list) if class_pck_0_1_list else 0\n",
    "#overall_mean_pck_0_2 = sum(class_pck_0_2_list) / len(class_pck_0_2_list) if class_pck_0_2_list else 0\n",
    "\n",
    "print(f\"Overall Mean PCK@0.05: {overall_mean_pck_0_05:.2f}%\")\n",
    "#print(f\"Overall Mean PCK@0.1: {overall_mean_pck_0_1:.2f}%\")\n",
    "#print(f\"Overall Mean PCK@0.2: {overall_mean_pck_0_2:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0-oTron10lL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PCK per Class ---\n",
      "Category: aeroplane\n",
      "  PCK@0.1: 37.62% (1963/5218)\n",
      "--------------------\n",
      "Category: bicycle\n",
      "  PCK@0.1: 23.37% (821/3513)\n",
      "--------------------\n",
      "Category: bird\n",
      "  PCK@0.1: 44.01% (1748/3972)\n",
      "--------------------\n",
      "Category: boat\n",
      "  PCK@0.1: 17.54% (529/3016)\n",
      "--------------------\n",
      "Category: bottle\n",
      "  PCK@0.1: 13.05% (706/5409)\n",
      "--------------------\n",
      "Category: bus\n",
      "  PCK@0.1: 32.58% (1361/4178)\n",
      "--------------------\n",
      "Category: car\n",
      "  PCK@0.1: 26.10% (875/3352)\n",
      "--------------------\n",
      "Category: cat\n",
      "  PCK@0.1: 48.91% (2871/5870)\n",
      "--------------------\n",
      "Category: chair\n",
      "  PCK@0.1: 10.14% (301/2968)\n",
      "--------------------\n",
      "Category: cow\n",
      "  PCK@0.1: 31.80% (1592/5007)\n",
      "--------------------\n",
      "Category: dog\n",
      "  PCK@0.1: 35.67% (1611/4517)\n",
      "--------------------\n",
      "Category: horse\n",
      "  PCK@0.1: 13.56% (551/4064)\n",
      "--------------------\n",
      "Category: motorbike\n",
      "  PCK@0.1: 25.69% (799/3110)\n",
      "--------------------\n",
      "Category: person\n",
      "  PCK@0.1: 26.96% (1095/4061)\n",
      "--------------------\n",
      "Category: pottedplant\n",
      "  PCK@0.1: 10.38% (348/3353)\n",
      "--------------------\n",
      "Category: sheep\n",
      "  PCK@0.1: 27.95% (993/3553)\n",
      "--------------------\n",
      "Category: train\n",
      "  PCK@0.1: 29.24% (2467/8437)\n",
      "--------------------\n",
      "Category: tvmonitor\n",
      "  PCK@0.1: 15.68% (1108/7066)\n",
      "--------------------\n",
      "\n",
      "--- Overall Mean PCK ---\n",
      "Overall Mean PCK@0.1: 26.12%\n"
     ]
    }
   ],
   "source": [
    "#PCK per point\n",
    "print(\"--- PCK per Class ---\")\n",
    "#class_pck_0_05_list = []\n",
    "class_pck_0_1_list = []\n",
    "#class_pck_0_2_list = []\n",
    "\n",
    "for category, data in class_pck_data.items():\n",
    "    total_kps = data['total_keypoints']\n",
    "    #correct_kps_0_05 = data['correct_kps_0_05']\n",
    "    correct_kps_0_1 = data['correct_kps_0_1']\n",
    "    #correct_kps_0_2 = data['correct_kps_0_2']\n",
    "\n",
    "    #pck_0_05 = (correct_kps_0_05 / total_kps) * 100 if total_kps > 0 else 0\n",
    "    pck_0_1 = (correct_kps_0_1 / total_kps) * 100 if total_kps > 0 else 0\n",
    "    #pck_0_2 = (correct_kps_0_2 / total_kps) * 100 if total_kps > 0 else 0\n",
    "\n",
    "    print(f\"Category: {category}\")\n",
    "    #print(f\"  PCK@0.05: {pck_0_05:.2f}% ({correct_kps_0_05}/{total_kps})\")\n",
    "    print(f\"  PCK@0.1: {pck_0_1:.2f}% ({correct_kps_0_1}/{total_kps})\")\n",
    "    #print(f\"  PCK@0.2: {pck_0_2:.2f}% ({correct_kps_0_2}/{total_kps})\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    if total_kps > 0: # Only add to the list if there were keypoints for this class\n",
    "        #class_pck_0_05_list.append(pck_0_05)\n",
    "        class_pck_0_1_list.append(pck_0_1)\n",
    "        #class_pck_0_2_list.append(pck_0_2)\n",
    "\n",
    "# 4. Calculate and Display Overall Mean PCK\n",
    "print(\"\\n--- Overall Mean PCK ---\")\n",
    "#overall_mean_pck_0_05 = sum(class_pck_0_05_list) / len(class_pck_0_05_list) if class_pck_0_05_list else 0\n",
    "overall_mean_pck_0_1 = sum(class_pck_0_1_list) / len(class_pck_0_1_list) if class_pck_0_1_list else 0\n",
    "#overall_mean_pck_0_2 = sum(class_pck_0_2_list) / len(class_pck_0_2_list) if class_pck_0_2_list else 0\n",
    "\n",
    "#print(f\"Overall Mean PCK@0.05: {overall_mean_pck_0_05:.2f}%\")\n",
    "print(f\"Overall Mean PCK@0.1: {overall_mean_pck_0_1:.2f}%\")\n",
    "#print(f\"Overall Mean PCK@0.2: {overall_mean_pck_0_2:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM94XQGmSqU+CbzhlwN85hS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
