{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN9/XgFMOcxWwhZUIIZ5bJ6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgEb0J5w9TLY","executionInfo":{"status":"ok","timestamp":1769475917003,"user_tz":-60,"elapsed":5778,"user":{"displayName":"Alexandra Elena Holota","userId":"05449167991295446999"}},"outputId":"6f5e97f1-a4f3-4c52-9a24-41408ece0e28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Scompattamento completato!\n"]}],"source":["import os\n","import torch\n","import torchvision\n","from torchvision import datasets\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import ToTensor, Lambda\n","import matplotlib.pyplot as plt\n","import requests\n","from zipfile import ZipFile\n","from io import BytesIO\n","import numpy as np\n","import zipfile\n","import os\n","from pathlib import Path\n","\n","# Monta Google Drive\n","from google.colab import drive\n","if not os.path.exists('/content/drive'):\n","    drive.mount(\"/content/drive\")\n","\n","# Configurazione Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 1. Crea la cartella di destinazione\n","!mkdir -p /content/spair_local\n","\n","# 2. Estrai il contenuto (cambia il percorso con quello corretto del tuo Drive)\n","!tar -xf \"/content/drive/MyDrive/AMLDataset/SPair-71k.tar\" -C /content/spair_local\n","\n","print(\"Scompattamento completato!\")\n","\n","# Percorsi Globali\n","SPAIR_ROOT = Path(\"/content/spair_local/SPair-71k\")\n","PAIR_ANN_PATH = SPAIR_ROOT / \"PairAnnotation\"\n","IMAGE_PATH    = SPAIR_ROOT / \"JPEGImages\"\n","LAYOUT_PATH   = SPAIR_ROOT / \"Layout\"\n","\n","# Verifica rapida\n","assert SPAIR_ROOT.exists(), \"ERRORE: Dataset locale non trovato in /content/spair_local\""]},{"cell_type":"code","source":["# ----------------------------\n","# Load DINOv3\n","# ----------------------------\n","%cd /content\n","!test -d dinov3 || git clone https://github.com/facebookresearch/dinov3.git\n","%cd /content/dinov3\n","!pip -q install einops timm opencv-python torchmetrics fvcore iopath\n","\n","DINOV3_DIR =Path(\"/content/dinov3\")\n","\n","DINOV3_WEIGHTS = Path(\"/content/drive/MyDrive/AMLDataset/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\")\n","\n","assert os.path.exists(DINOV3_WEIGHTS), f\"Pesi DINOv3 non trovati: {DINOV3_WEIGHTS}\"\n","\n","DEFAULT_MODEL_NAME = \"dinov3_vitb16\"\n","\n","def load_dinov3_backbone(\n","    *,\n","    dinov3_dir: str | Path,\n","    weights_path: str | Path,\n","    device: torch.device | str = \"cpu\",\n","    sanity_input_size: int = 512,\n","    verbose: bool = True,\n",") -> torch.nn.Module:\n","    \"\"\"\n","    Load DINOv3 ViT-B/16 backbone for Task 1 (training-free).\n","\n","    Requirements:\n","      - Official DINOv3 repository cloned locally\n","      - Pretrained checkpoint downloaded separately (licensed)\n","    \"\"\"\n","\n","    # ------------------------------------------------------------------\n","    # 2) Load model from local repo via torch.hub (same behavior as notebook)\n","    # ------------------------------------------------------------------\n","    model = torch.hub.load(\n","        str(dinov3_dir),\n","        DEFAULT_MODEL_NAME,\n","        source=\"local\",\n","        weights=str(weights_path),\n","    )\n","\n","    model.to(device).eval()\n","\n","    # ------------------------------------------------------------------\n","    # 3) Freeze backbone (Task 1 compliant)\n","    # ------------------------------------------------------------------\n","    for p in model.parameters():\n","        p.requires_grad_(False)\n","\n","    # ------------------------------------------------------------------\n","    # 4) Sanity checks: architecture + patch size\n","    # ------------------------------------------------------------------\n","    if not hasattr(model, \"blocks\"):\n","        raise RuntimeError(\"[DINOv3] Loaded model has no attribute 'blocks' (API mismatch?)\")\n","\n","    if not hasattr(model, \"patch_embed\") or not hasattr(model.patch_embed, \"patch_size\"):\n","        raise RuntimeError(\"[DINOv3] patch_embed.patch_size not found (API mismatch?)\")\n","\n","    n_blocks = len(model.blocks)\n","    patch = model.patch_embed.patch_size\n","    patch_int = patch[0] if isinstance(patch, (tuple, list)) else int(patch)\n","\n","    if patch_int != 16:\n","        raise RuntimeError(f\"[DINOv3] Expected patch size 16 for ViT-B/16, got {patch}\")\n","\n","    # ------------------------------------------------------------------\n","    # 5) Token-grid sanity check (important for correspondence)\n","    # ------------------------------------------------------------------\n","    if sanity_input_size is not None:\n","        x = torch.randn(1, 3, sanity_input_size, sanity_input_size, device=device)\n","        with torch.no_grad():\n","            feats = model.get_intermediate_layers(x, n=1)[0]  # [B, N, C]\n","\n","        expected_n = (sanity_input_size // patch_int) ** 2\n","        if feats.shape[1] != expected_n:\n","            raise RuntimeError(\n","                f\"[DINOv3] Unexpected token count: {feats.shape[1]} vs expected {expected_n}. \"\n","                \"Check input size / patch size / token handling.\"\n","            )\n","\n","    if verbose:\n","        print(\n","            f\"[DINOv3] loaded ViT-B/16 | blocks={n_blocks} | patch={patch_int} | \"\n","            f\"checkpoint={weights_path.name}\"\n","        )\n","\n","    return model\n","\n","model = load_dinov3_backbone(\n","    dinov3_dir=DINOV3_DIR,\n","    weights_path=DINOV3_WEIGHTS,\n","    device=device,\n","    sanity_input_size=512,\n","    verbose=True,\n","\n",").eval().to(device)\n","\n","with torch.no_grad():\n","    x = model.forward_features(torch.zeros(1, 3, 512, 512, device=device))\n","    print(\"x_norm_patchtokens:\", x[\"x_norm_patchtokens\"].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kkY54_Z9W9p","executionInfo":{"status":"ok","timestamp":1769475943119,"user_tz":-60,"elapsed":23949,"user":{"displayName":"Alexandra Elena Holota","userId":"05449167991295446999"}},"outputId":"d777a689-a8f0-4d6b-9936-22b510a5f8cd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'dinov3'...\n","remote: Enumerating objects: 538, done.\u001b[K\n","remote: Counting objects: 100% (363/363), done.\u001b[K\n","remote: Compressing objects: 100% (264/264), done.\u001b[K\n","remote: Total 538 (delta 201), reused 99 (delta 99), pack-reused 175 (from 1)\u001b[K\n","Receiving objects: 100% (538/538), 9.88 MiB | 16.64 MiB/s, done.\n","Resolving deltas: 100% (223/223), done.\n","/content/dinov3\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Downloading: \"file:///content/drive/MyDrive/AMLDataset/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\" to /root/.cache/torch/hub/checkpoints/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 327M/327M [00:06<00:00, 52.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[DINOv3] loaded ViT-B/16 | blocks=12 | patch=16 | checkpoint=dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\n","x_norm_patchtokens: torch.Size([1, 1024, 768])\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","import glob\n","import json\n","import numpy as np\n","import torch\n","\n","class Normalize(object):\n","    def __init__(self, image_keys):\n","        self.image_keys = image_keys\n","        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","    def __call__(self, image):\n","        for key in self.image_keys:\n","            image[key] /= 255.0\n","            image[key] = self.normalize(image[key])\n","        return image\n","\n","\n","def read_img(path):\n","    img = np.array(Image.open(path).convert('RGB'))\n","\n","    return torch.tensor(img.transpose(2, 0, 1).astype(np.float32))\n","\n","\n","class SPairDataset(Dataset):\n","    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n","\n","        self.datatype = datatype\n","        self.pck_alpha = pck_alpha\n","        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n","        self.ann_files = self.ann_files[:len(self.ann_files) - 1]\n","        self.pair_ann_path = pair_ann_path\n","        self.image_path = image_path\n","        self.categories = list(map(lambda x: os.path.basename(x), glob.glob('%s/*' % image_path)))\n","        self.categories.sort()\n","        self.transform = Normalize(['src_img', 'trg_img'])\n","\n","    def __len__(self):\n","        return len(self.ann_files)\n","\n","    def __getitem__(self, idx):\n","\n","        raw_line = self.ann_files[idx]\n","        ann_file = raw_line + '.json'\n","        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n","\n","        with open(json_path) as f:\n","            annotation = json.load(f)\n","\n","        category = annotation['category']\n","        src_img = read_img(os.path.join(self.image_path, category, annotation['src_imname']))\n","        trg_img = read_img(os.path.join(self.image_path, category, annotation['trg_imname']))\n","\n","        trg_bbox = annotation['trg_bndbox']\n","        pck_threshold = max(trg_bbox[2] - trg_bbox[0],  trg_bbox[3] - trg_bbox[1]) * self.pck_alpha\n","\n","        sample = {'pair_id': annotation['pair_id'],\n","                  'filename': annotation['filename'],\n","                  'src_imname': annotation['src_imname'],\n","                  'trg_imname': annotation['trg_imname'],\n","                  'src_imsize': src_img.size(),\n","                  'trg_imsize': trg_img.size(),\n","\n","                  'src_bbox': annotation['src_bndbox'],\n","                  'trg_bbox': annotation['trg_bndbox'],\n","                  'category': annotation['category'],\n","\n","                  'src_pose': annotation['src_pose'],\n","                  'trg_pose': annotation['trg_pose'],\n","\n","                  'src_img': src_img,\n","                  'trg_img': trg_img,\n","                  'src_kps': torch.tensor(annotation['src_kps']).float(),\n","                  'trg_kps': torch.tensor(annotation['trg_kps']).float(),\n","\n","                  'mirror': annotation['mirror'],\n","                  'vp_var': annotation['viewpoint_variation'],\n","                  'sc_var': annotation['scale_variation'],\n","                  'truncn': annotation['truncation'],\n","                  'occlsn': annotation['occlusion'],\n","\n","                  'pck_threshold': pck_threshold}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","\n","\n","if __name__ == '__main__':\n","    SPAIR_ROOT = Path(\"/content/spair_local/SPair-71k\")\n","    pair_ann_path = os.path.join(SPAIR_ROOT, 'PairAnnotation')\n","    layout_path = os.path.join(SPAIR_ROOT, 'Layout')\n","    image_path = os.path.join(SPAIR_ROOT, 'JPEGImages')\n","    dataset_size = 'large'\n","    pck_alpha = 0.1\n","\n","    # Verifica che i percorsi esistano prima di creare il dataset\n","    if os.path.exists(pair_ann_path) and os.path.exists(layout_path) and os.path.exists(image_path):\n","        trn_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='trn')\n","        val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='val')\n","        test_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='test')\n","\n","        trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n","        val_dataloader = DataLoader(val_dataset, num_workers=0)\n","        test_dataloader = DataLoader(test_dataset, num_workers=0)\n","        print(\"Dataset caricati correttamente.\")\n","    else:\n","        print(f\"Errore: Impossibile trovare i percorsi del dataset in '{base_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")\n","\n","    # Verifica che i percorsi esistano prima di creare il dataset\n","    if os.path.exists(pair_ann_path) and os.path.exists(layout_path) and os.path.exists(image_path):\n","        trn_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='trn')\n","        val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='val')\n","        test_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='test')\n","\n","        trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n","        val_dataloader = DataLoader(val_dataset, num_workers=0)\n","        test_dataloader = DataLoader(test_dataset, num_workers=0)\n","        print(\"Dataset caricati correttamente.\")\n","    else:\n","        print(f\"Errore: Impossibile trovare i percorsi del dataset in '{base_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_8woeBmI9eUA","executionInfo":{"status":"ok","timestamp":1769475943153,"user_tz":-60,"elapsed":18,"user":{"displayName":"Alexandra Elena Holota","userId":"05449167991295446999"}},"outputId":"a0f49ca1-c2c1-447d-9008-52daeafc7663"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset caricati correttamente.\n","Dataset caricati correttamente.\n"]}]},{"cell_type":"code","source":["import torch\n","import math\n","import numpy as np\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","\n","# --- CONFIGURAZIONE ---\n","output_dir = \"/content/drive/MyDrive/AMLDataset/qualitative_results_grouped\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","\n","TARGET_CLASSES = ['aeroplane', 'chair']\n","POINTS_NEEDED = 3  # Numero di punti da visualizzare insieme sulla stessa immagine\n","\n","# Stato: traccia se abbiamo finito una categoria\n","category_done = {cat: False for cat in TARGET_CLASSES}\n","\n","# Helper functions\n","def denormalize_image(tensor):\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    img = tensor.cpu().squeeze(0).permute(1, 2, 0).numpy()\n","    img = (img * std) + mean\n","    return np.clip(img, 0, 1)\n","\n","def pad_to_multiple(x, k=16):\n","    h, w = x.shape[-2:]\n","    new_h = math.ceil(h / k) * k\n","    new_w = math.ceil(w / k) * k\n","    pad_bottom = new_h - h\n","    pad_right = new_w - w\n","    if pad_bottom == 0 and pad_right == 0: return x\n","    return F.pad(x, (0, pad_right, 0, pad_bottom), value=0)\n","\n","print(f\"Searching for 1 valid image per class with at least {POINTS_NEEDED} keypoints...\")\n","\n","with torch.no_grad():\n","    for i, data in enumerate(tqdm(test_dataloader, desc=\"Scanning\")):\n","\n","        category = data['category'][0]\n","\n","        # Salta se categoria non richiesta o già completata\n","        if category not in TARGET_CLASSES: continue\n","        if category_done[category]: continue\n","\n","        # --- PROCESSIAMO L'IMMAGINE ---\n","        src_img = data['src_img'].to(device)\n","        trg_img = data['trg_img'].to(device)\n","        src_img_padded = pad_to_multiple(src_img, 16)\n","        trg_img_padded = pad_to_multiple(trg_img, 16)\n","\n","        # Forward Pass\n","        dict_src = model.forward_features(src_img_padded)\n","        dict_trg = model.forward_features(trg_img_padded)\n","        feats_src = dict_src[\"x_norm_patchtokens\"]\n","        feats_trg = dict_trg[\"x_norm_patchtokens\"]\n","\n","        # Grid Info\n","        _, _, H_padded, W_padded = src_img_padded.shape\n","        _, _, H_orig, W_orig = data['src_img'].shape\n","        patch_size = 16\n","        w_grid = W_padded // patch_size\n","        h_grid = H_padded // patch_size\n","        kps_list_src = data['src_kps'][0]\n","        trg_kps_gt = data['trg_kps'][0]\n","\n","        # --- LISTE PER ACCUMULARE I 3 PUNTI ---\n","        valid_src_points = []  # (x, y)\n","        valid_pred_points = [] # (x, y)\n","        valid_gt_points = []   # (x, y)\n","\n","        # Scansioniamo tutti i keypoint dell'immagine\n","        for n_keypoint, keypoint_src in enumerate(kps_list_src):\n","\n","            # Se ne abbiamo già trovati 3, smettiamo di calcolarne altri\n","            if len(valid_src_points) >= POINTS_NEEDED:\n","                break\n","\n","            # 1. Check Source Point\n","            x_src = keypoint_src[0].item()\n","            y_src = keypoint_src[1].item()\n","            if math.isnan(x_src) or math.isnan(y_src): continue\n","\n","            x_src, y_src = int(x_src), int(y_src)\n","            if not (0 <= x_src < W_orig and 0 <= y_src < H_orig): continue\n","\n","            # 2. Prediction Logic\n","            x_patch = min(x_src // patch_size, w_grid - 1)\n","            y_patch = min(y_src // patch_size, h_grid - 1)\n","            patch_idx = (y_patch * w_grid) + x_patch\n","            if patch_idx >= feats_src.shape[1]: patch_idx = feats_src.shape[1] - 1\n","\n","            source_vec = feats_src[0, patch_idx, :]\n","            sim_map = torch.cosine_similarity(source_vec, feats_trg[0], dim=-1)\n","            best_idx = torch.argmax(sim_map).item()\n","\n","            x_pred = (best_idx % w_grid) * patch_size + (patch_size // 2)\n","            y_pred = (best_idx // w_grid) * patch_size + (patch_size // 2)\n","\n","            # 3. Check GT Point\n","            x_gt = trg_kps_gt[n_keypoint, 0].item()\n","            y_gt = trg_kps_gt[n_keypoint, 1].item()\n","            if math.isnan(x_gt) or math.isnan(y_gt): continue\n","\n","            # --- PUNTO VALIDO! AGGIUNGIAMO ALLE LISTE ---\n","            valid_src_points.append((x_src, y_src))\n","            valid_pred_points.append((x_pred, y_pred))\n","            valid_gt_points.append((x_gt, y_gt))\n","\n","        # --- SE ABBIAMO TROVATO 3 PUNTI, FACCIAMO IL PLOT UNICO ---\n","        if len(valid_src_points) == POINTS_NEEDED:\n","\n","            img_s_vis = denormalize_image(src_img)\n","            img_t_vis = denormalize_image(trg_img)\n","\n","            fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n","\n","            # Colori per distinguere i punti 1, 2 e 3 (opzionale, utile per vedere le corrispondenze)\n","            colors = ['cyan', 'orange', 'lime']\n","\n","            # PANEL 1: Source Image con 3 punti\n","            ax[0].imshow(img_s_vis)\n","            ax[0].set_title(f\"SOURCE ({category})\\n(Blue Crosses)\")\n","            for idx, (x, y) in enumerate(valid_src_points):\n","                # Disegna croce blu grande\n","                ax[0].scatter(x, y, c='blue', s=150, marker='o', linewidth=3)\n","                # Aggiunge numero piccolo per capire quale punto è quale\n","                ax[0].text(x+5, y+5, str(idx+1), color='white', fontsize=12, fontweight='bold')\n","\n","            # PANEL 2: Prediction Image con 3 punti\n","            ax[1].imshow(img_t_vis)\n","            ax[1].set_title(f\"PREDICTION (DINOv2)\\n(Red X)\")\n","            for idx, (x, y) in enumerate(valid_pred_points):\n","                ax[1].scatter(x, y, c='red', s=150, marker='o', linewidth=3)\n","                ax[1].text(x+5, y+5, str(idx+1), color='white', fontsize=12, fontweight='bold')\n","\n","            # PANEL 3: Ground Truth Image con 3 punti\n","            ax[2].imshow(img_t_vis)\n","            ax[2].set_title(f\"GROUND TRUTH\\n(Green Circles)\")\n","            for idx, (x, y) in enumerate(valid_gt_points):\n","                ax[2].scatter(x, y, c='green', s=150, marker='o', facecolors='none', linewidth=3)\n","                ax[2].text(x+5, y+5, str(idx+1), color='white', fontsize=12, fontweight='bold')\n","\n","            # Cleanup plot\n","            for a in ax: a.axis('off')\n","\n","            # Salvataggio\n","            save_path = os.path.join(output_dir, f\"Comparison_{category}_ID{i}.png\")\n","            plt.tight_layout()\n","            plt.savefig(save_path)\n","            plt.close(fig)\n","\n","            print(f\"--> [SUCCESS] Saved comparison for {category} (Image ID: {i})\")\n","\n","            # Segna categoria come completata\n","            category_done[category] = True\n","\n","        # Check if all classes are done\n","        if all(category_done.values()):\n","            print(\"\\nGenerated all requested images. Exiting.\")\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ge7EIaJi9im6","executionInfo":{"status":"ok","timestamp":1769476020249,"user_tz":-60,"elapsed":72722,"user":{"displayName":"Alexandra Elena Holota","userId":"05449167991295446999"}},"outputId":"fde79b14-c28c-440a-fccd-503f5e387889"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Searching for 1 valid image per class with at least 3 keypoints...\n"]},{"output_type":"stream","name":"stderr","text":["Scanning:   0%|          | 5/12234 [00:02<1:32:16,  2.21it/s]"]},{"output_type":"stream","name":"stdout","text":["--> [SUCCESS] Saved comparison for aeroplane (Image ID: 0)\n"]},{"output_type":"stream","name":"stderr","text":["Scanning:  44%|████▍     | 5422/12234 [01:12<01:31, 74.80it/s]"]},{"output_type":"stream","name":"stdout","text":["--> [SUCCESS] Saved comparison for chair (Image ID: 5422)\n","\n","Generated all requested images. Exiting.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}