{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7041,
     "status": "ok",
     "timestamp": 1769475174475,
     "user": {
      "displayName": "Alexandra Elena Holota",
      "userId": "05449167991295446999"
     },
     "user_tz": -60
    },
    "id": "85LGbic96Esk",
    "outputId": "fb14c713-697c-474a-88e5-6069c23d1338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scompattamento completato!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Monta Google Drive\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "# Configurazione Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Crea la cartella di destinazione\n",
    "!mkdir -p /content/spair_local\n",
    "\n",
    "# 2. Estrai il contenuto (cambia il percorso con quello corretto del tuo Drive)\n",
    "!tar -xf \"/content/drive/MyDrive/AMLDataset/SPair-71k.tar\" -C /content/spair_local\n",
    "\n",
    "print(\"Scompattamento completato!\")\n",
    "\n",
    "# Percorsi Globali\n",
    "SPAIR_ROOT = Path(\"/content/spair_local/SPair-71k\")\n",
    "PAIR_ANN_PATH = SPAIR_ROOT / \"PairAnnotation\"\n",
    "IMAGE_PATH    = SPAIR_ROOT / \"JPEGImages\"\n",
    "LAYOUT_PATH   = SPAIR_ROOT / \"Layout\"\n",
    "\n",
    "# Verifica rapida\n",
    "assert SPAIR_ROOT.exists(), \"ERRORE: Dataset locale non trovato in /content/spair_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5020,
     "status": "ok",
     "timestamp": 1769475179511,
     "user": {
      "displayName": "Alexandra Elena Holota",
      "userId": "05449167991295446999"
     },
     "user_tz": -60
    },
    "id": "3JSz0i5s6u0f",
    "outputId": "a28ea961-f52b-4596-87cc-60b358f205dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "/content/dinov3\n",
      "[DINOv3] loaded ViT-B/16 | blocks=12 | patch=16 | checkpoint=dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\n",
      "x_norm_patchtokens: torch.Size([1, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Load DINOv3\n",
    "# ----------------------------\n",
    "%cd /content\n",
    "!test -d dinov3 || git clone https://github.com/facebookresearch/dinov3.git\n",
    "%cd /content/dinov3\n",
    "!pip -q install einops timm opencv-python torchmetrics fvcore iopath\n",
    "\n",
    "DINOV3_DIR =Path(\"/content/dinov3\")\n",
    "\n",
    "DINOV3_WEIGHTS = Path(\"/content/drive/MyDrive/AMLDataset/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\")\n",
    "\n",
    "assert os.path.exists(DINOV3_WEIGHTS), f\"Pesi DINOv3 non trovati: {DINOV3_WEIGHTS}\"\n",
    "\n",
    "DEFAULT_MODEL_NAME = \"dinov3_vitb16\"\n",
    "\n",
    "def load_dinov3_backbone(\n",
    "    *,\n",
    "    dinov3_dir: str | Path,\n",
    "    weights_path: str | Path,\n",
    "    device: torch.device | str = \"cpu\",\n",
    "    sanity_input_size: int = 512,\n",
    "    verbose: bool = True,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Load DINOv3 ViT-B/16 backbone for Task 1 (training-free).\n",
    "\n",
    "    Requirements:\n",
    "      - Official DINOv3 repository cloned locally\n",
    "      - Pretrained checkpoint downloaded separately (licensed)\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Load model from local repo via torch.hub (same behavior as notebook)\n",
    "    # ------------------------------------------------------------------\n",
    "    model = torch.hub.load(\n",
    "        str(dinov3_dir),\n",
    "        DEFAULT_MODEL_NAME,\n",
    "        source=\"local\",\n",
    "        weights=str(weights_path),\n",
    "    )\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Freeze backbone (Task 1 compliant)\n",
    "    # ------------------------------------------------------------------\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Sanity checks: architecture + patch size\n",
    "    # ------------------------------------------------------------------\n",
    "    if not hasattr(model, \"blocks\"):\n",
    "        raise RuntimeError(\"[DINOv3] Loaded model has no attribute 'blocks' (API mismatch?)\")\n",
    "\n",
    "    if not hasattr(model, \"patch_embed\") or not hasattr(model.patch_embed, \"patch_size\"):\n",
    "        raise RuntimeError(\"[DINOv3] patch_embed.patch_size not found (API mismatch?)\")\n",
    "\n",
    "    n_blocks = len(model.blocks)\n",
    "    patch = model.patch_embed.patch_size\n",
    "    patch_int = patch[0] if isinstance(patch, (tuple, list)) else int(patch)\n",
    "\n",
    "    if patch_int != 16:\n",
    "        raise RuntimeError(f\"[DINOv3] Expected patch size 16 for ViT-B/16, got {patch}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Token-grid sanity check (important for correspondence)\n",
    "    # ------------------------------------------------------------------\n",
    "    if sanity_input_size is not None:\n",
    "        x = torch.randn(1, 3, sanity_input_size, sanity_input_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            feats = model.get_intermediate_layers(x, n=1)[0]  # [B, N, C]\n",
    "\n",
    "        expected_n = (sanity_input_size // patch_int) ** 2\n",
    "        if feats.shape[1] != expected_n:\n",
    "            raise RuntimeError(\n",
    "                f\"[DINOv3] Unexpected token count: {feats.shape[1]} vs expected {expected_n}. \"\n",
    "                \"Check input size / patch size / token handling.\"\n",
    "            )\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"[DINOv3] loaded ViT-B/16 | blocks={n_blocks} | patch={patch_int} | \"\n",
    "            f\"checkpoint={weights_path.name}\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_dinov3_backbone(\n",
    "    dinov3_dir=DINOV3_DIR,\n",
    "    weights_path=DINOV3_WEIGHTS,\n",
    "    device=device,\n",
    "    sanity_input_size=512,\n",
    "    verbose=True,\n",
    "\n",
    ").eval().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model.forward_features(torch.zeros(1, 3, 512, 512, device=device))\n",
    "    print(\"x_norm_patchtokens:\", x[\"x_norm_patchtokens\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1769475228640,
     "user": {
      "displayName": "Alexandra Elena Holota",
      "userId": "05449167991295446999"
     },
     "user_tz": -60
    },
    "id": "NFl7_QkW6Sma",
    "outputId": "83f7fe7b-11f5-4ac1-e050-d6872d266ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricati correttamente.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, image_keys):\n",
    "        self.image_keys = image_keys\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for key in self.image_keys:\n",
    "            image[key] /= 255.0\n",
    "            image[key] = self.normalize(image[key])\n",
    "        return image\n",
    "\n",
    "\n",
    "def read_img(path):\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "\n",
    "    return torch.tensor(img.transpose(2, 0, 1).astype(np.float32))\n",
    "\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n",
    "\n",
    "        self.datatype = datatype\n",
    "        self.pck_alpha = pck_alpha\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = self.ann_files[:len(self.ann_files) - 1]\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        self.categories = list(map(lambda x: os.path.basename(x), glob.glob('%s/*' % image_path)))\n",
    "        self.categories.sort()\n",
    "        self.transform = Normalize(['src_img', 'trg_img'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        raw_line = self.ann_files[idx]\n",
    "        ann_file = raw_line + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_img = read_img(os.path.join(self.image_path, category, annotation['src_imname']))\n",
    "        trg_img = read_img(os.path.join(self.image_path, category, annotation['trg_imname']))\n",
    "\n",
    "        trg_bbox = annotation['trg_bndbox']\n",
    "        pck_threshold = max(trg_bbox[2] - trg_bbox[0],  trg_bbox[3] - trg_bbox[1]) * self.pck_alpha\n",
    "\n",
    "        sample = {'pair_id': annotation['pair_id'],\n",
    "                  'filename': annotation['filename'],\n",
    "                  'src_imname': annotation['src_imname'],\n",
    "                  'trg_imname': annotation['trg_imname'],\n",
    "                  'src_imsize': src_img.size(),\n",
    "                  'trg_imsize': trg_img.size(),\n",
    "\n",
    "                  'src_bbox': annotation['src_bndbox'],\n",
    "                  'trg_bbox': annotation['trg_bndbox'],\n",
    "                  'category': annotation['category'],\n",
    "\n",
    "                  'src_pose': annotation['src_pose'],\n",
    "                  'trg_pose': annotation['trg_pose'],\n",
    "\n",
    "                  'src_img': src_img,\n",
    "                  'trg_img': trg_img,\n",
    "                  'src_kps': torch.tensor(annotation['src_kps']).float(),\n",
    "                  'trg_kps': torch.tensor(annotation['trg_kps']).float(),\n",
    "\n",
    "                  'mirror': annotation['mirror'],\n",
    "                  'vp_var': annotation['viewpoint_variation'],\n",
    "                  'sc_var': annotation['scale_variation'],\n",
    "                  'truncn': annotation['truncation'],\n",
    "                  'occlsn': annotation['occlusion'],\n",
    "\n",
    "                  'pck_threshold': pck_threshold}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    SPAIR_ROOT = Path(\"/content/spair_local/SPair-71k\")\n",
    "    pair_ann_path = os.path.join(SPAIR_ROOT, 'PairAnnotation')\n",
    "    layout_path = os.path.join(SPAIR_ROOT, 'Layout')\n",
    "    image_path = os.path.join(SPAIR_ROOT, 'JPEGImages')\n",
    "    dataset_size = 'large'\n",
    "    pck_alpha = 0.1\n",
    "\n",
    "    # Verifica che i percorsi esistano prima di creare il dataset\n",
    "    if os.path.exists(pair_ann_path) and os.path.exists(layout_path) and os.path.exists(image_path):\n",
    "        trn_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='trn')\n",
    "        val_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='val')\n",
    "        test_dataset = SPairDataset(pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype='test')\n",
    "\n",
    "        trn_dataloader = DataLoader(trn_dataset, num_workers=0)\n",
    "        val_dataloader = DataLoader(val_dataset, num_workers=0)\n",
    "        test_dataloader = DataLoader(test_dataset, num_workers=0)\n",
    "        print(\"Dataset caricati correttamente.\")\n",
    "    else:\n",
    "        print(f\"Errore: Impossibile trovare i percorsi del dataset in '{base_dir}'.\\nVerifica l'estrazione e controlla se la struttura delle cartelle corrisponde.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74561,
     "status": "ok",
     "timestamp": 1769475471425,
     "user": {
      "displayName": "Alexandra Elena Holota",
      "userId": "05449167991295446999"
     },
     "user_tz": -60
    },
    "id": "MrypyGvn7FuM",
    "outputId": "db41aefa-2f56-45a6-d31c-1f6b33639f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PCA-only loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning:   0%|          | 6/12234 [00:02<59:02,  3.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Saved PCA Analysis to: /content/drive/MyDrive/AMLDataset/final_analysis_results/PCA_ANALYSIS_aeroplane.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning:  44%|████▍     | 5422/12234 [01:14<01:33, 72.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Saved PCA Analysis to: /content/drive/MyDrive/AMLDataset/final_analysis_results/PCA_ANALYSIS_chair.png\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/AMLDataset/final_analysis_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "done_classes = {'aeroplane': False, 'chair': False}\n",
    "\n",
    "# 2. FUNCTIONS\n",
    "def pad_to_multiple(x, k=16):\n",
    "    h, w = x.shape[-2:]\n",
    "    new_h = math.ceil(h / k) * k\n",
    "    new_w = math.ceil(w / k) * k\n",
    "    return F.pad(x, (0, new_w - w, 0, new_h - h), value=0)\n",
    "\n",
    "def denormalize(img_tensor):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = img_tensor.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "    return np.clip((img * std) + mean, 0, 1)\n",
    "\n",
    "def compute_joint_pca(feat1, feat2, h1, w1, h2, w2):\n",
    "    f1 = feat1[0].cpu().detach().numpy()\n",
    "    f2 = feat2[0].cpu().detach().numpy()\n",
    "\n",
    "    # Fit PCA on combined features so colors mean the same thing\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(np.concatenate((f1, f2), axis=0))\n",
    "\n",
    "    p1 = pca.transform(f1)\n",
    "    p2 = pca.transform(f2)\n",
    "\n",
    "    p_all = np.concatenate((p1, p2), axis=0)\n",
    "    p_min, p_max = p_all.min(0), p_all.max(0)\n",
    "\n",
    "    # Normalize and reshape\n",
    "    img1 = ((p1 - p_min) / (p_max - p_min)).reshape(h1, w1, 3)\n",
    "    img2 = ((p2 - p_min) / (p_max - p_min)).reshape(h2, w2, 3)\n",
    "    return img1, img2\n",
    "\n",
    "# 3. MAIN LOOP\n",
    "print(\"Starting PCA-only loop...\")\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_dataloader, desc=\"Scanning\")):\n",
    "        category = data['category'][0]\n",
    "        if category not in done_classes or done_classes[category]: continue\n",
    "\n",
    "        # A. PREPARE\n",
    "        src_img, trg_img = data['src_img'].to(device), data['trg_img'].to(device)\n",
    "\n",
    "        src_pad, trg_pad = pad_to_multiple(src_img, 16), pad_to_multiple(trg_img, 16)\n",
    "        h_g_s, w_g_s = src_pad.shape[-2] // 16, src_pad.shape[-1] // 16\n",
    "        h_g_t, w_g_t = trg_pad.shape[-2] // 16, trg_pad.shape[-1] // 16\n",
    "\n",
    "        # n=3 fetches the last 4 blocks: [Layer 10, Layer 11, Layer 12]\n",
    "        out_s = model.get_intermediate_layers(src_pad, n=3, reshape=False, return_class_token=False)\n",
    "        out_t = model.get_intermediate_layers(trg_pad, n=3, reshape=False, return_class_token=False)\n",
    "\n",
    "        f_s_9, f_t_9 = F.normalize(out_s[0], p=2, dim=-1), F.normalize(out_t[0], p=2, dim=-1)\n",
    "        f_s_11, f_t_11 = F.normalize(out_s[2], p=2, dim=-1), F.normalize(out_t[2], p=2, dim=-1)\n",
    "\n",
    "        # C. VISUALIZATION\n",
    "        pca_s_9, pca_t_9 = compute_joint_pca(f_s_9, f_t_9, h_g_s, w_g_s, h_g_t, w_g_t)\n",
    "        _, pca_t_11 = compute_joint_pca(f_s_11, f_t_11, h_g_s, w_g_s, h_g_t, w_g_t)\n",
    "\n",
    "        fig, ax = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        plt.subplots_adjust(hspace=0.2, wspace=0.1)\n",
    "        img_s, img_t = denormalize(src_img), denormalize(trg_img)\n",
    "\n",
    "        # Plotting\n",
    "        # Row 1: Original Images\n",
    "        ax[0, 0].imshow(img_s)\n",
    "        ax[0, 0].set_title(f\"SOURCE ({category})\\nOriginal Image\", fontsize=16)\n",
    "\n",
    "        ax[0, 1].imshow(img_t)\n",
    "        ax[0, 1].set_title(\"TARGET\\nOriginal Image\", fontsize=16)\n",
    "\n",
    "        ax[0, 2].axis('off') # Empty slot\n",
    "\n",
    "        # Row 2: PCA Visualization (Using bicubic for paper-like look)\n",
    "        ax[1, 0].imshow(pca_s_9, interpolation='bicubic')\n",
    "        ax[1, 0].set_title(\"Source PCA (Layer 10)\\nGeometric Features\", fontsize=16)\n",
    "\n",
    "        ax[1, 1].imshow(pca_t_9, interpolation='bicubic')\n",
    "        ax[1, 1].set_title(\"Target PCA (Layer 10)\\nShould Match Source Colors\", fontsize=16)\n",
    "\n",
    "        ax[1, 2].imshow(pca_t_11, interpolation='bicubic')\n",
    "        ax[1, 2].set_title(\"Target PCA (Last Layer)\\nSemantic Collapse\", fontsize=16)\n",
    "\n",
    "        for a in ax.flatten(): a.axis('off')\n",
    "\n",
    "        save_path = os.path.join(output_dir, f\"PCA_ANALYSIS_{category}.png\")\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"--> Saved PCA Analysis to: {save_path}\")\n",
    "\n",
    "        done_classes[category] = True\n",
    "        if done_classes['aeroplane'] and done_classes['chair']: break\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO64ga5xWzBRK8f4UYuwYrB",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
