{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-15T19:25:24.904546050Z",
     "start_time": "2026-01-15T19:25:24.889902203Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data.spair import SPairDataset\n",
    "import os\n",
    "\n",
    "from data.transform import SDTransform\n",
    "\n",
    "\n",
    "print(\"Hello\")\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "dataset_size = 'small'\n",
    "\n",
    "# Load dataset and construct dataloader\n",
    "\n",
    "test_dataset = SPairDataset(dataset_size=dataset_size, datatype='test',\n",
    "                                transform=SDTransform(['src_img', 'trg_img']))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=0, batch_size=1, collate_fn=collate_single)\n",
    "print(\"Dataset loeaded\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "cwd: /home/pasquale/PycharmProjects/AML-polito\n",
      "file: /home/pasquale/PycharmProjects/AML-polito/data/dataset.py\n",
      "Dataset loeaded\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T19:27:06.501240704Z",
     "start_time": "2026-01-15T19:27:06.464001151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install xformers\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n"
   ],
   "id": "8feaedcf3a0a2ad9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T19:25:28.379258611Z",
     "start_time": "2026-01-15T19:25:25.100060302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers import DDIMScheduler\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "class MyUNet2DConditionModel(UNet2DConditionModel):\n",
    "    def forward(\n",
    "            self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            encoder_hidden_states: torch.Tensor,\n",
    "            up_ft_indices,\n",
    "            class_labels: Optional[torch.Tensor] = None,\n",
    "            timestep_cond: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n",
    "            down_block_additional_residuals: Optional[Tuple[torch.Tensor]] = None,\n",
    "            mid_block_additional_residual: Optional[torch.Tensor] = None,\n",
    "            down_intrablock_additional_residuals: Optional[Tuple[torch.Tensor]] = None,\n",
    "            encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "            return_dict: bool = True\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n",
    "            timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps\n",
    "            encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states\n",
    "            cross_attention_kwargs (`dict`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
    "        \"\"\"\n",
    "        # By default samples have to be AT least a multiple of the overall upsampling factor.\n",
    "        # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n",
    "        # However, the upsampling interpolation output size can be forced to fit any upsampling size\n",
    "        # on the fly if necessary.\n",
    "        default_overall_up_factor = 2 ** self.num_upsamplers\n",
    "\n",
    "        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n",
    "        forward_upsample_size = False\n",
    "        upsample_size = None\n",
    "\n",
    "        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
    "            # logger.info(\"Forward upsample size to force interpolation output size.\")\n",
    "            forward_upsample_size = True\n",
    "\n",
    "        # prepare attention_mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb, timestep_cond)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when num_class_embeds > 0\")\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "\n",
    "        # 2. pre-process\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(\n",
    "                sample,\n",
    "                emb,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                cross_attention_kwargs=cross_attention_kwargs,\n",
    "            )\n",
    "\n",
    "        # 5. up\n",
    "        up_ft = {}\n",
    "        for i, upsample_block in enumerate(self.up_blocks):\n",
    "\n",
    "            if i > np.max(up_ft_indices):\n",
    "                break\n",
    "\n",
    "            is_final_block = i == len(self.up_blocks) - 1\n",
    "\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets):]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            # if we have not reached the final block and need to forward the\n",
    "            # upsample size, we do it here\n",
    "            if not is_final_block and forward_upsample_size:\n",
    "                upsample_size = down_block_res_samples[-1].shape[2:]\n",
    "\n",
    "            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    res_hidden_states_tuple=res_samples,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    upsample_size=upsample_size,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n",
    "                )\n",
    "\n",
    "            if i in up_ft_indices:\n",
    "                up_ft[i] = sample.detach()\n",
    "\n",
    "        output = {}\n",
    "        output['up_ft'] = up_ft\n",
    "        return output\n",
    "\n",
    "\n",
    "class OneStepSDPipeline(StableDiffusionPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "            self,\n",
    "            img_tensor,\n",
    "            t,\n",
    "            up_ft_indices,\n",
    "            prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ):\n",
    "        device = self._execution_device\n",
    "        latents = self.vae.encode(img_tensor).latent_dist.sample() * self.vae.config.scaling_factor\n",
    "        t = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        noise = torch.randn_like(latents).to(device)\n",
    "        latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "        print(\"forwarding unet...\")\n",
    "        unet_output = self.unet(\n",
    "            latents_noisy,\n",
    "            t,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            up_ft_indices=up_ft_indices,\n",
    "        )\n",
    "        return unet_output"
   ],
   "id": "52aaab5945727f04",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T19:25:28.477732134Z",
     "start_time": "2026-01-15T19:25:28.427442933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_prompt_embeds(pipe, prompt: str, device: str = \"cuda\"):\n",
    "    # Tokenizza anche la stringa vuota e produce embeddings validi [1, 77, dim]\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        [prompt],\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = text_inputs.input_ids.to(device)\n",
    "    attention_mask = getattr(text_inputs, \"attention_mask\", None)\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = pipe.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prompt_embeds = out[0]\n",
    "    return prompt_embeds"
   ],
   "id": "851a28ef086c3b25",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T19:25:28.532996485Z",
     "start_time": "2026-01-15T19:25:28.479896029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class SDFeaturizer4Eval():\n",
    "    def __init__(self, sd_id='Manojb/stable-diffusion-2-1-base', null_prompt='', cat_list=[]):\n",
    "        print(\"called init SDFeaturizer4Eval\")\n",
    "        unet = MyUNet2DConditionModel.from_pretrained(sd_id, subfolder=\"unet\")\n",
    "        print(\"loaded unet\")\n",
    "        onestep_pipe = OneStepSDPipeline.from_pretrained(sd_id, unet=unet, safety_checker=None)\n",
    "        print(\"loaded onestep pipe\")\n",
    "        onestep_pipe.vae.decoder = None\n",
    "        onestep_pipe.scheduler = DDIMScheduler.from_pretrained(sd_id, subfolder=\"scheduler\")\n",
    "        print(\"loaded scheduler\")\n",
    "        onestep_pipe = onestep_pipe.to(\"cuda\")\n",
    "        onestep_pipe.enable_attention_slicing()\n",
    "        onestep_pipe.enable_xformers_memory_efficient_attention()\n",
    "        print(\"moved onestep pipe to cuda\")\n",
    "        null_prompt_embeds = encode_prompt_embeds(onestep_pipe, null_prompt, device=\"cuda\")  # [1, 77, dim]\n",
    "        print(\"loaded null prompt embeds\")\n",
    "\n",
    "        self.null_prompt_embeds = null_prompt_embeds\n",
    "        self.null_prompt = null_prompt\n",
    "        self.pipe = onestep_pipe\n",
    "        with torch.no_grad():\n",
    "            cat2prompt_embeds = {}\n",
    "            print(\"start encoding prompts for categories...\")\n",
    "            for cat in cat_list:\n",
    "                prompt = f\"a photo of a {cat}\"\n",
    "                prompt_embeds = encode_prompt_embeds(self.pipe, prompt, device=\"cuda\")  # [1, 77, dim]\n",
    "                cat2prompt_embeds[cat] = prompt_embeds\n",
    "            print(\"encoded prompts for categories:\", list(cat2prompt_embeds.keys()))\n",
    "            self.cat2prompt_embeds = cat2prompt_embeds\n",
    "\n",
    "        self.pipe.tokenizer = None\n",
    "        self.pipe.text_encoder = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self,\n",
    "                img,\n",
    "                category=None,\n",
    "                t=261,\n",
    "                up_ft_index=1,\n",
    "                ensemble_size=8):\n",
    "        '''\n",
    "        Args:\n",
    "            img_tensor: should be a single torch tensor in the shape of [1, C, H, W] or [C, H, W]\n",
    "            prompt: the prompt to use, a string\n",
    "            t: the time step to use, should be an int in the range of [0, 1000]\n",
    "            up_ft_index: which upsampling block of the U-Net to extract feature, you can choose [0, 1, 2, 3]\n",
    "            ensemble_size: the number of repeated images used in the batch to extract features\n",
    "        Return:\n",
    "            unet_ft: a torch tensor in the shape of [1, c, h, w]\n",
    "        '''\n",
    "\n",
    "        if img is not None:\n",
    "            img = img.cuda()  # ensem, c, h, w\n",
    "        if category in self.cat2prompt_embeds:\n",
    "            prompt_embeds = self.cat2prompt_embeds[category]\n",
    "        else:\n",
    "            prompt_embeds = self.null_prompt_embeds\n",
    "        prompt_embeds = prompt_embeds.repeat(ensemble_size, 1, 1).cuda()\n",
    "        unet_ft_all = self.pipe(\n",
    "            img_tensor=img,\n",
    "            t=t,\n",
    "            up_ft_indices=[up_ft_index],\n",
    "            prompt_embeds=prompt_embeds)\n",
    "        unet_ft = unet_ft_all['up_ft'][up_ft_index]  # ensem, c, h, w\n",
    "        unet_ft = unet_ft.mean(0, keepdim=True)  # 1,c,h,w\n",
    "        return unet_ft"
   ],
   "id": "ddfe1089f144b36f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T19:25:37.016670831Z",
     "start_time": "2026-01-15T19:25:28.535701022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = next(iter(test_dataloader))\n",
    "\n",
    "img = sample[\"src_img\"]          # atteso: [E, 3, 768, 768] in [-1,1]\n",
    "category = sample.get(\"category\", None)\n",
    "dift = SDFeaturizer4Eval(cat_list=['aeroplane'])\n",
    "# Test estrazione feature\n",
    "ft = dift.forward(img=img, category=category)\n",
    "\n",
    "print(\"Feature map shape:\", tuple(ft.shape))  # atteso: [1, C, Hf, Wf]\n",
    "print(\"dtype:\", ft.dtype, \"device:\", ft.device)\n",
    "print(\"min/max:\", float(ft.min()), float(ft.max()))"
   ],
   "id": "ec539453bdfb71d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called init SDFeaturizer4Eval\n",
      "loaded unet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a16f74a69dfa441f8de0c6e92eabbbb4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded onestep pipe\n",
      "loaded scheduler\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 89.88 MiB is free. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.41 GiB is allocated by PyTorch, and 78.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m img = sample[\u001B[33m\"\u001B[39m\u001B[33msrc_img\u001B[39m\u001B[33m\"\u001B[39m]          \u001B[38;5;66;03m# atteso: [E, 3, 768, 768] in [-1,1]\u001B[39;00m\n\u001B[32m      4\u001B[39m category = sample.get(\u001B[33m\"\u001B[39m\u001B[33mcategory\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m dift = \u001B[43mSDFeaturizer4Eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcat_list\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43maeroplane\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Test estrazione feature\u001B[39;00m\n\u001B[32m      7\u001B[39m ft = dift.forward(img=img, category=category)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36mSDFeaturizer4Eval.__init__\u001B[39m\u001B[34m(self, sd_id, null_prompt, cat_list)\u001B[39m\n\u001B[32m     11\u001B[39m onestep_pipe.scheduler = DDIMScheduler.from_pretrained(sd_id, subfolder=\u001B[33m\"\u001B[39m\u001B[33mscheduler\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mloaded scheduler\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m onestep_pipe = \u001B[43monestep_pipe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcuda\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m onestep_pipe.enable_attention_slicing()\n\u001B[32m     15\u001B[39m onestep_pipe.enable_xformers_memory_efficient_attention()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py:545\u001B[39m, in \u001B[36mDiffusionPipeline.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    543\u001B[39m     module.to(device=device)\n\u001B[32m    544\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_loaded_in_4bit_bnb \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_loaded_in_8bit_bnb \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_group_offloaded:\n\u001B[32m--> \u001B[39m\u001B[32m545\u001B[39m     \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    548\u001B[39m     module.dtype == torch.float16\n\u001B[32m    549\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(device) \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    550\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m silence_dtype_warnings\n\u001B[32m    551\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_offloaded\n\u001B[32m    552\u001B[39m ):\n\u001B[32m    553\u001B[39m     logger.warning(\n\u001B[32m    554\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    555\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    558\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    559\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/diffusers/models/modeling_utils.py:1435\u001B[39m, in \u001B[36mModelMixin.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1430\u001B[39m     logger.warning(\n\u001B[32m   1431\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThe module \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m is group offloaded and moving it using `.to()` is not supported.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1432\u001B[39m     )\n\u001B[32m   1433\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1435\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1371\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1368\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1369\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1371\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[31m[... skipping similar frames: Module._apply at line 930 (2 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    933\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    934\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    935\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    940\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    941\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:957\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    953\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    954\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    955\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    956\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m957\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    958\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_subclasses\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfake_tensor\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FakeTensor\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/nn/modules/module.py:1357\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1350\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1351\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1352\u001B[39m             device,\n\u001B[32m   1353\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1354\u001B[39m             non_blocking,\n\u001B[32m   1355\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1356\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1357\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1358\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1359\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1360\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1361\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1362\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1363\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 89.88 MiB is free. Including non-PyTorch memory, this process has 3.54 GiB memory in use. Of the allocated memory 3.41 GiB is allocated by PyTorch, and 78.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
