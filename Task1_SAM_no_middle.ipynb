{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:41:41.887687978Z",
     "start_time": "2026-01-16T23:41:41.837386716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from data.spair import SPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "\n",
    "dataset_size = 'large'  # 'small' or 'large'\n",
    "\n",
    "# Load dataset and construct dataloader\n",
    "\n",
    "test_dataset = SPairDataset(datatype='test', dataset_size=dataset_size)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=2, batch_size=1, collate_fn=collate_single)\n",
    "print(\"Dataset loaded\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T22:58:42.211309443Z",
     "start_time": "2026-01-16T22:58:40.774134184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# SAM initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "\n",
    "print(f\"SAM '{selected_model}' loaded. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM 'vit_b' loaded. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:24:14.833590718Z",
     "start_time": "2026-01-16T22:58:46.882005557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from utils.utils_correspondence import hard_argmax\n",
    "from utils.utils_results import *\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "save_root = Path(base_dir) / \"data\" / \"features\"\n",
    "save_root.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Saving features to:\", save_root)\n",
    "\n",
    "current_cat = None\n",
    "output_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_tensor, img_size, img_category, img_name in tqdm(\n",
    "            test_dataset.iter_images(),\n",
    "            total=test_dataset.num_images(),\n",
    "            desc=\"Generating embeddings\"\n",
    "    ):\n",
    "        # Se cambia categoria, salva quella precedente\n",
    "        if current_cat is None:\n",
    "            current_cat = img_category\n",
    "\n",
    "        if img_category != current_cat:\n",
    "            torch.save(output_dict, save_root / f\"{current_cat}.pth\")\n",
    "            output_dict = {}\n",
    "            current_cat = img_category\n",
    "\n",
    "        # --- la tua parte (corretta) ---\n",
    "        img_tensor = img_tensor.to(device).unsqueeze(0)  # [1,3,H,W]\n",
    "        orig_size = tuple(img_size[1:])  # (H,W)\n",
    "        resized = predictor.transform.apply_image_torch(img_tensor)  # [1,3,H',W']\n",
    "        predictor.set_torch_image(resized, orig_size)\n",
    "        img_emb = predictor.get_image_embedding()[0]  # [C,h',w']\n",
    "        # -------------------------------\n",
    "\n",
    "        # Salva su CPU (consigliato per non tenere CUDA tensors nel dict)\n",
    "        output_dict[img_name] = img_emb.detach().cpu()\n",
    "        # opzionale per risparmiare spazio:\n",
    "        # output_dict[img_name] = img_emb.detach().cpu().to(torch.float16)\n",
    "\n",
    "# Salva l’ultima categoria\n",
    "if current_cat is not None and len(output_dict) > 0:\n",
    "    torch.save(output_dict, save_root / f\"{current_cat}.pth\")\n",
    "\n",
    "\n"
   ],
   "id": "c888ec88287af227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features to: /home/pasquale/PycharmProjects/AML-polito/data/features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 1800/1800 [25:27<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:33:08.487437303Z",
     "start_time": "2026-01-16T23:33:08.431895364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale → lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "16781315f17b51d3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:41:47.168576493Z",
     "start_time": "2026-01-16T23:41:47.156847426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "cat_cache = OrderedDict()\n",
    "MAX_CATS_IN_RAM = 2  # o 4\n",
    "\n",
    "def get_cat_dict(category):\n",
    "    if category in cat_cache:\n",
    "        cat_cache.move_to_end(category)\n",
    "        return cat_cache[category]\n",
    "\n",
    "    d = torch.load(save_root / f\"{category}.pth\", map_location=\"cpu\")\n",
    "    cat_cache[category] = d\n",
    "    cat_cache.move_to_end(category)\n",
    "\n",
    "    while len(cat_cache) > MAX_CATS_IN_RAM:\n",
    "        cat_cache.popitem(last=False)  # elimina la meno recente\n",
    "    return d"
   ],
   "id": "ad2a1683fd649632",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:49:10.560022709Z",
     "start_time": "2026-01-16T23:42:11.816972261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, total=len(test_dataloader), desc=f\"Elaborazione con SAM {selected_model}\"):\n",
    "\n",
    "        category = batch[\"category\"]\n",
    "        orig_size_src = tuple(batch[\"src_imsize\"][1:])  # (Hs, Ws)\n",
    "        orig_size_trg = tuple(batch[\"trg_imsize\"][1:])  # (Ht, Wt)\n",
    "\n",
    "        cat_dict = get_cat_dict(category)\n",
    "\n",
    "        src_emb = cat_dict[batch['src_imname']]         # [C,hs,ws]\n",
    "        trg_emb = cat_dict[batch['trg_imname']]         # [C,ht,wt]\n",
    "\n",
    "        # Keypoints & metadata\n",
    "        src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "        pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "        pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "        pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "        # Target valid region\n",
    "        Ht, Wt = orig_size_trg\n",
    "        H_prime, W_prime = predictor.transform.get_preprocess_shape(\n",
    "            Ht, Wt, predictor.transform.target_length\n",
    "        )\n",
    "\n",
    "        hv_t = (H_prime + PATCH - 1) // PATCH\n",
    "        wv_t = (W_prime + PATCH - 1) // PATCH\n",
    "\n",
    "        N_kps = src_kps.shape[0]\n",
    "\n",
    "        C_ft = trg_emb.shape[0]\n",
    "\n",
    "        trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "        trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "        distances_this_image = []\n",
    "\n",
    "        for i in range(N_kps):\n",
    "            src_keypoint = src_kps[i].unsqueeze(0)  # [1,2] (x,y)\n",
    "            trg_keypoint = trg_kps[i]  # [2]   (x,y)\n",
    "\n",
    "            if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                continue\n",
    "\n",
    "            # originale src -> feature src\n",
    "            x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "            # feature vector sorgente\n",
    "            src_vec = src_emb[:, y_idx, x_idx]  # [256]\n",
    "\n",
    "            # cosine similarity con tutte le posizioni valide del target\n",
    "            sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "\n",
    "            sim2d = sim.view(hv_t, wv_t)\n",
    "            x_idx_t, y_idx_t = hard_argmax(sim2d)\n",
    "\n",
    "            # feature target -> pixel originali target\n",
    "            x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "            dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "            distances_this_image.append(dist)\n",
    "\n",
    "        results.append(\n",
    "            CorrespondenceResult(\n",
    "                category=category,\n",
    "                distances=distances_this_image,\n",
    "                pck_threshold_0_05=pck_thr_0_05,\n",
    "                pck_threshold_0_1=pck_thr_0_1,\n",
    "                pck_threshold_0_2=pck_thr_0_2\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # cleanup\n",
    "        del src_emb, trg_emb, trg_valid, trg_flat, src_vec, sim\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # cleanup per-batch\n",
    "        predictor.reset_image()\n",
    "        torch.cuda.empty_cache()"
   ],
   "id": "1bba535963e7f5a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b: 100%|██████████| 12234/12234 [06:58<00:00, 29.22it/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T23:49:14.357898640Z",
     "start_time": "2026-01-16T23:49:13.791066598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute and print results\n",
    "correct = compute_correct_per_category(results)\n",
    "compute_pckt_keypoints(correct)\n",
    "compute_pckt_images(correct)"
   ],
   "id": "e3a7661228d5944c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCK Results per keypoints (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  17.938931  26.045075  39.912759\n",
      "1       bicycle  10.191412  16.063114  27.340921\n",
      "2          bird  20.377185  30.910764  45.216191\n",
      "3          boat  10.336680  18.133491  32.959244\n",
      "4        bottle  17.284335  26.720648  41.902834\n",
      "5           bus  13.421170  18.133095  28.115230\n",
      "6           car  15.033408  20.545657  30.846325\n",
      "7           cat  28.170378  39.415941  54.969345\n",
      "8         chair   9.364732  13.910186  22.426068\n",
      "9           cow  19.186264  27.323628  40.854797\n",
      "10          dog  12.911287  20.741358  35.485214\n",
      "11        horse   9.885204  16.347789  27.848639\n",
      "12    motorbike   8.877131  16.460905  26.807760\n",
      "13       person  18.179702  30.726257  45.297952\n",
      "14  pottedplant  12.887511  23.095660  33.392383\n",
      "15        sheep   9.000000  15.631579  26.842105\n",
      "16        train  19.630485  30.542725  49.907621\n",
      "17    tvmonitor  14.320719  24.612034  40.525456\n",
      "18          All  14.833141  23.075550  36.147269\n",
      "PCK per-image (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  15.815338  23.647742  37.358754\n",
      "1       bicycle   9.014452  14.314169  24.890543\n",
      "2          bird  19.420726  29.703419  43.593368\n",
      "3          boat   8.274827  14.591134  27.930516\n",
      "4        bottle  16.147601  25.468528  40.541097\n",
      "5           bus  10.523696  14.388576  23.140779\n",
      "6           car  10.175401  14.568447  24.710800\n",
      "7           cat  28.306876  39.484397  55.154671\n",
      "8         chair   8.030242  12.381086  20.204896\n",
      "9           cow  15.906482  23.822078  35.984364\n",
      "10          dog  11.913576  19.309898  33.607117\n",
      "11        horse   8.877345  14.874461  26.294372\n",
      "12    motorbike   8.371490  16.071994  26.509858\n",
      "13       person  15.262093  26.840728  41.991079\n",
      "14  pottedplant  10.650665  19.566991  30.383613\n",
      "15        sheep   7.132161  12.501456  22.954477\n",
      "16        train  17.655086  28.064349  47.076000\n",
      "17    tvmonitor  13.881544  23.782018  39.487985\n",
      "18          All  13.075533  20.743415  33.434127\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
