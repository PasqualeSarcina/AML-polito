{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T00:30:42.923074756Z",
     "start_time": "2026-01-18T00:30:40.663020205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from data.spair import SPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "dataset_size = 'large'  # 'small' or 'large'\n",
    "\n",
    "# Load dataset and construct dataloader\n",
    "\n",
    "test_dataset = SPairDataset(datatype='test', dataset_size=dataset_size)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1, collate_fn=collate_single)\n",
    "print(\"Dataset loaded\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T00:30:44.023433242Z",
     "start_time": "2026-01-18T00:30:42.939710323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# SAM initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "\n",
    "print(f\"SAM '{selected_model}' loaded. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM 'vit_b' loaded. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T00:48:09.406262772Z",
     "start_time": "2026-01-18T00:41:25.099527063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_featuremaps import save_featuremap\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_dir = Path(base_dir) / \"data\" / \"features\" / \"SAM\"\n",
    "print(\"Saving features to:\", save_dir)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    for img_name, img_tensor, img_size in tqdm(\n",
    "            test_dataset.iter_dist_images(),\n",
    "            total=test_dataset.len_dist_images(),\n",
    "            desc=\"Generating embeddings\"\n",
    "    ):\n",
    "        img_tensor = img_tensor.to(device).unsqueeze(0)  # [1,3,H,W]\n",
    "        orig_size = tuple(img_size[1:])  # (H,W)\n",
    "        resized = predictor.transform.apply_image_torch(img_tensor)  # [1,3,H',W']\n",
    "        predictor.set_torch_image(resized, orig_size)\n",
    "        img_emb = predictor.get_image_embedding()[0]  # [C,h',w']\n",
    "\n",
    "        save_featuremap(img_emb, img_name, save_dir)\n",
    "\n"
   ],
   "id": "dcb80b0af61ca562",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features to: /home/pasquale/PycharmProjects/AML-polito/data/features/SAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 481/481 [06:44<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T00:48:18.488460235Z",
     "start_time": "2026-01-18T00:48:18.438189450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def kps_src_to_featmap(kps_src: torch.Tensor, img_src_size: torch.Size):\n",
    "    img_h = int(img_src_size[-2])\n",
    "    img_w = int(img_src_size[-1])\n",
    "\n",
    "    # (N,2) coords nella resized (no padding)\n",
    "    coords = predictor.transform.apply_coords_torch(kps_src, (img_h, img_w))  # (N,2)\n",
    "\n",
    "    img_resized_h, img_resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    xf = torch.floor(coords[:, 0] / PATCH).long()\n",
    "    yf = torch.floor(coords[:, 1] / PATCH).long()\n",
    "\n",
    "    wv = math.ceil(img_resized_w / PATCH)\n",
    "    hv = math.ceil(img_resized_h / PATCH)\n",
    "\n",
    "    xf = xf.clamp(0, wv - 1)\n",
    "    yf = yf.clamp(0, hv - 1)\n",
    "\n",
    "    return torch.stack([xf, yf], dim=1)  # (N,2) (x_idx,y_idx)\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(y_featmap, x_featmap, trg_img_size: torch.Size):\n",
    "    img_h = int(trg_img_size[-2])\n",
    "    img_w = int(trg_img_size[-1])\n",
    "\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # token center in preprocessed (padded) image coords\n",
    "    yr = (y_featmap + 0.5) * 16\n",
    "    xr = (x_featmap + 0.5) * 16\n",
    "\n",
    "    # discard tokens that fall into padding\n",
    "    if xr < 0 or yr < 0 or xr >= resized_w or yr >= resized_h:\n",
    "        return None\n",
    "\n",
    "    scale = resized_w / img_w  # uniforme\n",
    "    y_trg = yr / scale\n",
    "    x_trg = xr / scale\n",
    "\n",
    "    return y_trg, x_trg"
   ],
   "id": "16781315f17b51d3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T00:50:46.781852831Z",
     "start_time": "2026-01-18T00:48:21.908278689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_featuremaps import load_featuremap\n",
    "from utils.utils_correspondence import hard_argmax\n",
    "from utils.utils_results import CorrespondenceResult\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(\n",
    "            test_dataloader,\n",
    "            total=len(test_dataloader),\n",
    "            desc=f\"Elaborazione con SAM {selected_model}\"\n",
    "    ):\n",
    "        category = batch[\"category\"]\n",
    "\n",
    "        orig_size_src = batch[\"src_imsize\"]  # torch.Size([C, Hs, Ws]) o simile\n",
    "        orig_size_trg = batch[\"trg_imsize\"]  # torch.Size([C, Ht, Wt]) o simile\n",
    "\n",
    "        src_imname = batch[\"src_imname\"]\n",
    "        trg_imname = batch[\"trg_imname\"]\n",
    "\n",
    "        # Embeddings SAM (C, h, w) tipicamente (256, 64, 64) o simili\n",
    "        src_emb = load_featuremap(src_imname, save_dir, device)  # [C,hs,ws]\n",
    "        trg_emb = load_featuremap(trg_imname, save_dir, device)  # [C,ht,wt]\n",
    "\n",
    "        # Keypoints (N,2) in pixel originali, ordine (x,y)\n",
    "        src_kps = batch[\"src_kps\"].to(device)\n",
    "        trg_kps = batch[\"trg_kps\"].to(device)\n",
    "\n",
    "        # -------------------------\n",
    "        # Target: dimensioni originali + dimensioni resize (no padding)\n",
    "        # -------------------------\n",
    "        Ht = int(orig_size_trg[-2])\n",
    "        Wt = int(orig_size_trg[-1])\n",
    "\n",
    "        H_prime, W_prime = predictor.transform.get_preprocess_shape(\n",
    "            Ht, Wt, predictor.transform.target_length\n",
    "        )\n",
    "\n",
    "        # Regione valida in token (no padding)\n",
    "        hv_t = (H_prime + PATCH - 1) // PATCH\n",
    "        wv_t = (W_prime + PATCH - 1) // PATCH\n",
    "\n",
    "        # -------------------------\n",
    "        # Prepara target flat sulla regione valida\n",
    "        # -------------------------\n",
    "        C_ft = trg_emb.shape[0]\n",
    "        trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C, hv, wv]\n",
    "        trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid, C]\n",
    "\n",
    "        # -------------------------\n",
    "        # Mappa i keypoint SRC -> indici featuremap SRC (token space)\n",
    "        # (Assumo che kps_src_to_featmap ritorni (N,2) (x_idx, y_idx) long)\n",
    "        # -------------------------\n",
    "        src_kps_idx = kps_src_to_featmap(src_kps, orig_size_src)  # (N,2) (x_idx,y_idx)\n",
    "\n",
    "        N_kps = src_kps_idx.shape[0]\n",
    "        distances_this_image = []\n",
    "\n",
    "        # scala SAM (uniforme sul lato lungo)\n",
    "        if Wt >= Ht:\n",
    "            scale = W_prime / Wt\n",
    "        else:\n",
    "            scale = H_prime / Ht\n",
    "\n",
    "        # -------------------------\n",
    "        # Loop keypoints\n",
    "        # -------------------------\n",
    "        for i in range(N_kps):\n",
    "            src_idx = src_kps_idx[i]  # (x_idx, y_idx) su featuremap\n",
    "            trg_kp = trg_kps[i]  # (x,y) originale\n",
    "\n",
    "            if torch.isnan(src_idx).any() or torch.isnan(trg_kp).any():\n",
    "                continue\n",
    "\n",
    "            x_idx = int(src_idx[0].item())\n",
    "            y_idx = int(src_idx[1].item())\n",
    "\n",
    "            # Feature vector sorgente (C,)\n",
    "            src_vec = src_emb[:, y_idx, x_idx]  # [C]\n",
    "\n",
    "            # Cosine similarity su tutte le posizioni target valide\n",
    "            sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [Pvalid]\n",
    "\n",
    "            # (hv_t, wv_t) similarity map in token space\n",
    "            sim2d = sim.view(hv_t, wv_t)\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # UPSAMPLE SOLO DELLA SIMILARITY MAP (hard argmax più fine)\n",
    "            # token space (hv,wv) -> resized pixel space (H_prime,W_prime)\n",
    "            # ------------------------------------------------------------\n",
    "            sim_r = torch.nn.functional.interpolate(\n",
    "                sim2d[None, None],  # (1,1,hv,wv)\n",
    "                size=(H_prime, W_prime),  # resized (no pad)\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )[0, 0]  # (H_prime, W_prime)\n",
    "\n",
    "            # argmax in resized pixel coords\n",
    "            x_r, y_r = hard_argmax(sim_r)\n",
    "\n",
    "            # resized -> originale\n",
    "            x_pred = x_r / scale\n",
    "            y_pred = y_r / scale\n",
    "\n",
    "            # distanza in pixel originali\n",
    "            dx = x_pred - float(trg_kp[0])\n",
    "            dy = y_pred - float(trg_kp[1])\n",
    "            dist = math.sqrt(dx * dx + dy * dy)\n",
    "            distances_this_image.append(dist)\n",
    "\n",
    "        # salva risultato\n",
    "        results.append(\n",
    "            CorrespondenceResult(\n",
    "                category=category,\n",
    "                distances=distances_this_image,\n",
    "                pck_threshold_0_05=batch[\"pck_threshold_0_05\"],\n",
    "                pck_threshold_0_1=batch[\"pck_threshold_0_1\"],\n",
    "                pck_threshold_0_2=batch[\"pck_threshold_0_2\"]\n",
    "            )\n",
    "        )"
   ],
   "id": "1bba535963e7f5a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b: 100%|██████████| 12234/12234 [02:24<00:00, 84.65it/s] \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T00:50:47.277565937Z",
     "start_time": "2026-01-18T00:50:46.793876708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_results import compute_pckt_images, compute_correct_per_category, compute_pckt_keypoints\n",
    "\n",
    "# Compute and print results\n",
    "correct = compute_correct_per_category(results)\n",
    "compute_pckt_keypoints(correct)\n",
    "compute_pckt_images(correct)"
   ],
   "id": "e3a7661228d5944c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCK Results per keypoints (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  18.029807  25.881498  40.094511\n",
      "1       bicycle  10.113813  16.425246  27.728919\n",
      "2          bird  20.193192  31.025759  45.055198\n",
      "3          boat  10.395747  18.074424  32.693444\n",
      "4        bottle  17.175335  26.860791  41.731548\n",
      "5           bus  13.599821  18.222421  28.092899\n",
      "6           car  15.228285  20.740535  31.041203\n",
      "7           cat  28.234914  39.367538  55.017748\n",
      "8         chair   9.392114  13.691128  22.207010\n",
      "9           cow  19.372900  27.286301  40.425532\n",
      "10          dog  12.703040  20.512287  35.422741\n",
      "11        horse   9.800170  16.390306  27.933673\n",
      "12    motorbike   9.200470  16.754850  27.042916\n",
      "13       person  18.226257  31.028864  45.414339\n",
      "14  pottedplant  13.020372  23.184234  33.503100\n",
      "15        sheep   9.052632  15.710526  27.394737\n",
      "16        train  19.722864  31.004619  50.323326\n",
      "17    tvmonitor  14.443234  24.829839  41.110809\n",
      "18          All  14.883609  23.166176  36.235203\n",
      "PCK per-image (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  15.998564  23.496795  37.428686\n",
      "1       bicycle   8.919775  14.786330  25.398418\n",
      "2          bird  19.197936  29.708311  43.287039\n",
      "3          boat   8.377877  14.734206  27.864152\n",
      "4        bottle  16.036627  25.548942  40.224047\n",
      "5           bus  10.776821  14.415948  23.196662\n",
      "6           car  10.267217  14.634633  24.860560\n",
      "7           cat  28.333173  39.451486  55.168817\n",
      "8         chair   8.049335  12.110373  20.000949\n",
      "9           cow  16.066497  23.713835  35.645883\n",
      "10          dog  11.639045  19.059825  33.530187\n",
      "11        horse   8.826494  14.968457  26.365969\n",
      "12    motorbike   8.548704  16.279112  26.847556\n",
      "13       person  15.383887  27.161906  42.204255\n",
      "14  pottedplant  10.831078  19.707583  30.272714\n",
      "15        sheep   7.259181  12.715409  23.552061\n",
      "16        train  17.705268  28.445495  47.416039\n",
      "17    tvmonitor  14.039578  23.890012  39.960139\n",
      "18          All  13.125392  20.823814  33.512452\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
