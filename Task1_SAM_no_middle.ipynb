{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from data.spair import SPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.utils_featuremaps import PreComputedFeaturemaps\n",
    "\n",
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "save_dir = Path(base_dir) / \"data\" / \"features\"\n",
    "dataset_size = 'large'  # 'small' or 'large'\n",
    "\n",
    "# Load dataset and construct dataloader\n",
    "\n",
    "test_dataset = SPairDataset(datatype='test', dataset_size=dataset_size)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1, collate_fn=collate_single)\n",
    "print(\"Dataset loaded\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# SAM initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "\n",
    "print(f\"SAM '{selected_model}' loaded. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_dir = Path(base_dir) / \"data\" / \"features\"\n",
    "print(\"Saving features to:\", save_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with PreComputedFeaturemaps(save_dir, device=device) as pcm:\n",
    "        for img_tensor, img_size, img_category, img_name in tqdm(\n",
    "                test_dataset.iter_images(),\n",
    "                total=test_dataset.num_images(),\n",
    "                desc=\"Generating embeddings\"\n",
    "        ):\n",
    "            img_tensor = img_tensor.to(device).unsqueeze(0)  # [1,3,H,W]\n",
    "            orig_size = tuple(img_size[1:])  # (H,W)\n",
    "            resized = predictor.transform.apply_image_torch(img_tensor)  # [1,3,H',W']\n",
    "            predictor.set_torch_image(resized, orig_size)\n",
    "            img_emb = predictor.get_image_embedding()[0]  # [C,h',w']\n",
    "\n",
    "            pcm.save_featuremaps(img_emb, img_category, img_name)\n",
    "\n"
   ],
   "id": "dcb80b0af61ca562",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "def kps_src_to_featmap(kps_src: torch.Tensor, img_src_size: torch.Size):\n",
    "    img_h = int(img_src_size[-2])\n",
    "    img_w = int(img_src_size[-1])\n",
    "\n",
    "    # (N,2) coords nella resized (no padding)\n",
    "    coords = predictor.transform.apply_coords_torch(kps_src, (img_h, img_w))  # (N,2)\n",
    "\n",
    "    img_resized_h, img_resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    xf = torch.floor(coords[:, 0] / PATCH).long()\n",
    "    yf = torch.floor(coords[:, 1] / PATCH).long()\n",
    "\n",
    "    wv = math.ceil(img_resized_w / PATCH)\n",
    "    hv = math.ceil(img_resized_h / PATCH)\n",
    "\n",
    "    xf = xf.clamp(0, wv - 1)\n",
    "    yf = yf.clamp(0, hv - 1)\n",
    "\n",
    "    return torch.stack([xf, yf], dim=1)  # (N,2) (x_idx,y_idx)\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(y_featmap, x_featmap, trg_img_size: torch.Size):\n",
    "    img_h = int(trg_img_size[-2])\n",
    "    img_w = int(trg_img_size[-1])\n",
    "\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # token center in preprocessed (padded) image coords\n",
    "    yr = (y_featmap + 0.5) * 16\n",
    "    xr = (x_featmap + 0.5) * 16\n",
    "\n",
    "    # discard tokens that fall into padding\n",
    "    if xr < 0 or yr < 0 or xr >= resized_w or yr >= resized_h:\n",
    "        return None\n",
    "\n",
    "    scale = resized_w / img_w  # uniforme\n",
    "    y_trg = yr / scale\n",
    "    x_trg = xr / scale\n",
    "\n",
    "    return y_trg, x_trg"
   ],
   "id": "16781315f17b51d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T11:02:44.567673743Z",
     "start_time": "2026-01-17T11:00:42.427263754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_correspondence import hard_argmax\n",
    "from utils.utils_results import CorrespondenceResult\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with PreComputedFeaturemaps(save_dir, device=device) as pcm:\n",
    "        for batch in tqdm(\n",
    "            test_dataloader,\n",
    "            total=len(test_dataloader),\n",
    "            desc=f\"Elaborazione con SAM {selected_model}\"\n",
    "        ):\n",
    "            category = batch[\"category\"]\n",
    "\n",
    "            orig_size_src = batch[\"src_imsize\"]  # torch.Size([C, Hs, Ws]) o simile\n",
    "            orig_size_trg = batch[\"trg_imsize\"]  # torch.Size([C, Ht, Wt]) o simile\n",
    "\n",
    "            src_imname = batch[\"src_imname\"]\n",
    "            trg_imname = batch[\"trg_imname\"]\n",
    "\n",
    "            # Embeddings SAM (C, h, w) tipicamente (256, 64, 64) o simili\n",
    "            src_emb = pcm.load_featuremaps(category, src_imname)  # [C,hs,ws]\n",
    "            trg_emb = pcm.load_featuremaps(category, trg_imname)  # [C,ht,wt]\n",
    "\n",
    "            # Keypoints (N,2) in pixel originali, ordine (x,y)\n",
    "            src_kps = batch[\"src_kps\"].to(device)\n",
    "            trg_kps = batch[\"trg_kps\"].to(device)\n",
    "\n",
    "            # -------------------------\n",
    "            # Target: dimensioni originali + dimensioni resize (no padding)\n",
    "            # -------------------------\n",
    "            Ht = int(orig_size_trg[-2])\n",
    "            Wt = int(orig_size_trg[-1])\n",
    "\n",
    "            H_prime, W_prime = predictor.transform.get_preprocess_shape(\n",
    "                Ht, Wt, predictor.transform.target_length\n",
    "            )\n",
    "\n",
    "            # Regione valida in token (no padding)\n",
    "            hv_t = (H_prime + PATCH - 1) // PATCH\n",
    "            wv_t = (W_prime + PATCH - 1) // PATCH\n",
    "\n",
    "            # -------------------------\n",
    "            # Prepara target flat sulla regione valida\n",
    "            # -------------------------\n",
    "            C_ft = trg_emb.shape[0]\n",
    "            trg_valid = trg_emb[:, :hv_t, :wv_t]                       # [C, hv, wv]\n",
    "            trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)    # [Pvalid, C]\n",
    "\n",
    "            # -------------------------\n",
    "            # Mappa i keypoint SRC -> indici featuremap SRC (token space)\n",
    "            # (Assumo che kps_src_to_featmap ritorni (N,2) (x_idx, y_idx) long)\n",
    "            # -------------------------\n",
    "            src_kps_idx = kps_src_to_featmap(src_kps, orig_size_src)   # (N,2) (x_idx,y_idx)\n",
    "\n",
    "            N_kps = src_kps_idx.shape[0]\n",
    "            distances_this_image = []\n",
    "\n",
    "            # scala SAM (uniforme sul lato lungo)\n",
    "            if Wt >= Ht:\n",
    "                scale = W_prime / Wt\n",
    "            else:\n",
    "                scale = H_prime / Ht\n",
    "\n",
    "            # -------------------------\n",
    "            # Loop keypoints\n",
    "            # -------------------------\n",
    "            for i in range(N_kps):\n",
    "                src_idx = src_kps_idx[i]   # (x_idx, y_idx) su featuremap\n",
    "                trg_kp = trg_kps[i]        # (x,y) originale\n",
    "\n",
    "                if torch.isnan(src_idx).any() or torch.isnan(trg_kp).any():\n",
    "                    continue\n",
    "\n",
    "                x_idx = int(src_idx[0].item())\n",
    "                y_idx = int(src_idx[1].item())\n",
    "\n",
    "                # Feature vector sorgente (C,)\n",
    "                src_vec = src_emb[:, y_idx, x_idx]  # [C]\n",
    "\n",
    "                # Cosine similarity su tutte le posizioni target valide\n",
    "                sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [Pvalid]\n",
    "\n",
    "                # (hv_t, wv_t) similarity map in token space\n",
    "                sim2d = sim.view(hv_t, wv_t)\n",
    "\n",
    "                # ------------------------------------------------------------\n",
    "                # UPSAMPLE SOLO DELLA SIMILARITY MAP (hard argmax più fine)\n",
    "                # token space (hv,wv) -> resized pixel space (H_prime,W_prime)\n",
    "                # ------------------------------------------------------------\n",
    "                sim_r = torch.nn.functional.interpolate(\n",
    "                    sim2d[None, None],               # (1,1,hv,wv)\n",
    "                    size=(H_prime, W_prime),         # resized (no pad)\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )[0, 0]                              # (H_prime, W_prime)\n",
    "\n",
    "                # argmax in resized pixel coords\n",
    "                x_r, y_r = hard_argmax(sim_r)\n",
    "\n",
    "                # resized -> originale\n",
    "                x_pred = x_r / scale\n",
    "                y_pred = y_r / scale\n",
    "\n",
    "                # distanza in pixel originali\n",
    "                dx = x_pred - float(trg_kp[0])\n",
    "                dy = y_pred - float(trg_kp[1])\n",
    "                dist = math.sqrt(dx * dx + dy * dy)\n",
    "                distances_this_image.append(dist)\n",
    "\n",
    "            # salva risultato\n",
    "            results.append(\n",
    "                CorrespondenceResult(\n",
    "                    category=category,\n",
    "                    distances=distances_this_image,\n",
    "                    pck_threshold_0_05=batch[\"pck_threshold_0_05\"],\n",
    "                    pck_threshold_0_1=batch[\"pck_threshold_0_1\"],\n",
    "                    pck_threshold_0_2=batch[\"pck_threshold_0_2\"]\n",
    "                )\n",
    "            )"
   ],
   "id": "1bba535963e7f5a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b: 100%|██████████| 12234/12234 [02:02<00:00, 100.21it/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T11:02:47.538015703Z",
     "start_time": "2026-01-17T11:02:47.074402501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_results import compute_pckt_images, compute_correct_per_category, compute_pckt_keypoints\n",
    "\n",
    "# Compute and print results\n",
    "correct = compute_correct_per_category(results)\n",
    "compute_pckt_keypoints(correct)\n",
    "compute_pckt_images(correct)"
   ],
   "id": "e3a7661228d5944c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCK Results per keypoints (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  17.938931  26.045075  39.912759\n",
      "1       bicycle  10.194049  16.067270  27.347995\n",
      "2          bird  20.414747  30.967742  45.299539\n",
      "3          boat  10.366214  18.133491  32.959244\n",
      "4        bottle  17.274143  26.728972  41.915888\n",
      "5           bus  13.424168  18.137145  28.121510\n",
      "6           car  15.033408  20.545657  30.846325\n",
      "7           cat  28.170378  39.415941  54.969345\n",
      "8         chair   9.364732  13.910186  22.426068\n",
      "9           cow  19.186264  27.323628  40.854797\n",
      "10          dog  12.919358  20.795999  35.507397\n",
      "11        horse   9.868141  16.333475  27.839217\n",
      "12    motorbike   8.847737  16.460905  26.807760\n",
      "13       person  18.179702  30.726257  45.297952\n",
      "14  pottedplant  12.887511  23.095660  33.392383\n",
      "15        sheep   9.000000  15.631579  26.842105\n",
      "16        train  19.630485  30.542725  49.896074\n",
      "17    tvmonitor  14.326570  24.622089  40.542013\n",
      "18          All  14.834808  23.082433  36.154354\n",
      "PCK per-image (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  15.815338  23.647742  37.358754\n",
      "1       bicycle   9.018115  14.317832  24.901532\n",
      "2          bird  19.465609  29.767295  43.699923\n",
      "3          boat   8.298569  14.591134  27.930516\n",
      "4        bottle  16.132549  25.474685  40.550447\n",
      "5           bus  10.527678  14.394548  23.146751\n",
      "6           car  10.175401  14.568447  24.710800\n",
      "7           cat  28.306876  39.484397  55.154671\n",
      "8         chair   8.030242  12.381086  20.204896\n",
      "9           cow  15.906482  23.822078  35.984364\n",
      "10          dog  11.917544  19.346874  33.615053\n",
      "11        horse   8.864046  14.867559  26.292015\n",
      "12    motorbike   8.343000  16.071994  26.509858\n",
      "13       person  15.262093  26.840728  41.991079\n",
      "14  pottedplant  10.650665  19.566991  30.383613\n",
      "15        sheep   7.132161  12.501456  22.954477\n",
      "16        train  17.655086  28.064349  47.066552\n",
      "17    tvmonitor  13.881544  23.785230  39.518492\n",
      "18          All  13.076833  20.749690  33.442989\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
