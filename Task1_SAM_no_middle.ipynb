{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:26:59.849441854Z",
     "start_time": "2026-01-17T01:26:59.813021034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from data.spair import SPairDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.utils_featuremaps import PreComputedFeaturemaps\n",
    "\n",
    "using_colab = 'google.colab' in str(get_ipython())\n",
    "base_dir = os.path.abspath(os.path.curdir)\n",
    "\n",
    "if using_colab:\n",
    "    base_dir = os.path.join(os.path.abspath(os.path.curdir), 'AML-polito')\n",
    "\n",
    "\n",
    "def collate_single(batch_list):\n",
    "    return batch_list[0]\n",
    "\n",
    "save_dir = Path(base_dir) / \"data\" / \"features\"\n",
    "dataset_size = 'large'  # 'small' or 'large'\n",
    "\n",
    "# Load dataset and construct dataloader\n",
    "\n",
    "test_dataset = SPairDataset(datatype='test', dataset_size=dataset_size)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1, collate_fn=collate_single)\n",
    "print(\"Dataset loaded\")"
   ],
   "id": "18a123c5d94c968",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:27:03.736249929Z",
     "start_time": "2026-01-17T01:27:02.946845437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_size = {\n",
    "    \"vit_b\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\", \"sam_vit_b_01ec64.pth\"),\n",
    "    \"vit_l\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\", \"sam_vit_l_0b3195.pth\"),\n",
    "    \"vit_h\": (\"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\", \"sam_vit_h_4b8939.pth\")\n",
    "}\n",
    "\n",
    "selected_model = \"vit_b\"\n",
    "\n",
    "if using_colab:\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git -q\n",
    "    !wget -P ./AML-polito/models/ {model_size[selected_model][0]}\n",
    "    !clear\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "\n",
    "# SAM initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "sam_checkpoint_path = os.path.join(base_dir, 'models', model_size[selected_model][1])\n",
    "sam = sam_model_registry[selected_model](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device)\n",
    "sam.eval()\n",
    "predictor = SamPredictor(sam)\n",
    "transform = predictor.transform\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = predictor.model.image_encoder.img_size  # 1024\n",
    "PATCH = int(predictor.model.image_encoder.patch_embed.proj.kernel_size[0])  # 16\n",
    "\n",
    "print(f\"SAM '{selected_model}' loaded. IMG_SIZE={IMG_SIZE}, PATCH={PATCH}\")"
   ],
   "id": "6e6dfe580103c74a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SAM 'vit_b' loaded. IMG_SIZE=1024, PATCH=16\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_dir = Path(base_dir) / \"data\" / \"features\"\n",
    "print(\"Saving features to:\", save_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with PreComputedFeaturemaps(save_dir, device=device) as pcm:\n",
    "        for img_tensor, img_size, img_category, img_name in tqdm(\n",
    "                test_dataset.iter_images(),\n",
    "                total=test_dataset.num_images(),\n",
    "                desc=\"Generating embeddings\"\n",
    "        ):\n",
    "            img_tensor = img_tensor.to(device).unsqueeze(0)  # [1,3,H,W]\n",
    "            orig_size = tuple(img_size[1:])  # (H,W)\n",
    "            resized = predictor.transform.apply_image_torch(img_tensor)  # [1,3,H',W']\n",
    "            predictor.set_torch_image(resized, orig_size)\n",
    "            img_emb = predictor.get_image_embedding()[0]  # [C,h',w']\n",
    "\n",
    "            pcm.save_featuremaps(img_emb, img_category, img_name)\n",
    "\n"
   ],
   "id": "dcb80b0af61ca562"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:27:07.311172041Z",
     "start_time": "2026-01-17T01:27:07.300678302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_scale_factor(img_size: tuple) -> float:\n",
    "    img_h, img_w = img_size\n",
    "    resized_h, resized_w = predictor.transform.get_preprocess_shape(img_h, img_w, IMG_SIZE)\n",
    "\n",
    "    # lato lungo originale → lato lungo resized\n",
    "    if img_w >= img_h:\n",
    "        return resized_w / img_w\n",
    "    else:\n",
    "        return resized_h / img_h\n",
    "\n",
    "\n",
    "def kp_src_to_featmap(kp_src_coordinates: torch.Tensor, img_src_size: tuple):\n",
    "    img_src_h, img_src_w = img_src_size\n",
    "\n",
    "    # Compute coordinates in resized image (without padding)\n",
    "    x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[0]\n",
    "\n",
    "    # dimensioni resized reali\n",
    "    img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n",
    "                                                                                          IMG_SIZE)\n",
    "\n",
    "    # resized -> feature map\n",
    "    xf = int(x_prepad // PATCH)\n",
    "    yf = int(y_prepad // PATCH)\n",
    "\n",
    "    # regione valida (no padding)\n",
    "    wv = math.ceil(img_resized_w_prepad / PATCH)\n",
    "    hv = math.ceil(img_resized_h_prepad / PATCH)\n",
    "\n",
    "    xf = min(max(xf, 0), wv - 1)\n",
    "    yf = min(max(yf, 0), hv - 1)\n",
    "\n",
    "    return xf, yf\n",
    "\n",
    "\n",
    "def kp_featmap_to_trg(featmap_coords: tuple, trg_img_size: tuple):\n",
    "    xf, yf = featmap_coords\n",
    "    xr = (xf + 0.5) * PATCH\n",
    "    yr = (yf + 0.5) * PATCH\n",
    "\n",
    "    scale = get_scale_factor(trg_img_size)\n",
    "    xo = xr / scale\n",
    "    yo = yr / scale\n",
    "\n",
    "    return xo, yo"
   ],
   "id": "16781315f17b51d3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T01:27:44.204757251Z",
     "start_time": "2026-01-17T01:27:28.605307963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_correspondence import hard_argmax\n",
    "from utils.utils_results import CorrespondenceResult\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with PreComputedFeaturemaps(save_dir, device=device) as pcm:\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader), desc=f\"Elaborazione con SAM {selected_model}\"):\n",
    "\n",
    "            category = batch[\"category\"]\n",
    "\n",
    "            orig_size_src = tuple(batch[\"src_imsize\"][1:])  # (Hs, Ws)\n",
    "            orig_size_trg = tuple(batch[\"trg_imsize\"][1:])  # (Ht, Wt)\n",
    "\n",
    "            src_imname = batch[\"src_imname\"]\n",
    "            trg_imname = batch[\"trg_imname\"]\n",
    "\n",
    "            src_emb = pcm.load_featuremaps(category, src_imname)  # [C,hs,ws]\n",
    "            trg_emb = pcm.load_featuremaps(category, trg_imname)  # [C,ht,wt]\n",
    "\n",
    "            # Keypoints & metadata\n",
    "            src_kps = batch[\"src_kps\"].to(device)  # [N,2]\n",
    "            trg_kps = batch[\"trg_kps\"].to(device)  # [N,2]\n",
    "            pck_thr_0_05 = batch[\"pck_threshold_0_05\"]\n",
    "            pck_thr_0_1 = batch[\"pck_threshold_0_1\"]\n",
    "            pck_thr_0_2 = batch[\"pck_threshold_0_2\"]\n",
    "\n",
    "            # Target valid region\n",
    "            Ht, Wt = orig_size_trg\n",
    "            H_prime, W_prime = predictor.transform.get_preprocess_shape(\n",
    "                Ht, Wt, predictor.transform.target_length\n",
    "            )\n",
    "\n",
    "            hv_t = (H_prime + PATCH - 1) // PATCH\n",
    "            wv_t = (W_prime + PATCH - 1) // PATCH\n",
    "\n",
    "            N_kps = src_kps.shape[0]\n",
    "\n",
    "            C_ft = trg_emb.shape[0]\n",
    "\n",
    "            trg_valid = trg_emb[:, :hv_t, :wv_t]  # [C,hv,wv]\n",
    "            trg_flat = trg_valid.permute(1, 2, 0).reshape(-1, C_ft)  # [Pvalid,C]\n",
    "\n",
    "            distances_this_image = []\n",
    "\n",
    "            for i in range(N_kps):\n",
    "                src_keypoint = src_kps[i].unsqueeze(0)  # [1,2] (x,y)\n",
    "                trg_keypoint = trg_kps[i]  # [2]   (x,y)\n",
    "\n",
    "                if torch.isnan(src_keypoint).any() or torch.isnan(trg_keypoint).any():\n",
    "                    continue\n",
    "\n",
    "                # originale src -> feature src\n",
    "                x_idx, y_idx = kp_src_to_featmap(src_keypoint, orig_size_src)\n",
    "\n",
    "                # feature vector sorgente\n",
    "                src_vec = src_emb[:, y_idx, x_idx]  # [256]\n",
    "\n",
    "                # cosine similarity con tutte le posizioni valide del target\n",
    "                sim = torch.cosine_similarity(trg_flat, src_vec.unsqueeze(0), dim=1)  # [P]\n",
    "\n",
    "                sim2d = sim.view(hv_t, wv_t)\n",
    "                x_idx_t, y_idx_t = hard_argmax(sim2d)\n",
    "\n",
    "                # feature target -> pixel originali target\n",
    "                x_pred, y_pred = kp_featmap_to_trg((x_idx_t, y_idx_t), orig_size_trg)\n",
    "\n",
    "                dist = math.sqrt((x_pred - trg_keypoint[0]) ** 2 + (y_pred - trg_keypoint[1]) ** 2)\n",
    "                distances_this_image.append(dist)\n",
    "\n",
    "            results.append(\n",
    "                CorrespondenceResult(\n",
    "                    category=category,\n",
    "                    distances=distances_this_image,\n",
    "                    pck_threshold_0_05=pck_thr_0_05,\n",
    "                    pck_threshold_0_1=pck_thr_0_1,\n",
    "                    pck_threshold_0_2=pck_thr_0_2\n",
    "                )\n",
    "            )"
   ],
   "id": "1bba535963e7f5a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Elaborazione con SAM vit_b:  13%|█▎        | 1609/12234 [00:15<01:41, 105.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 55\u001B[39m\n\u001B[32m     52\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     54\u001B[39m \u001B[38;5;66;03m# originale src -> feature src\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m x_idx, y_idx = \u001B[43mkp_src_to_featmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_keypoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morig_size_src\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[38;5;66;03m# feature vector sorgente\u001B[39;00m\n\u001B[32m     58\u001B[39m src_vec = src_emb[:, y_idx, x_idx]  \u001B[38;5;66;03m# [256]\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mkp_src_to_featmap\u001B[39m\u001B[34m(kp_src_coordinates, img_src_size)\u001B[39m\n\u001B[32m     16\u001B[39m img_src_h, img_src_w = img_src_size\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Compute coordinates in resized image (without padding)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m x_prepad, y_prepad = predictor.transform.apply_coords_torch(kp_src_coordinates, img_src_size)[\u001B[32m0\u001B[39m]\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# dimensioni resized reali\u001B[39;00m\n\u001B[32m     22\u001B[39m img_resized_h_prepad, img_resized_w_prepad = predictor.transform.get_preprocess_shape(img_src_h, img_src_w,\n\u001B[32m     23\u001B[39m                                                                                       IMG_SIZE)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/aml-python-env/lib64/python3.12/site-packages/torch/_tensor.py:1161\u001B[39m, in \u001B[36mTensor.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1151\u001B[39m         warnings.warn(\n\u001B[32m   1152\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mUsing len to get tensor shape might cause the trace to be incorrect. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1153\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mRecommended usage would be tensor.shape[0]. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1157\u001B[39m             stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m   1158\u001B[39m         )\n\u001B[32m   1159\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.shape[\u001B[32m0\u001B[39m]\n\u001B[32m-> \u001B[39m\u001B[32m1161\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m   1162\u001B[39m     \u001B[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001B[39;00m\n\u001B[32m   1163\u001B[39m     \u001B[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001B[39;00m\n\u001B[32m   1164\u001B[39m     \u001B[38;5;66;03m# save us work, and also helps keep trace ordering deterministic\u001B[39;00m\n\u001B[32m   1165\u001B[39m     \u001B[38;5;66;03m# (e.g., if you zip(*hiddens), the eager map will force all the\u001B[39;00m\n\u001B[32m   1166\u001B[39m     \u001B[38;5;66;03m# indexes of hiddens[0] before hiddens[1], while the generator\u001B[39;00m\n\u001B[32m   1167\u001B[39m     \u001B[38;5;66;03m# map will interleave them.)\u001B[39;00m\n\u001B[32m   1168\u001B[39m     \u001B[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001B[39;00m\n\u001B[32m   1169\u001B[39m     \u001B[38;5;66;03m# See gh-54457\u001B[39;00m\n\u001B[32m   1170\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dim() == \u001B[32m0\u001B[39m:\n\u001B[32m   1171\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33miteration over a 0-d tensor\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T00:02:24.507732231Z",
     "start_time": "2026-01-17T00:02:23.988755455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.utils_results import compute_pckt_images, compute_correct_per_category, compute_pckt_keypoints\n",
    "\n",
    "# Compute and print results\n",
    "correct = compute_correct_per_category(results)\n",
    "compute_pckt_keypoints(correct)\n",
    "compute_pckt_images(correct)"
   ],
   "id": "e3a7661228d5944c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCK Results per keypoints (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  17.938931  26.045075  39.912759\n",
      "1       bicycle  10.191412  16.063114  27.340921\n",
      "2          bird  20.377185  30.910764  45.216191\n",
      "3          boat  10.336680  18.133491  32.959244\n",
      "4        bottle  17.284335  26.720648  41.902834\n",
      "5           bus  13.421170  18.133095  28.115230\n",
      "6           car  15.033408  20.545657  30.846325\n",
      "7           cat  28.170378  39.415941  54.969345\n",
      "8         chair   9.364732  13.910186  22.426068\n",
      "9           cow  19.186264  27.323628  40.854797\n",
      "10          dog  12.911287  20.741358  35.485214\n",
      "11        horse   9.885204  16.347789  27.848639\n",
      "12    motorbike   8.877131  16.460905  26.807760\n",
      "13       person  18.179702  30.726257  45.297952\n",
      "14  pottedplant  12.887511  23.095660  33.392383\n",
      "15        sheep   9.000000  15.631579  26.842105\n",
      "16        train  19.630485  30.542725  49.907621\n",
      "17    tvmonitor  14.320719  24.612034  40.525456\n",
      "18          All  14.833141  23.075550  36.147269\n",
      "PCK per-image (%):\n",
      "       Category   PCK 0.05    PCK 0.1    PCK 0.2\n",
      "0     aeroplane  15.815338  23.647742  37.358754\n",
      "1       bicycle   9.014452  14.314169  24.890543\n",
      "2          bird  19.420726  29.703419  43.593368\n",
      "3          boat   8.274827  14.591134  27.930516\n",
      "4        bottle  16.147601  25.468528  40.541097\n",
      "5           bus  10.523696  14.388576  23.140779\n",
      "6           car  10.175401  14.568447  24.710800\n",
      "7           cat  28.306876  39.484397  55.154671\n",
      "8         chair   8.030242  12.381086  20.204896\n",
      "9           cow  15.906482  23.822078  35.984364\n",
      "10          dog  11.913576  19.309898  33.607117\n",
      "11        horse   8.877345  14.874461  26.294372\n",
      "12    motorbike   8.371490  16.071994  26.509858\n",
      "13       person  15.262093  26.840728  41.991079\n",
      "14  pottedplant  10.650665  19.566991  30.383613\n",
      "15        sheep   7.132161  12.501456  22.954477\n",
      "16        train  17.655086  28.064349  47.076000\n",
      "17    tvmonitor  13.881544  23.782018  39.487985\n",
      "18          All  13.075533  20.743415  33.434127\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
