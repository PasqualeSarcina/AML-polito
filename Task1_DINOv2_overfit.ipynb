{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y8AKrNt5lxaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' estratto con successo nella directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
      "Contenuti della directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted':\n",
      "['SPair-71k']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_file_path = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' \n",
    "extract_dir = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
    "\n",
    "# Crea la directory di estrazione se non esiste\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Estrai il file ZIP solo se esiste\n",
    "if os.path.exists(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"File '{zip_file_path}' estratto con successo nella directory '{extract_dir}'\")\n",
    "    print(f\"Contenuti della directory '{extract_dir}':\\n{os.listdir(extract_dir)}\")\n",
    "else:\n",
    "    print(f\"File zip '{zip_file_path}' non trovato. Assicurati che il dataset sia estratto in '{extract_dir}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "honcpimEq_B2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Set...\n",
      "Loading Validation Set...\n",
      "Testing batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Images: torch.Size([4, 3, 518, 518])\n",
      "Val Batch Images:   torch.Size([4, 3, 518, 518])\n",
      "Dataset setup complete. Ready for Training Loop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- 1. Define the Augmentation Pipeline ---\n",
    "def get_transforms(mode='train', img_size=518):\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            # Geometric Augmentations (Hard - Moves Keypoints)\n",
    "            A.Resize(height=img_size, width=img_size), # Force DINOv2 size\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            \n",
    "            # Pixel Augmentations (Safe - Colors only)\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.GaussianBlur(p=0.1),\n",
    "            \n",
    "            # Normalization & Conversion\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(), # Converts to (C, H, W)\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "    \n",
    "    else:\n",
    "        # Validation/Test: Only Resize & Normalize\n",
    "        return A.Compose([\n",
    "            A.Resize(height=img_size, width=img_size),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "# --- 2. Simple Image Reader ---\n",
    "def read_img(path):\n",
    "    # Keep as HWC (Standard for Albumentations)\n",
    "    # Do not transpose or convert to Tensor here yet\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "    return img\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n",
    "        self.datatype = datatype\n",
    "        self.pck_alpha = pck_alpha\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = [x for x in self.ann_files if x] # Remove empty strings\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        \n",
    "        # Initialize the Transform Pipeline\n",
    "        mode = 'train' if datatype == 'trn' else 'test'\n",
    "        self.transform = get_transforms(mode=mode, img_size=518)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_line = self.ann_files[idx]\n",
    "        ann_filename = raw_line.replace(':', '_')\n",
    "        ann_file = ann_filename + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_path = os.path.join(self.image_path, category, annotation['src_imname'])\n",
    "        trg_path = os.path.join(self.image_path, category, annotation['trg_imname'])\n",
    "\n",
    "        # 1. Load Images\n",
    "        src_img_raw = read_img(src_path)\n",
    "        trg_img_raw = read_img(trg_path)\n",
    "\n",
    "        # 2. Get Keypoints\n",
    "        src_kps = np.array(annotation['src_kps']).astype(np.float32)\n",
    "        trg_kps = np.array(annotation['trg_kps']).astype(np.float32)\n",
    "\n",
    "        # 3. Apply Augmentations\n",
    "        src_aug = self.transform(image=src_img_raw, keypoints=src_kps)\n",
    "        src_tensor = src_aug['image']\n",
    "        src_kps_aug = np.array(src_aug['keypoints'])\n",
    "        \n",
    "        trg_aug = self.transform(image=trg_img_raw, keypoints=trg_kps)\n",
    "        trg_tensor = trg_aug['image']\n",
    "        trg_kps_aug = np.array(trg_aug['keypoints'])\n",
    "\n",
    "        # ==========================================================\n",
    "        # ⚠️ CRITICAL FIX: PADDING LOGIC (Prevents Stack Error)\n",
    "        # ==========================================================\n",
    "        # We enforce a fixed size of 40 points per image.\n",
    "        MAX_KPS = 40 \n",
    "        \n",
    "        # Create empty containers filled with zeros (Shape: [40, 2])\n",
    "        src_kps_padded = np.zeros((MAX_KPS, 2), dtype=np.float32)\n",
    "        trg_kps_padded = np.zeros((MAX_KPS, 2), dtype=np.float32)\n",
    "        \n",
    "        # Get the actual number of points (limit to 40 just in case)\n",
    "        n_src = min(len(src_kps_aug), MAX_KPS)\n",
    "        n_trg = min(len(trg_kps_aug), MAX_KPS)\n",
    "        \n",
    "        # Copy the real points into the empty container\n",
    "        if n_src > 0:\n",
    "            src_kps_padded[:n_src] = src_kps_aug[:n_src]\n",
    "        if n_trg > 0:\n",
    "            trg_kps_padded[:n_trg] = trg_kps_aug[:n_trg]\n",
    "\n",
    "        # Check which points are inside the image (Visibility)\n",
    "        src_vis = self._check_visibility(src_kps_padded, 518, 518)\n",
    "        trg_vis = self._check_visibility(trg_kps_padded, 518, 518)\n",
    "        \n",
    "        # Create the Valid Mask\n",
    "        # A point is valid ONLY if:\n",
    "        # 1. It existed in the original file (index < n_src)\n",
    "        # 2. It is still inside the image boundaries (vis=1)\n",
    "        valid_mask = np.zeros(MAX_KPS, dtype=np.float32)\n",
    "        \n",
    "        # We assume points are ordered pairs (1st src matches 1st trg)\n",
    "        # So we only mark as valid if BOTH exist and are visible\n",
    "        common_len = min(n_src, n_trg)\n",
    "        valid_mask[:common_len] = src_vis[:common_len] * trg_vis[:common_len]\n",
    "        # ==========================================================\n",
    "\n",
    "        pck_threshold = 518 * self.pck_alpha\n",
    "\n",
    "        sample = {\n",
    "            'pair_id': annotation['pair_id'],\n",
    "            'src_img': src_tensor,\n",
    "            'trg_img': trg_tensor,\n",
    "            \n",
    "            # Now these are ALWAYS [40, 2], so PyTorch won't crash!\n",
    "            'src_kps': torch.from_numpy(src_kps_padded).float(), \n",
    "            'trg_kps': torch.from_numpy(trg_kps_padded).float(), \n",
    "            'valid_mask': torch.from_numpy(valid_mask).float(), \n",
    "            \n",
    "            'pck_threshold': pck_threshold,\n",
    "            'category': category\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _check_visibility(self, kps, h, w):\n",
    "        \"\"\"Returns a binary mask (N,) where 1=visible, 0=out of bounds\"\"\"\n",
    "        # kps is shape (N, 2) -> (x, y)\n",
    "        x = kps[:, 0]\n",
    "        y = kps[:, 1]\n",
    "        \n",
    "        # Check boundaries\n",
    "        vis_x = (x >= 0) & (x < w)\n",
    "        vis_y = (y >= 0) & (y < h)\n",
    "        return (vis_x & vis_y).astype(np.float32)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Update this path to your actual path\n",
    "    base_dir = r\"C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted\\SPair-71k\\SPair-71k\"    \n",
    "    \n",
    "    pair_ann_path = os.path.join(base_dir, 'PairAnnotation')\n",
    "    layout_path = os.path.join(base_dir, 'Layout')\n",
    "    image_path = os.path.join(base_dir, 'JPEGImages')\n",
    "\n",
    "    # Check paths\n",
    "    if os.path.exists(base_dir):\n",
    "        \n",
    "        # --- 1. Load TRAINING Set ---\n",
    "        print(\"Loading Training Set...\")\n",
    "        trn_dataset = SPairDataset(\n",
    "            pair_ann_path, layout_path, image_path, \n",
    "            dataset_size='large', pck_alpha=0.05, \n",
    "            datatype='trn'  # <--- Loads from trn.txt\n",
    "        )\n",
    "        # SHUFFLE = TRUE for training (important for learning)\n",
    "        trn_loader = DataLoader(trn_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "        # --- 2. Load VALIDATION Set ---\n",
    "        print(\"Loading Validation Set...\")\n",
    "        val_dataset = SPairDataset(\n",
    "            pair_ann_path, layout_path, image_path, \n",
    "            dataset_size='large', pck_alpha=0.05, \n",
    "            datatype='val'  # <--- Loads from val.txt\n",
    "        )\n",
    "        # SHUFFLE = FALSE for validation (keep order stable)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "        \n",
    "        # --- 3. Test Loading ---\n",
    "        print(\"Testing batches...\")\n",
    "        \n",
    "        # Grab a training batch\n",
    "        trn_batch = next(iter(trn_loader))\n",
    "        print(f\"Train Batch Images: {trn_batch['src_img'].shape}\")\n",
    "        \n",
    "        # Grab a validation batch\n",
    "        val_batch = next(iter(val_loader))\n",
    "        print(f\"Val Batch Images:   {val_batch['src_img'].shape}\")\n",
    "        \n",
    "        print(\"Dataset setup complete. Ready for Training Loop.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Path not found: {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Overfit Small Sample ---\n",
      "Loading Model on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing one batch...\n",
      "Starting Training Loop (100 Iterations)...\n",
      "Iter   0: Loss = 4.52095\n",
      "Iter  10: Loss = 1.39504\n",
      "Iter  20: Loss = 0.59009\n",
      "Iter  30: Loss = 0.37727\n",
      "Iter  40: Loss = 0.27629\n",
      "Iter  50: Loss = 0.12469\n",
      "Iter  60: Loss = 0.62037\n",
      "Iter  70: Loss = 0.21654\n",
      "Iter  80: Loss = 0.53351\n",
      "Iter  90: Loss = 0.24069\n",
      "Iter 100: Loss = 0.08809\n",
      ">>> Converged! Model has memorized the batch.\n",
      "\n",
      "--- Result Analysis ---\n",
      "✅ PASSED. Model is capable of learning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- 1. Helper: Map (x,y) coordinates to Patch Index ---\n",
    "def get_patch_indices(keypoints, img_size=518, patch_size=14):\n",
    "    \"\"\"\n",
    "    Converts (x, y) pixel coordinates into the index of the patch (0 to 1368).\n",
    "    keypoints: [Batch, N_kps, 2]\n",
    "    \"\"\"\n",
    "    grid_w = img_size // patch_size # 37\n",
    "    \n",
    "    # Scale coordinates to grid integers (0..36)\n",
    "    grid_x = (keypoints[:, :, 0] / patch_size).long().clamp(0, grid_w-1)\n",
    "    grid_y = (keypoints[:, :, 1] / patch_size).long().clamp(0, grid_w-1)\n",
    "    \n",
    "    # Calculate flat index (y * width + x)\n",
    "    flat_indices = grid_y * grid_w + grid_x  # [Batch, N_kps]\n",
    "    return flat_indices\n",
    "\n",
    "# --- 2. Helper: Extract Features at those Indices ---\n",
    "def extract_features_at_indices(features, indices):\n",
    "    \"\"\"\n",
    "    features: [Batch, 1369, Dim]\n",
    "    indices:  [Batch, N_kps]\n",
    "    Returns:  [Batch, N_kps, Dim]\n",
    "    \"\"\"\n",
    "    B, N_patches, Dim = features.shape\n",
    "    B, N_kps = indices.shape\n",
    "    \n",
    "    # Expand indices to gather across the Dim dimension\n",
    "    # [B, N_kps] -> [B, N_kps, Dim]\n",
    "    indices_expanded = indices.unsqueeze(-1).expand(-1, -1, Dim)\n",
    "    \n",
    "    # Gather specific features\n",
    "    kps_features = torch.gather(features, 1, indices_expanded)\n",
    "    return kps_features\n",
    "\n",
    "# --- 3. The Contrastive Loss ---\n",
    "def contrastive_loss(feat_src_kps, feat_trg_all, trg_kps_indices, mask, temp=0.1):\n",
    "    \"\"\"\n",
    "    feat_src_kps:    [B, N, Dim]   (Query: Feature at Source Nose)\n",
    "    feat_trg_all:    [B, 1369, Dim](Keys: All patches in Target Image)\n",
    "    trg_kps_indices: [B, N]        (Label: Index of Target Nose)\n",
    "    mask:            [B, N]        (Valid points only)\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    feat_src_kps = F.normalize(feat_src_kps, dim=-1)\n",
    "    feat_trg_all = F.normalize(feat_trg_all, dim=-1)\n",
    "    \n",
    "    # Similarity: [B, N, Dim] @ [B, Dim, 1369] -> [B, N, 1369]\n",
    "    # We compare every Source Keypoint against ALL Target Patches\n",
    "    logits = torch.bmm(feat_src_kps, feat_trg_all.transpose(1, 2)) / temp\n",
    "    \n",
    "    # Flatten everything to 2D for CrossEntropy\n",
    "    # We only care about VALID keypoints\n",
    "    valid = mask.bool()\n",
    "    \n",
    "    logits_valid = logits[valid]       # [Total_Valid_Kps, 1369]\n",
    "    targets_valid = trg_kps_indices[valid] # [Total_Valid_Kps]\n",
    "    \n",
    "    loss = F.cross_entropy(logits_valid, targets_valid)\n",
    "    return loss\n",
    "\n",
    "# --- 4. Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Step 2: Overfit Small Sample ---\")\n",
    "    \n",
    "    # A. Setup Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading Model on {device}...\")\n",
    "    model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').to(device)\n",
    "    \n",
    "    # Freeze Backbone\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze Last 2 Blocks\n",
    "    for block in model.blocks[-2:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # B. Setup Optimizer (High LR, No Decay)\n",
    "    # We filter specifically for parameters that require grad\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                            lr=1e-3, weight_decay=0)\n",
    "    \n",
    "    # C. Get ONE Fixed Batch\n",
    "    print(\"Grabbing one batch...\")\n",
    "    try:\n",
    "        batch = next(iter(trn_loader))\n",
    "    except NameError:\n",
    "        print(\"Error: trn_loader not defined. Run dataset code first.\")\n",
    "        exit()\n",
    "\n",
    "    src = batch['src_img'].to(device)\n",
    "    trg = batch['trg_img'].to(device)\n",
    "    src_kps = batch['src_kps'].to(device)\n",
    "    trg_kps = batch['trg_kps'].to(device)\n",
    "    mask    = batch['valid_mask'].to(device)\n",
    "    \n",
    "    # Pre-calculate INDICES for Source and Target Keypoints\n",
    "    # We need to know which patch corresponds to the nose/tail/wing\n",
    "    src_indices = get_patch_indices(src_kps) # [B, N]\n",
    "    trg_indices = get_patch_indices(trg_kps) # [B, N] (Labels)\n",
    "    \n",
    "    print(\"Starting Training Loop (100 Iterations)...\")\n",
    "    model.train()\n",
    "    \n",
    "    # D. The Loop\n",
    "    for i in range(101):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        dict_A = model.forward_features(src)\n",
    "        dict_B = model.forward_features(trg)\n",
    "        \n",
    "        feat_A_all = dict_A['x_norm_patchtokens'] # [B, 1369, 768]\n",
    "        feat_B_all = dict_B['x_norm_patchtokens'] # [B, 1369, 768]\n",
    "        \n",
    "        # 2. Extract specific features at Source Keypoints\n",
    "        feat_A_kps = extract_features_at_indices(feat_A_all, src_indices)\n",
    "        \n",
    "        # 3. Calculate Loss\n",
    "        # Query: Src Keypoints | Keys: All Trg Patches | Correct: Trg Keypoints\n",
    "        loss = contrastive_loss(feat_A_kps, feat_B_all, trg_indices, mask)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. Monitor\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iter {i:3d}: Loss = {loss.item():.5f}\")\n",
    "            \n",
    "            # Sanity Check for \"Accuracy\":\n",
    "            if loss.item() < 0.1:\n",
    "                print(\">>> Converged! Model has memorized the batch.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\n--- Result Analysis ---\")\n",
    "    if loss.item() > 1.0:\n",
    "        print(\"❌ FAILED. Loss is still high. Try unfreezing more blocks (Last 4) or check LR.\")\n",
    "    else:\n",
    "        print(\"✅ PASSED. Model is capable of learning.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM94XQGmSqU+CbzhlwN85hS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
