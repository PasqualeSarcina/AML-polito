{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y8AKrNt5lxaQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' estratto con successo nella directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
      "Contenuti della directory 'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted':\n",
      "['SPair-71k']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "zip_file_path = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k.zip' \n",
    "extract_dir = r'C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted'\n",
    "\n",
    "# Crea la directory di estrazione se non esiste\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Estrai il file ZIP solo se esiste\n",
    "if os.path.exists(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"File '{zip_file_path}' estratto con successo nella directory '{extract_dir}'\")\n",
    "    print(f\"Contenuti della directory '{extract_dir}':\\n{os.listdir(extract_dir)}\")\n",
    "else:\n",
    "    print(f\"File zip '{zip_file_path}' non trovato. Assicurati che il dataset sia estratto in '{extract_dir}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "honcpimEq_B2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Set...\n",
      "Loading Validation Set...\n",
      "Testing batches...\n",
      "Train Batch Images: torch.Size([4, 3, 518, 518])\n",
      "Val Batch Images:   torch.Size([4, 3, 518, 518])\n",
      "Dataset setup complete. Ready for Training Loop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- 1. Define the Augmentation Pipeline ---\n",
    "def get_transforms(mode='train', img_size=518):\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            # Geometric Augmentations (Hard - Moves Keypoints)\n",
    "            A.Resize(height=img_size, width=img_size), # Force DINOv2 size\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            \n",
    "            # Pixel Augmentations (Safe - Colors only)\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.GaussianBlur(p=0.1),\n",
    "            \n",
    "            # Normalization & Conversion\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(), # Converts to (C, H, W)\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "    \n",
    "    else:\n",
    "        # Validation/Test: Only Resize & Normalize\n",
    "        return A.Compose([\n",
    "            A.Resize(height=img_size, width=img_size),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "# --- 2. Simple Image Reader ---\n",
    "def read_img(path):\n",
    "    # Keep as HWC (Standard for Albumentations)\n",
    "    # Do not transpose or convert to Tensor here yet\n",
    "    img = np.array(Image.open(path).convert('RGB'))\n",
    "    return img\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    def __init__(self, pair_ann_path, layout_path, image_path, dataset_size, pck_alpha, datatype):\n",
    "        self.datatype = datatype\n",
    "        self.pck_alpha = pck_alpha\n",
    "        self.ann_files = open(os.path.join(layout_path, dataset_size, datatype + '.txt'), \"r\").read().split('\\n')\n",
    "        self.ann_files = [x for x in self.ann_files if x] # Remove empty strings\n",
    "        self.pair_ann_path = pair_ann_path\n",
    "        self.image_path = image_path\n",
    "        \n",
    "        # Initialize the Transform Pipeline\n",
    "        mode = 'train' if datatype == 'trn' else 'test'\n",
    "        self.transform = get_transforms(mode=mode, img_size=518)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_line = self.ann_files[idx]\n",
    "        ann_filename = raw_line.replace(':', '_')\n",
    "        ann_file = ann_filename + '.json'\n",
    "        json_path = os.path.join(self.pair_ann_path, self.datatype, ann_file)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        category = annotation['category']\n",
    "        src_path = os.path.join(self.image_path, category, annotation['src_imname'])\n",
    "        trg_path = os.path.join(self.image_path, category, annotation['trg_imname'])\n",
    "\n",
    "        # 1. Load Images\n",
    "        src_img_raw = read_img(src_path)\n",
    "        trg_img_raw = read_img(trg_path)\n",
    "\n",
    "        # 2. Get Keypoints\n",
    "        src_kps = np.array(annotation['src_kps']).astype(np.float32)\n",
    "        trg_kps = np.array(annotation['trg_kps']).astype(np.float32)\n",
    "\n",
    "        # 3. Apply Augmentations\n",
    "        src_aug = self.transform(image=src_img_raw, keypoints=src_kps)\n",
    "        src_tensor = src_aug['image']\n",
    "        src_kps_aug = np.array(src_aug['keypoints'])\n",
    "        \n",
    "        trg_aug = self.transform(image=trg_img_raw, keypoints=trg_kps)\n",
    "        trg_tensor = trg_aug['image']\n",
    "        trg_kps_aug = np.array(trg_aug['keypoints'])\n",
    "\n",
    "        # ==========================================================\n",
    "        # ⚠️ CRITICAL FIX: PADDING LOGIC (Prevents Stack Error)\n",
    "        # ==========================================================\n",
    "        # We enforce a fixed size of 40 points per image.\n",
    "        MAX_KPS = 40 \n",
    "        \n",
    "        # Create empty containers filled with zeros (Shape: [40, 2])\n",
    "        src_kps_padded = np.zeros((MAX_KPS, 2), dtype=np.float32)\n",
    "        trg_kps_padded = np.zeros((MAX_KPS, 2), dtype=np.float32)\n",
    "        \n",
    "        # Get the actual number of points (limit to 40 just in case)\n",
    "        n_src = min(len(src_kps_aug), MAX_KPS)\n",
    "        n_trg = min(len(trg_kps_aug), MAX_KPS)\n",
    "        \n",
    "        # Copy the real points into the empty container\n",
    "        if n_src > 0:\n",
    "            src_kps_padded[:n_src] = src_kps_aug[:n_src]\n",
    "        if n_trg > 0:\n",
    "            trg_kps_padded[:n_trg] = trg_kps_aug[:n_trg]\n",
    "\n",
    "        # Check which points are inside the image (Visibility)\n",
    "        src_vis = self._check_visibility(src_kps_padded, 518, 518)\n",
    "        trg_vis = self._check_visibility(trg_kps_padded, 518, 518)\n",
    "        \n",
    "        # Create the Valid Mask\n",
    "        # A point is valid ONLY if:\n",
    "        # 1. It existed in the original file (index < n_src)\n",
    "        # 2. It is still inside the image boundaries (vis=1)\n",
    "        valid_mask = np.zeros(MAX_KPS, dtype=np.float32)\n",
    "        \n",
    "        # We assume points are ordered pairs (1st src matches 1st trg)\n",
    "        # So we only mark as valid if BOTH exist and are visible\n",
    "        common_len = min(n_src, n_trg)\n",
    "        valid_mask[:common_len] = src_vis[:common_len] * trg_vis[:common_len]\n",
    "        # ==========================================================\n",
    "\n",
    "        pck_threshold = 518 * self.pck_alpha\n",
    "\n",
    "        sample = {\n",
    "            'pair_id': annotation['pair_id'],\n",
    "            'src_img': src_tensor,\n",
    "            'trg_img': trg_tensor,\n",
    "            \n",
    "            # Now these are ALWAYS [40, 2], so PyTorch won't crash!\n",
    "            'src_kps': torch.from_numpy(src_kps_padded).float(), \n",
    "            'trg_kps': torch.from_numpy(trg_kps_padded).float(), \n",
    "            'valid_mask': torch.from_numpy(valid_mask).float(), \n",
    "            \n",
    "            'pck_threshold': pck_threshold,\n",
    "            'category': category\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _check_visibility(self, kps, h, w):\n",
    "        \"\"\"Returns a binary mask (N,) where 1=visible, 0=out of bounds\"\"\"\n",
    "        # kps is shape (N, 2) -> (x, y)\n",
    "        x = kps[:, 0]\n",
    "        y = kps[:, 1]\n",
    "        \n",
    "        # Check boundaries\n",
    "        vis_x = (x >= 0) & (x < w)\n",
    "        vis_y = (y >= 0) & (y < h)\n",
    "        return (vis_x & vis_y).astype(np.float32)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Update this path to your actual path\n",
    "    base_dir = r\"C:\\Users\\nicol\\Documents\\PoliTo\\AdvancedML\\project\\SPair-71k_extracted\\SPair-71k\\SPair-71k\"    \n",
    "    \n",
    "    pair_ann_path = os.path.join(base_dir, 'PairAnnotation')\n",
    "    layout_path = os.path.join(base_dir, 'Layout')\n",
    "    image_path = os.path.join(base_dir, 'JPEGImages')\n",
    "\n",
    "    # Check paths\n",
    "    if os.path.exists(base_dir):\n",
    "        \n",
    "        # --- 1. Load TRAINING Set ---\n",
    "        print(\"Loading Training Set...\")\n",
    "        trn_dataset = SPairDataset(\n",
    "            pair_ann_path, layout_path, image_path, \n",
    "            dataset_size='large', pck_alpha=0.05, \n",
    "            datatype='trn'  # <--- Loads from trn.txt\n",
    "        )\n",
    "        # SHUFFLE = TRUE for training (important for learning)\n",
    "        trn_loader = DataLoader(trn_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "        # --- 2. Load VALIDATION Set ---\n",
    "        print(\"Loading Validation Set...\")\n",
    "        val_dataset = SPairDataset(\n",
    "            pair_ann_path, layout_path, image_path, \n",
    "            dataset_size='large', pck_alpha=0.05, \n",
    "            datatype='val'  # <--- Loads from val.txt\n",
    "        )\n",
    "        # SHUFFLE = FALSE for validation (keep order stable)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "        \n",
    "        # --- 3. Test Loading ---\n",
    "        print(\"Testing batches...\")\n",
    "        \n",
    "        # Grab a training batch\n",
    "        trn_batch = next(iter(trn_loader))\n",
    "        print(f\"Train Batch Images: {trn_batch['src_img'].shape}\")\n",
    "        \n",
    "        # Grab a validation batch\n",
    "        val_batch = next(iter(val_loader))\n",
    "        print(f\"Val Batch Images:   {val_batch['src_img'].shape}\")\n",
    "        \n",
    "        print(\"Dataset setup complete. Ready for Training Loop.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Path not found: {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: InfoNCE Sanity Check ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated InfoNCE Loss: 6.5601\n",
      "------------------------------\n",
      "✅ PASSED. Loss is close to expected random guess (7.22).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# --- 1. Define the Loss Function (InfoNCE) ---\n",
    "def info_nce_loss(feat_A, feat_B, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Calculates the loss assuming the correct match for patch[i] in A \n",
    "    is patch[i] in B (The Diagonal).\n",
    "    \"\"\"\n",
    "    # Normalize features (Crucial for Dot Product Similarity)\n",
    "    feat_A = F.normalize(feat_A, dim=-1)\n",
    "    feat_B = F.normalize(feat_B, dim=-1)\n",
    "    \n",
    "    # Similarity Matrix: [Batch, Tokens, Tokens]\n",
    "    # We compare every patch in A against every patch in B\n",
    "    sim_matrix = torch.bmm(feat_A, feat_B.transpose(1, 2)) / temperature\n",
    "    \n",
    "    # Labels: We assume Identity matching for the check (0->0, 1->1...)\n",
    "    B, T, _ = sim_matrix.shape\n",
    "    labels = torch.arange(T).to(feat_A.device).expand(B, T)\n",
    "    \n",
    "    # Calculate Cross Entropy\n",
    "    loss = F.cross_entropy(sim_matrix.flatten(0, 1), labels.flatten())\n",
    "    return loss\n",
    "\n",
    "# --- 2. The Sanity Check ---\n",
    "if __name__ == '__main__':\n",
    "    # (Assuming trn_loader is defined from your previous code...)\n",
    "    # from your_dataset_script import trn_loader \n",
    "    \n",
    "    print(\"\\n--- Step 1: InfoNCE Sanity Check ---\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 1. Initialize Model (Standard DINOv2 base)\n",
    "    backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').to(device)\n",
    "    backbone.eval() # Freeze for check\n",
    "    \n",
    "    # 2. Get One Batch\n",
    "    try:\n",
    "        batch = next(iter(trn_loader))\n",
    "    except NameError:\n",
    "        print(\"Error: 'trn_loader' is not defined. Run the Dataset setup code first!\")\n",
    "        exit()\n",
    "\n",
    "    src_img = batch['src_img'].to(device)\n",
    "    trg_img = batch['trg_img'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 3. Forward Pass\n",
    "        dict_A = backbone.forward_features(src_img)\n",
    "        dict_B = backbone.forward_features(trg_img)\n",
    "        \n",
    "        # Extract Patch Tokens: [Batch, 1369, 384]\n",
    "        feat_A = dict_A['x_norm_patchtokens']\n",
    "        feat_B = dict_B['x_norm_patchtokens']\n",
    "        \n",
    "        # 4. Calculate Loss\n",
    "        loss = info_nce_loss(feat_A, feat_B)\n",
    "        \n",
    "    print(f\"Calculated InfoNCE Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # --- INTERPRETATION ---\n",
    "    print(\"-\" * 30)\n",
    "    expected_loss = 7.22 # ln(1369)\n",
    "    \n",
    "    if 6.0 < loss.item() < 8.5:\n",
    "        print(f\"✅ PASSED. Loss is close to expected random guess ({expected_loss}).\")\n",
    "    elif loss.item() > 9.0:\n",
    "        print(f\"⚠️ HIGH. Loss is {loss.item():.2f}. This is normal if images are very different (not aligned).\")\n",
    "    else:\n",
    "        print(f\"❌ LOW. Loss is {loss.item():.2f}. Something is wrong (Masking? Identity leakage?).\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM94XQGmSqU+CbzhlwN85hS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
