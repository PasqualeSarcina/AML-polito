{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Overfit Small Sample ---\n",
      "Loading Model on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\nicol/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing one batch...\n",
      "Starting Training Loop (100 Iterations)...\n",
      "Iter   0: Loss = 4.52095\n",
      "Iter  10: Loss = 1.39504\n",
      "Iter  20: Loss = 0.59009\n",
      "Iter  30: Loss = 0.37727\n",
      "Iter  40: Loss = 0.27629\n",
      "Iter  50: Loss = 0.12469\n",
      "Iter  60: Loss = 0.62037\n",
      "Iter  70: Loss = 0.21654\n",
      "Iter  80: Loss = 0.53351\n",
      "Iter  90: Loss = 0.24069\n",
      "Iter 100: Loss = 0.08809\n",
      ">>> Converged! Model has memorized the batch.\n",
      "\n",
      "--- Result Analysis ---\n",
      "✅ PASSED. Model is capable of learning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- 1. Helper: Map (x,y) coordinates to Patch Index ---\n",
    "def get_patch_indices(keypoints, img_size=518, patch_size=14):\n",
    "    \"\"\"\n",
    "    Converts (x, y) pixel coordinates into the index of the patch (0 to 1368).\n",
    "    keypoints: [Batch, N_kps, 2]\n",
    "    \"\"\"\n",
    "    grid_w = img_size // patch_size # 37\n",
    "    \n",
    "    # Scale coordinates to grid integers (0..36)\n",
    "    grid_x = (keypoints[:, :, 0] / patch_size).long().clamp(0, grid_w-1)\n",
    "    grid_y = (keypoints[:, :, 1] / patch_size).long().clamp(0, grid_w-1)\n",
    "    \n",
    "    # Calculate flat index (y * width + x)\n",
    "    flat_indices = grid_y * grid_w + grid_x  # [Batch, N_kps]\n",
    "    return flat_indices\n",
    "\n",
    "# --- 2. Helper: Extract Features at those Indices ---\n",
    "def extract_features_at_indices(features, indices):\n",
    "    \"\"\"\n",
    "    features: [Batch, 1369, Dim]\n",
    "    indices:  [Batch, N_kps]\n",
    "    Returns:  [Batch, N_kps, Dim]\n",
    "    \"\"\"\n",
    "    B, N_patches, Dim = features.shape\n",
    "    B, N_kps = indices.shape\n",
    "    \n",
    "    # Expand indices to gather across the Dim dimension\n",
    "    # [B, N_kps] -> [B, N_kps, Dim]\n",
    "    indices_expanded = indices.unsqueeze(-1).expand(-1, -1, Dim)\n",
    "    \n",
    "    # Gather specific features\n",
    "    kps_features = torch.gather(features, 1, indices_expanded)\n",
    "    return kps_features\n",
    "\n",
    "# --- 3. The Contrastive Loss ---\n",
    "def contrastive_loss(feat_src_kps, feat_trg_all, trg_kps_indices, mask, temp=0.1):\n",
    "    \"\"\"\n",
    "    feat_src_kps:    [B, N, Dim]   (Query: Feature at Source Nose)\n",
    "    feat_trg_all:    [B, 1369, Dim](Keys: All patches in Target Image)\n",
    "    trg_kps_indices: [B, N]        (Label: Index of Target Nose)\n",
    "    mask:            [B, N]        (Valid points only)\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    feat_src_kps = F.normalize(feat_src_kps, dim=-1)\n",
    "    feat_trg_all = F.normalize(feat_trg_all, dim=-1)\n",
    "    \n",
    "    # Similarity: [B, N, Dim] @ [B, Dim, 1369] -> [B, N, 1369]\n",
    "    # We compare every Source Keypoint against ALL Target Patches\n",
    "    logits = torch.bmm(feat_src_kps, feat_trg_all.transpose(1, 2)) / temp\n",
    "    \n",
    "    # Flatten everything to 2D for CrossEntropy\n",
    "    # We only care about VALID keypoints\n",
    "    valid = mask.bool()\n",
    "    \n",
    "    logits_valid = logits[valid]       # [Total_Valid_Kps, 1369]\n",
    "    targets_valid = trg_kps_indices[valid] # [Total_Valid_Kps]\n",
    "    \n",
    "    loss = F.cross_entropy(logits_valid, targets_valid)\n",
    "    return loss\n",
    "\n",
    "# --- 4. Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Step 2: Overfit Small Sample ---\")\n",
    "    \n",
    "    # A. Setup Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading Model on {device}...\")\n",
    "    model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').to(device)\n",
    "    \n",
    "    # Freeze Backbone\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreeze Last 2 Blocks\n",
    "    for block in model.blocks[-2:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # B. Setup Optimizer (High LR, No Decay)\n",
    "    # We filter specifically for parameters that require grad\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                            lr=1e-3, weight_decay=0)\n",
    "    \n",
    "    # C. Get ONE Fixed Batch\n",
    "    print(\"Grabbing one batch...\")\n",
    "    try:\n",
    "        batch = next(iter(trn_loader))\n",
    "    except NameError:\n",
    "        print(\"Error: trn_loader not defined. Run dataset code first.\")\n",
    "        exit()\n",
    "\n",
    "    src = batch['src_img'].to(device)\n",
    "    trg = batch['trg_img'].to(device)\n",
    "    src_kps = batch['src_kps'].to(device)\n",
    "    trg_kps = batch['trg_kps'].to(device)\n",
    "    mask    = batch['valid_mask'].to(device)\n",
    "    \n",
    "    # Pre-calculate INDICES for Source and Target Keypoints\n",
    "    # We need to know which patch corresponds to the nose/tail/wing\n",
    "    src_indices = get_patch_indices(src_kps) # [B, N]\n",
    "    trg_indices = get_patch_indices(trg_kps) # [B, N] (Labels)\n",
    "    \n",
    "    print(\"Starting Training Loop (100 Iterations)...\")\n",
    "    model.train()\n",
    "    \n",
    "    # D. The Loop\n",
    "    for i in range(101):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        dict_A = model.forward_features(src)\n",
    "        dict_B = model.forward_features(trg)\n",
    "        \n",
    "        feat_A_all = dict_A['x_norm_patchtokens'] # [B, 1369, 768]\n",
    "        feat_B_all = dict_B['x_norm_patchtokens'] # [B, 1369, 768]\n",
    "        \n",
    "        # 2. Extract specific features at Source Keypoints\n",
    "        feat_A_kps = extract_features_at_indices(feat_A_all, src_indices)\n",
    "        \n",
    "        # 3. Calculate Loss\n",
    "        # Query: Src Keypoints | Keys: All Trg Patches | Correct: Trg Keypoints\n",
    "        loss = contrastive_loss(feat_A_kps, feat_B_all, trg_indices, mask)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. Monitor\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iter {i:3d}: Loss = {loss.item():.5f}\")\n",
    "            \n",
    "            # Sanity Check for \"Accuracy\":\n",
    "            if loss.item() < 0.1:\n",
    "                print(\">>> Converged! Model has memorized the batch.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\n--- Result Analysis ---\")\n",
    "    if loss.item() > 1.0:\n",
    "        print(\"❌ FAILED. Loss is still high. Try unfreezing more blocks (Last 4) or check LR.\")\n",
    "    else:\n",
    "        print(\"✅ PASSED. Model is capable of learning.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM94XQGmSqU+CbzhlwN85hS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
